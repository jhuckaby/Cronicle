{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467930,"source":"Manual (admin)","id":"jjh8zldet16","time_start":1526467930.661,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zldet16.log","pid":18056,"cpu":{"min":0.1,"max":2.2,"total":10.000000000000002,"count":20,"current":0.1},"mem":{"min":69058560,"max":69332992,"total":1381445632,"count":20,"current":69332992},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526468137.032,"elapsed":206.3710000514984}