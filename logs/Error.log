[1526461443.865][2018-05-16 05:04:03][osboxes][Error][error][socket][Socket client XkTqyfDDl4QImUKQAAAA failed to authenticate (IP: ::ffff:10.0.2.15)][]
[1526461504.42][2018-05-16 05:05:04][osboxes][Error][error][socket][Socket client 2OlavseF7YrCqhoRAAAB failed to authenticate (IP: ::ffff:10.0.2.15)][]
[1526461680.388][2018-05-16 05:08:00][osboxes][Error][error][job][Job failed: jjh8vu3nt02][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461619,"source":"Manual (admin)","id":"jjh8vu3nt02","time_start":1526461619.465,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vu3nt02.log","pid":6948,"cpu":{"min":0.8999999999999999,"max":3.3,"total":8.4,"count":5,"current":0.8999999999999999},"mem":{"min":68730880,"max":68730880,"total":343654400,"count":5,"current":68730880},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":633,"time_end":1526461680.388,"elapsed":60.92300009727478}]
[1526461831.417][2018-05-16 05:10:31][osboxes][Error][error][job][Job failed: jjh8vvopc03][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461693,"source":"Manual (admin)","id":"jjh8vvopc03","time_start":1526461693.392,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vvopc03.log","pid":7074,"cpu":{"min":0.6,"max":3.4000000000000004,"total":14.299999999999997,"count":13,"current":0.6},"mem":{"min":67620864,"max":67780608,"total":879230976,"count":13,"current":67780608},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":633,"time_end":1526461831.417,"elapsed":138.02500009536743}]
[1526462020.285][2018-05-16 05:13:40][osboxes][Error][error][job][Job failed: jjh8w1uy405][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461981,"source":"Manual (admin)","id":"jjh8w1uy405","time_start":1526461981.42,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w1uy405.log","pid":7806,"cpu":{"min":1.3,"max":3.2,"total":6.3,"count":3,"current":1.3},"mem":{"min":68640768,"max":68792320,"total":206073856,"count":3,"current":68792320},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":633,"time_end":1526462020.285,"elapsed":38.86500000953674}]
[1526462132.136][2018-05-16 05:15:32][osboxes][Error][error][job][Job failed: jjh8w52vh06][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : latest\t\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462131,"source":"Manual (admin)","id":"jjh8w52vh06","time_start":1526462131.661,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w52vh06.log","pid":7996,"complete":1,"code":1,"description":"Script exited with code: 1: A problem occurred when sending our message","html":{"title":"Error Output","content":"<pre>A problem occurred when sending our message\nReferenceError: latest is not defined\n    at Producer.&lt;anonymous> (/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/bin/cronicle-script-temp-jjh8w52vh06.sh:43:29)\n    at Producer.emit (events.js:185:15)\n    at /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/client.js:212:12\n    at /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/client.js:348:7</pre>"},"log_file_size":856,"time_end":1526462132.136,"elapsed":0.47499990463256836}]
[1526462206.934][2018-05-16 05:16:46][osboxes][Error][error][job][Job failed: jjh8w6okq07][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462206,"source":"Manual (admin)","id":"jjh8w6okq07","time_start":1526462206.442,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w6okq07.log","pid":8142,"complete":1,"code":1,"description":"Script exited with code: 1: A problem occurred when sending our message","html":{"title":"Error Output","content":"<pre>A problem occurred when sending our message\nError: No such configuration property: \"auto.commit.reset\"\n    at KafkaConsumer.Client (/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/client.js:54:18)\n    at new KafkaConsumer (/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/kafka-consumer.js:117:10)\n    at Producer.&lt;anonymous> (/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/bin/cronicle-script-temp-jjh8w6okq07.sh:38:20)\n    at Producer.emit (events.js:185:15)\n    at /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/client.js:212:12\n    at /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/client.js:348:7</pre>"},"log_file_size":1147,"time_end":1526462206.934,"elapsed":0.49200010299682617}]
[1526462430.479][2018-05-16 05:20:30][osboxes][Error][error][job][Job failed: jjh8wa5gk08][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462368,"source":"Manual (admin)","id":"jjh8wa5gk08","time_start":1526462368.292,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log","pid":8375,"cpu":{"min":0.8,"max":3.7,"total":10.000000000000002,"count":6,"current":0.8},"mem":{"min":68816896,"max":68964352,"total":413048832,"count":6,"current":68964352},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":633,"time_end":1526462430.479,"elapsed":62.187000036239624}]
[1526462598.869][2018-05-16 05:23:18][osboxes][Error][error][job][Job failed: jjh8wbk7u09][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462434,"source":"Manual (admin)","id":"jjh8wbk7u09","time_start":1526462434.075,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log","pid":8541,"cpu":{"min":0.5,"max":6,"total":17.4,"count":16,"current":0.5},"mem":{"min":68698112,"max":68739072,"total":1099210752,"count":16,"current":68739072},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":633,"time_end":1526462598.869,"elapsed":164.79399991035461}]
[1526462627.858][2018-05-16 05:23:47][osboxes][Error][error][job][Job failed: jjh8wf61x0a][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462602,"source":"Manual (admin)","id":"jjh8wf61x0a","time_start":1526462602.341,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wf61x0a.log","pid":8784,"cpu":{"min":2,"max":3.7,"total":5.7,"count":2,"current":2},"mem":{"min":68980736,"max":69128192,"total":138108928,"count":2,"current":69128192},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":633,"time_end":1526462627.858,"elapsed":25.51699995994568}]
[1526462949.392][2018-05-16 05:29:09][osboxes][Error][error][job][Job failed: jjh8wlz6z0d][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462920,"source":"Manual (admin)","id":"jjh8wlz6z0d","time_start":1526462920.043,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlz6z0d.log","pid":9284,"cpu":{"min":1.7,"max":3.3,"total":5,"count":2,"current":1.7},"mem":{"min":68939776,"max":68939776,"total":137879552,"count":2,"current":68939776},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":633,"time_end":1526462949.392,"elapsed":29.348999977111816}]
[1526463346.323][2018-05-16 05:35:46][osboxes][Error][error][job][Job failed: jjh8wt7lx0e][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463257,"source":"Manual (admin)","id":"jjh8wt7lx0e","time_start":1526463257.541,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wt7lx0e.log","pid":9637,"cpu":{"min":0.5,"max":3.1,"total":9.100000000000001,"count":8,"current":0.5},"mem":{"min":68771840,"max":68923392,"total":550326272,"count":8,"current":68923392},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":633,"time_end":1526463346.323,"elapsed":88.78200006484985}]
[1526463384.472][2018-05-16 05:36:24][osboxes][Error][error][job][Job failed: jjh8wv98c0f][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463352,"source":"Manual (admin)","id":"jjh8wv98c0f","time_start":1526463352.956,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wv98c0f.log","pid":9796,"cpu":{"min":1.4,"max":3.6,"total":7,"count":3,"current":1.4},"mem":{"min":68857856,"max":69005312,"total":206721024,"count":3,"current":69005312},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":633,"time_end":1526463384.472,"elapsed":31.515999794006348}]
[1526463973.123][2018-05-16 05:46:13][osboxes][Error][error][job][Job failed: jjh8x84il0k][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463953,"source":"Manual (admin)","id":"jjh8x84il0k","time_start":1526463953.373,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x84il0k.log","pid":10818,"abort_reason":"Manually aborted by user: admin","cpu":{"min":3.2,"max":3.2,"total":3.2,"count":1,"current":3.2},"mem":{"min":67944448,"max":67944448,"total":67944448,"count":1,"current":67944448},"complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526463973.123,"elapsed":19.75}]
[1526464158.4][2018-05-16 05:49:18][osboxes][Error][error][job][Job failed: jjh8xbzmv0l][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'beginning'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464133,"source":"Manual (admin)","id":"jjh8xbzmv0l","time_start":1526464133.671,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xbzmv0l.log","pid":11036,"cpu":{"min":2.2,"max":3.8,"total":6,"count":2,"current":2.2},"mem":{"min":68796416,"max":68947968,"total":137744384,"count":2,"current":68947968},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526464158.4,"elapsed":24.729000091552734}]
[1526464842.032][2018-05-16 06:00:42][osboxes][Error][error][job][Job failed: jjh8xi9qd0m][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': false,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464426,"source":"Manual (admin)","id":"jjh8xi9qd0m","time_start":1526464426.693,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xi9qd0m.log","pid":11407,"cpu":{"min":0.3,"max":3.2,"total":21.000000000000018,"count":41,"current":0.3},"mem":{"min":67751936,"max":67903488,"total":2777980928,"count":41,"current":67903488},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526464842.032,"elapsed":415.33899998664856}]
[1526464911.92][2018-05-16 06:01:51][osboxes][Error][error][job][Job failed: jjh8xrnh10n][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464864,"source":"Manual (admin)","id":"jjh8xrnh10n","time_start":1526464864.405,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xrnh10n.log","pid":12353,"cpu":{"min":3.5999999999999996,"max":6.5,"total":18.6,"count":4,"current":3.5999999999999996},"mem":{"min":72687616,"max":74321920,"total":293122048,"count":4,"current":73146368},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":471397,"time_end":1526464911.92,"elapsed":47.515000104904175}]
[1526465498.92][2018-05-16 06:11:38][osboxes][Error][error][job][Job failed: jjh8y15eo0q][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465307,"source":"Manual (admin)","id":"jjh8y15eo0q","time_start":1526465307.552,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y15eo0q.log","pid":13176,"cpu":{"min":0.5,"max":4.4,"total":17.6,"count":19,"current":0.5},"mem":{"min":68866048,"max":69013504,"total":1308602368,"count":19,"current":69013504},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526465498.92,"elapsed":191.36800003051758}]
[1526465500.963][2018-05-16 06:11:40][osboxes][Error][error][job][Job failed: jjh8y0mgr0p][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465282,"source":"Manual (admin)","id":"jjh8y0mgr0p","time_start":1526465283.003,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y0mgr0p.log","pid":13115,"cpu":{"min":0.4,"max":3.2,"total":17.499999999999996,"count":21,"current":0.4},"mem":{"min":68743168,"max":69001216,"total":1443864576,"count":21,"current":69001216},"jobid":"ForecastOptimizationjjh8y15eo0q","result":"Success","percentage":100,"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526465500.963,"elapsed":217.96000003814697}]
[1526465502.936][2018-05-16 06:11:42][osboxes][Error][error][job][Job failed: jjh8y09bl0o][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465265,"source":"Manual (admin)","id":"jjh8y09bl0o","time_start":1526465265.969,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log","pid":13066,"cpu":{"min":0.4,"max":4.2,"total":20.499999999999993,"count":23,"current":0.4},"mem":{"min":68542464,"max":68747264,"total":1576681472,"count":23,"current":68747264},"jobid":"ForecastOptimizationjjh8y15eo0q","result":"Success","percentage":100,"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526465502.936,"elapsed":236.9670000076294}]
[1526465542.414][2018-05-16 06:12:22][osboxes][Error][error][job][Job failed: jjh8y5kok0r][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465513,"source":"Manual (admin)","id":"jjh8y5kok0r","time_start":1526465513.972,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y5kok0r.log","pid":13521,"cpu":{"min":4.7,"max":6.7,"total":11.4,"count":2,"current":4.7},"mem":{"min":71909376,"max":73498624,"total":145408000,"count":2,"current":71909376},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":257637,"time_end":1526465542.414,"elapsed":28.442000150680542}]
[1526465969.378][2018-05-16 06:19:29][osboxes][Error][error][job][Job failed: jjh8y837u0u][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465631,"source":"Manual (admin)","id":"jjh8y837u0u","time_start":1526465631.306,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y837u0u.log","pid":13953,"cpu":{"min":0.3,"max":4.3,"total":20.30000000000001,"count":34,"current":0.3},"mem":{"min":68927488,"max":69144576,"total":2343751680,"count":34,"current":69144576},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526465969.378,"elapsed":338.0720000267029}]
[1526465971.192][2018-05-16 06:19:31][osboxes][Error][error][job][Job failed: jjh8y71lx0t][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465582,"source":"Manual (admin)","id":"jjh8y71lx0t","time_start":1526465582.565,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y71lx0t.log","pid":13865,"cpu":{"min":0.3,"max":5.4,"total":24.300000000000015,"count":39,"current":0.3},"mem":{"min":67837952,"max":68034560,"total":2645876736,"count":39,"current":68034560},"jobid":"ForecastOptimizationjjh8y837u0u","result":"Success","percentage":100,"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":670,"time_end":1526465971.192,"elapsed":388.6269998550415}]
[1526465973.031][2018-05-16 06:19:33][osboxes][Error][error][job][Job failed: jjh8y6q9h0s][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465567,"source":"Manual (admin)","id":"jjh8y6q9h0s","time_start":1526465567.861,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log","pid":13806,"cpu":{"min":0.3,"max":3,"total":20.400000000000016,"count":40,"current":0.3},"mem":{"min":68911104,"max":69115904,"total":2756648960,"count":40,"current":69115904},"jobid":"ForecastOptimizationjjh8y837u0u","result":"Success","percentage":100,"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":705,"time_end":1526465973.031,"elapsed":405.16999983787537}]
[1526466265.277][2018-05-16 06:24:25][osboxes][Error][error][job][Job failed: jjh8yg2y10w][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466004,"source":"Manual (admin)","id":"jjh8yg2y10w","time_start":1526466004.201,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yg2y10w.log","pid":14425,"cpu":{"min":0.3,"max":3,"total":17.700000000000003,"count":25,"current":0.3},"mem":{"min":68870144,"max":69033984,"total":1721917440,"count":25,"current":69033984},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526466265.277,"elapsed":261.07599997520447}]
[1526466267.318][2018-05-16 06:24:27][osboxes][Error][error][job][Job failed: jjh8yfjmq0v][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465979,"source":"Manual (admin)","id":"jjh8yfjmq0v","time_start":1526465979.17,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yfjmq0v.log","pid":14366,"cpu":{"min":0.3,"max":3.5,"total":18.200000000000006,"count":28,"current":0.3},"mem":{"min":67837952,"max":67891200,"total":1899515904,"count":28,"current":67891200},"value":{"type":"Buffer","data":[123,34,116,105,109,101,111,117,116,34,58,32,51,54,48,48,44,32,34,106,111,98,105,100,34,58,32,34,70,111,114,101,99,97,115,116,79,112,116,105,109,105,122,97,116,105,111,110,106,106,104,56,121,103,50,121,49,48,119,34,44,32,34,114,101,115,117,108,116,34,58,32,34,83,117,99,99,101,115,115,34,44,32,34,112,101,114,99,101,110,116,97,103,101,34,58,32,49,48,48,125]},"size":101,"key":null,"topic":"TriggerForecastOptimizationResponse","offset":86,"partition":0,"timestamp":1526466005905,"jobid":"ForecastOptimizationjjh8yg2y10w","result":"Success","percentage":100,"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":670,"time_end":1526466267.318,"elapsed":288.14800000190735}]
[1526466704.344][2018-05-16 06:31:44][osboxes][Error][error][job][Job failed: jjh8ymd1w0x][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466297,"source":"Manual (admin)","id":"jjh8ymd1w0x","time_start":1526466297.236,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log","pid":14945,"cpu":{"min":0.3,"max":3.0999999999999996,"total":20.900000000000016,"count":40,"current":0.3},"mem":{"min":68796416,"max":69066752,"total":2752126976,"count":40,"current":69066752},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526466704.344,"elapsed":407.1080000400543}]
[1526466733.922][2018-05-16 06:32:13][osboxes][Error][error][job][Job failed: jjh8yvc1810][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466715,"source":"Manual (admin)","id":"jjh8yvc1810","time_start":1526466715.82,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yvc1810.log","pid":15778,"abort_reason":"Manually aborted by user: admin","cpu":{"min":3.3,"max":3.3,"total":3.3,"count":1,"current":3.3},"mem":{"min":69038080,"max":69038080,"total":69038080,"count":1,"current":69038080},"complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526466733.922,"elapsed":18.10199999809265}]
[1526467649.36][2018-05-16 06:47:29][osboxes][Error][error][job][Job failed: jjh8z0g9h11][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466954,"source":"Manual (admin)","id":"jjh8z0g9h11","time_start":1526466954.581,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8z0g9h11.log","pid":16137,"cpu":{"min":0.2,"max":3.2,"total":26.400000000000002,"count":69,"current":0.2},"mem":{"min":69140480,"max":69464064,"total":4771016704,"count":69,"current":69464064},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526467649.36,"elapsed":694.7789998054504}]
[1526468134.826][2018-05-16 06:55:34][osboxes][Error][error][job][Job failed: jjh8znj8717][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468031,"source":"Manual (admin)","id":"jjh8znj8717","time_start":1526468031.511,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8znj8717.log","pid":18330,"cpu":{"min":0.4,"max":2.2,"total":7.500000000000002,"count":9,"current":0.4},"mem":{"min":69091328,"max":69091328,"total":621821952,"count":9,"current":69091328},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526468134.826,"elapsed":103.31500005722046}]
[1526468137.032][2018-05-16 06:55:37][osboxes][Error][error][job][Job failed: jjh8zldet16][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467930,"source":"Manual (admin)","id":"jjh8zldet16","time_start":1526467930.661,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zldet16.log","pid":18056,"cpu":{"min":0.1,"max":2.2,"total":10.000000000000002,"count":20,"current":0.1},"mem":{"min":69058560,"max":69332992,"total":1381445632,"count":20,"current":69332992},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526468137.032,"elapsed":206.3710000514984}]
[1526468139.34][2018-05-16 06:55:39][osboxes][Error][error][job][Job failed: jjh8zi4fr14][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467779,"source":"Manual (admin)","id":"jjh8zi4fr14","time_start":1526467779.063,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zi4fr14.log","pid":17219,"cpu":{"min":0.1,"max":4.4,"total":14.599999999999998,"count":36,"current":0.1},"mem":{"min":68800512,"max":69050368,"total":2477068288,"count":36,"current":69050368},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526468139.34,"elapsed":360.27699995040894}]
[1526468141.468][2018-05-16 06:55:41][osboxes][Error][error][job][Job failed: jjh8zfeuj12][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467652,"source":"Manual (admin)","id":"jjh8zfeuj12","time_start":1526467652.587,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log","pid":16860,"cpu":{"min":0.1,"max":2.7,"total":16.8,"count":48,"current":0.1},"mem":{"min":69038080,"max":69308416,"total":3314098176,"count":48,"current":69308416},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526468141.468,"elapsed":488.88100004196167}]
[1526468158.049][2018-05-16 06:55:58][osboxes][Error][error][job][Job failed: jjh8zgj2m13][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467704,"source":"Manual (admin)","id":"jjh8zgj2m13","time_start":1526467704.719,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zgj2m13.log","pid":17049,"cpu":{"min":0.3,"max":3.5,"total":23.000000000000018,"count":45,"current":0.3},"mem":{"min":68980736,"max":69238784,"total":3104391168,"count":45,"current":69238784},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526468158.049,"elapsed":453.32999992370605}]
[1526468164.354][2018-05-16 06:56:04][osboxes][Error][error][job][Job failed: jjh8zkpzy15][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467900,"source":"Manual (admin)","id":"jjh8zkpzy15","time_start":1526467900.318,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zkpzy15.log","pid":17779,"cpu":{"min":0.1,"max":2.3,"total":11.5,"count":25,"current":0.2},"mem":{"min":69074944,"max":69074944,"total":1726873600,"count":25,"current":69074944},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","log_file_size":635,"time_end":1526468164.354,"elapsed":264.03600001335144}]
[1526469314.78][2018-05-16 07:15:14][osboxes][Error][error][job][Job failed: jjh8zwoh618][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468458,"source":"Manual (admin)","id":"jjh8zwoh618","time_start":1526468458.218,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zwoh618.log","pid":18815,"cpu":{"min":0.2,"max":5,"total":33.90000000000003,"count":85,"current":0.2},"mem":{"min":69029888,"max":69103616,"total":5867614208,"count":85,"current":69103616},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","html":{"title":"Error Output","content":"<pre>undefined:1\n\n\n\nSyntaxError: Unexpected token</pre>"},"log_file_size":548,"time_end":1526469314.78,"elapsed":856.5620000362396}]
