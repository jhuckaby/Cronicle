[1526461443.184][2018-05-16 05:04:03][osboxes][Cronicle][debug][1][Cronicle v0.8.2 Starting Up][]
[1526461443.206][2018-05-16 05:04:03][osboxes][Cronicle][debug][2][Server IP: 10.0.2.15, Daemon PID: 6737][]
[1526461443.207][2018-05-16 05:04:03][osboxes][Cronicle][debug][3][Starting component: Storage][]
[1526461443.211][2018-05-16 05:04:03][osboxes][Cronicle][debug][3][Starting component: WebServer][]
[1526461443.233][2018-05-16 05:04:03][osboxes][Cronicle][debug][3][Starting component: API][]
[1526461443.234][2018-05-16 05:04:03][osboxes][Cronicle][debug][3][Starting component: User][]
[1526461443.235][2018-05-16 05:04:03][osboxes][Cronicle][debug][3][Starting component: Cronicle][]
[1526461443.236][2018-05-16 05:04:03][osboxes][Cronicle][debug][3][Cronicle engine starting up][["/usr/local/bin/node","/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/lib/main.js"]]
[1526461443.269][2018-05-16 05:04:03][osboxes][Cronicle][debug][4][Using broadcast IP: 10.0.2.255][]
[1526461443.273][2018-05-16 05:04:03][osboxes][Cronicle][debug][4][Starting UDP server on port: 3014][]
[1526461443.297][2018-05-16 05:04:03][osboxes][Cronicle][debug][4][Server is eligible to become master (Master Group)][]
[1526461443.298][2018-05-16 05:04:03][osboxes][Cronicle][debug][2][Startup complete, entering main loop][]
[1526461443.856][2018-05-16 05:04:03][osboxes][Cronicle][debug][5][New socket.io client connected: XkTqyfDDl4QImUKQAAAA (IP: ::ffff:10.0.2.15)][]
[1526461486.57][2018-05-16 05:04:46][osboxes][Cronicle][debug][5][Socket.io client disconnected: XkTqyfDDl4QImUKQAAAA (IP: ::ffff:10.0.2.15)][]
[1526461503.431][2018-05-16 05:05:03][osboxes][Cronicle][debug][3][No master ping received within 60 seconds, choosing new master][]
[1526461503.46][2018-05-16 05:05:03][osboxes][Cronicle][debug][5][We are the top candidate for master][]
[1526461503.465][2018-05-16 05:05:03][osboxes][Cronicle][debug][3][We are becoming the master server][]
[1526461503.58][2018-05-16 05:05:03][osboxes][Cronicle][debug][4][Looking for leftover job JSON files: logs/jobs/*.json][]
[1526461503.646][2018-05-16 05:05:03][osboxes][Cronicle][debug][9][Job recovery complete][]
[1526461503.647][2018-05-16 05:05:03][osboxes][Cronicle][debug][4][Looking for leftover job logs: logs/jobs/*.log][]
[1526461503.647][2018-05-16 05:05:03][osboxes][Cronicle][debug][9][Log recovery complete][]
[1526461504.354][2018-05-16 05:05:04][osboxes][Cronicle][debug][5][New socket.io client connected: 2OlavseF7YrCqhoRAAAB (IP: ::ffff:10.0.2.15)][]
[1526461508.077][2018-05-16 05:05:08][osboxes][Cronicle][debug][4][Socket client 2OlavseF7YrCqhoRAAAB has authenticated via user session (IP: ::ffff:10.0.2.15)][]
[1526461560.095][2018-05-16 05:06:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:06:00][]
[1526461614.967][2018-05-16 05:06:54][osboxes][Cronicle][debug][6][Creating new event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin"}]
[1526461614.989][2018-05-16 05:06:54][osboxes][Cronicle][debug][6][Successfully created event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin"}]
[1526461619.46][2018-05-16 05:06:59][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461619,"source":"Manual (admin)"}]
[1526461619.464][2018-05-16 05:06:59][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526461619.464][2018-05-16 05:06:59][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526461619.47][2018-05-16 05:06:59][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461619,"source":"Manual (admin)","id":"jjh8vu3nt02","time_start":1526461619.465,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vu3nt02.log"}]
[1526461619.471][2018-05-16 05:06:59][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8vu3nt02","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vu3nt02.log","JOB_NOW":"1526461619","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526461619.465","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vu3nt02.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526461619.484][2018-05-16 05:06:59][osboxes][Cronicle][debug][3][Spawned child process: 6948 for job: jjh8vu3nt02][bin/shell-plugin.js]
[1526461620.276][2018-05-16 05:07:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:07:00][]
[1526461635.003][2018-05-16 05:07:15][osboxes][Cronicle][debug][5][New socket.io client connected: DcuhRzZDHN7BCV0NAAAC (IP: ::ffff:10.0.2.15)][]
[1526461635.16][2018-05-16 05:07:15][osboxes][Cronicle][debug][5][Socket client DcuhRzZDHN7BCV0NAAAC (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vu3nt02.log][]
[1526461667.247][2018-05-16 05:07:47][osboxes][Cronicle][debug][5][Socket.io client disconnected: DcuhRzZDHN7BCV0NAAAC (IP: ::ffff:10.0.2.15)][]
[1526461671.332][2018-05-16 05:07:51][osboxes][Cronicle][debug][4][Aborting local job: jjh8vu3nt02: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461619,"source":"Manual (admin)","id":"jjh8vu3nt02","time_start":1526461619.465,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vu3nt02.log","pid":6948,"cpu":{"min":0.8999999999999999,"max":3.3,"total":8.4,"count":5,"current":0.8999999999999999},"mem":{"min":68730880,"max":68730880,"total":343654400,"count":5,"current":68730880}}]
[1526461680.382][2018-05-16 05:08:00][osboxes][Cronicle][debug][3][Child 6948 exited with code: 0][]
[1526461680.384][2018-05-16 05:08:00][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461619,"source":"Manual (admin)","id":"jjh8vu3nt02","time_start":1526461619.465,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vu3nt02.log","pid":6948,"cpu":{"min":0.8999999999999999,"max":3.3,"total":8.4,"count":5,"current":0.8999999999999999},"mem":{"min":68730880,"max":68730880,"total":343654400,"count":5,"current":68730880},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526461680.386][2018-05-16 05:08:00][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vu3nt02.log: jobs/jjh8vu3nt02/log.txt.gz][]
[1526461680.478][2018-05-16 05:08:00][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8vu3nt02/log.txt.gz][]
[1526461680.478][2018-05-16 05:08:00][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vu3nt02.log][]
[1526461680.481][2018-05-16 05:08:00][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vu3nt02.log][]
[1526461680.508][2018-05-16 05:08:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:08:00][]
[1526461693.39][2018-05-16 05:08:13][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461693,"source":"Manual (admin)"}]
[1526461693.391][2018-05-16 05:08:13][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526461693.391][2018-05-16 05:08:13][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526461693.392][2018-05-16 05:08:13][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461693,"source":"Manual (admin)","id":"jjh8vvopc03","time_start":1526461693.392,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vvopc03.log"}]
[1526461693.393][2018-05-16 05:08:13][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8vvopc03","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vvopc03.log","JOB_NOW":"1526461693","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526461693.392","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vvopc03.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526461693.396][2018-05-16 05:08:13][osboxes][Cronicle][debug][3][Spawned child process: 7074 for job: jjh8vvopc03][bin/shell-plugin.js]
[1526461740.796][2018-05-16 05:09:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:09:00][]
[1526461777.233][2018-05-16 05:09:37][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461777,"source":"Manual (admin)"}]
[1526461800.212][2018-05-16 05:10:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:10:00][]
[1526461822.38][2018-05-16 05:10:22][osboxes][Cronicle][debug][4][Aborting local job: jjh8vvopc03: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461693,"source":"Manual (admin)","id":"jjh8vvopc03","time_start":1526461693.392,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vvopc03.log","pid":7074,"cpu":{"min":0.6,"max":3.4000000000000004,"total":13.699999999999998,"count":12,"current":0.6},"mem":{"min":67620864,"max":67620864,"total":811450368,"count":12,"current":67620864}}]
[1526461831.416][2018-05-16 05:10:31][osboxes][Cronicle][debug][3][Child 7074 exited with code: 0][]
[1526461831.416][2018-05-16 05:10:31][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461693,"source":"Manual (admin)","id":"jjh8vvopc03","time_start":1526461693.392,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vvopc03.log","pid":7074,"cpu":{"min":0.6,"max":3.4000000000000004,"total":14.299999999999997,"count":13,"current":0.6},"mem":{"min":67620864,"max":67780608,"total":879230976,"count":13,"current":67780608},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526461831.417][2018-05-16 05:10:31][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vvopc03.log: jobs/jjh8vvopc03/log.txt.gz][]
[1526461831.448][2018-05-16 05:10:31][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8vvopc03/log.txt.gz][]
[1526461831.448][2018-05-16 05:10:31][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vvopc03.log][]
[1526461831.449][2018-05-16 05:10:31][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vvopc03.log][]
[1526461851.332][2018-05-16 05:10:51][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461851,"source":"Manual (admin)"}]
[1526461851.335][2018-05-16 05:10:51][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526461851.335][2018-05-16 05:10:51][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526461851.335][2018-05-16 05:10:51][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461851,"source":"Manual (admin)","id":"jjh8vz2kn04","time_start":1526461851.335,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vz2kn04.log"}]
[1526461851.337][2018-05-16 05:10:51][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8vz2kn04","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vz2kn04.log","JOB_NOW":"1526461851","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526461851.335","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vz2kn04.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526461851.343][2018-05-16 05:10:51][osboxes][Cronicle][debug][3][Spawned child process: 7356 for job: jjh8vz2kn04][bin/shell-plugin.js]
[1526461855.098][2018-05-16 05:10:55][osboxes][Cronicle][debug][3][Child 7356 exited with code: 0][]
[1526461855.098][2018-05-16 05:10:55][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461851,"source":"Manual (admin)","id":"jjh8vz2kn04","time_start":1526461851.335,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vz2kn04.log","pid":7356,"jobid":"ForecastOptimizationjjh8vz2kn04","result":"Success","percentage":100,"progress":1,"complete":1,"code":0,"description":""}]
[1526461855.098][2018-05-16 05:10:55][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vz2kn04.log: jobs/jjh8vz2kn04/log.txt.gz][]
[1526461855.12][2018-05-16 05:10:55][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8vz2kn04/log.txt.gz][]
[1526461855.12][2018-05-16 05:10:55][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vz2kn04.log][]
[1526461855.121][2018-05-16 05:10:55][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vz2kn04.log][]
[1526461860.441][2018-05-16 05:11:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:11:00][]
[1526461920.586][2018-05-16 05:12:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:12:00][]
[1526461977.571][2018-05-16 05:12:57][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461977,"created":1526461614,"username":"admin"}]
[1526461977.598][2018-05-16 05:12:57][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526461980.75][2018-05-16 05:13:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:13:00][]
[1526461981.419][2018-05-16 05:13:01][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461977,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461981,"source":"Manual (admin)"}]
[1526461981.419][2018-05-16 05:13:01][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526461981.42][2018-05-16 05:13:01][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526461981.42][2018-05-16 05:13:01][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461981,"source":"Manual (admin)","id":"jjh8w1uy405","time_start":1526461981.42,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w1uy405.log"}]
[1526461981.42][2018-05-16 05:13:01][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8w1uy405","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w1uy405.log","JOB_NOW":"1526461981","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526461981.42","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w1uy405.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526461981.427][2018-05-16 05:13:01][osboxes][Cronicle][debug][3][Spawned child process: 7806 for job: jjh8w1uy405][bin/shell-plugin.js]
[1526461996.148][2018-05-16 05:13:16][osboxes][Cronicle][debug][5][New socket.io client connected: hNQQzE-_Tf5_PgufAAAD (IP: ::ffff:10.0.2.15)][]
[1526461996.232][2018-05-16 05:13:16][osboxes][Cronicle][debug][5][Socket client hNQQzE-_Tf5_PgufAAAD (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w1uy405.log][]
[1526462007.44][2018-05-16 05:13:27][osboxes][Cronicle][debug][5][Socket.io client disconnected: hNQQzE-_Tf5_PgufAAAD (IP: ::ffff:10.0.2.15)][]
[1526462011.237][2018-05-16 05:13:31][osboxes][Cronicle][debug][4][Aborting local job: jjh8w1uy405: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461981,"source":"Manual (admin)","id":"jjh8w1uy405","time_start":1526461981.42,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w1uy405.log","pid":7806,"cpu":{"min":1.8,"max":3.2,"total":5,"count":2,"current":1.8},"mem":{"min":68640768,"max":68640768,"total":137281536,"count":2,"current":68640768}}]
[1526462020.284][2018-05-16 05:13:40][osboxes][Cronicle][debug][3][Child 7806 exited with code: 0][]
[1526462020.285][2018-05-16 05:13:40][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461981,"source":"Manual (admin)","id":"jjh8w1uy405","time_start":1526461981.42,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w1uy405.log","pid":7806,"cpu":{"min":1.3,"max":3.2,"total":6.3,"count":3,"current":1.3},"mem":{"min":68640768,"max":68792320,"total":206073856,"count":3,"current":68792320},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526462020.285][2018-05-16 05:13:40][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w1uy405.log: jobs/jjh8w1uy405/log.txt.gz][]
[1526462020.334][2018-05-16 05:13:40][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8w1uy405/log.txt.gz][]
[1526462020.334][2018-05-16 05:13:40][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w1uy405.log][]
[1526462020.335][2018-05-16 05:13:40][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w1uy405.log][]
[1526462040.989][2018-05-16 05:14:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:14:00][]
[1526462100.147][2018-05-16 05:15:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:15:00][]
[1526462128.41][2018-05-16 05:15:28][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : latest\t\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462128,"created":1526461614,"username":"admin"}]
[1526462128.42][2018-05-16 05:15:28][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526462131.657][2018-05-16 05:15:31][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : latest\t\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462128,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462131,"source":"Manual (admin)"}]
[1526462131.661][2018-05-16 05:15:31][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526462131.661][2018-05-16 05:15:31][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526462131.661][2018-05-16 05:15:31][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : latest\t\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462131,"source":"Manual (admin)","id":"jjh8w52vh06","time_start":1526462131.661,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w52vh06.log"}]
[1526462131.662][2018-05-16 05:15:31][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8w52vh06","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w52vh06.log","JOB_NOW":"1526462131","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526462131.661","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w52vh06.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : latest\t\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526462131.665][2018-05-16 05:15:31][osboxes][Cronicle][debug][3][Spawned child process: 7996 for job: jjh8w52vh06][bin/shell-plugin.js]
[1526462132.136][2018-05-16 05:15:32][osboxes][Cronicle][debug][3][Child 7996 exited with code: 0][]
[1526462132.136][2018-05-16 05:15:32][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : latest\t\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462131,"source":"Manual (admin)","id":"jjh8w52vh06","time_start":1526462131.661,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w52vh06.log","pid":7996,"complete":1,"code":1,"description":"Script exited with code: 1: A problem occurred when sending our message","html":{"title":"Error Output","content":"<pre>A problem occurred when sending our message\nReferenceError: latest is not defined\n    at Producer.&lt;anonymous> (/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/bin/cronicle-script-temp-jjh8w52vh06.sh:43:29)\n    at Producer.emit (events.js:185:15)\n    at /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/client.js:212:12\n    at /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/client.js:348:7</pre>"}}]
[1526462132.136][2018-05-16 05:15:32][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w52vh06.log: jobs/jjh8w52vh06/log.txt.gz][]
[1526462132.152][2018-05-16 05:15:32][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8w52vh06/log.txt.gz][]
[1526462132.152][2018-05-16 05:15:32][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w52vh06.log][]
[1526462132.153][2018-05-16 05:15:32][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w52vh06.log][]
[1526462160.296][2018-05-16 05:16:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:16:00][]
[1526462203.599][2018-05-16 05:16:43][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462203,"created":1526461614,"username":"admin"}]
[1526462203.606][2018-05-16 05:16:43][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526462206.441][2018-05-16 05:16:46][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462203,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462206,"source":"Manual (admin)"}]
[1526462206.442][2018-05-16 05:16:46][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526462206.442][2018-05-16 05:16:46][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526462206.442][2018-05-16 05:16:46][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462206,"source":"Manual (admin)","id":"jjh8w6okq07","time_start":1526462206.442,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w6okq07.log"}]
[1526462206.443][2018-05-16 05:16:46][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8w6okq07","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w6okq07.log","JOB_NOW":"1526462206","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526462206.442","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w6okq07.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526462206.446][2018-05-16 05:16:46][osboxes][Cronicle][debug][3][Spawned child process: 8142 for job: jjh8w6okq07][bin/shell-plugin.js]
[1526462206.933][2018-05-16 05:16:46][osboxes][Cronicle][debug][3][Child 8142 exited with code: 0][]
[1526462206.934][2018-05-16 05:16:46][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462206,"source":"Manual (admin)","id":"jjh8w6okq07","time_start":1526462206.442,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w6okq07.log","pid":8142,"complete":1,"code":1,"description":"Script exited with code: 1: A problem occurred when sending our message","html":{"title":"Error Output","content":"<pre>A problem occurred when sending our message\nError: No such configuration property: \"auto.commit.reset\"\n    at KafkaConsumer.Client (/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/client.js:54:18)\n    at new KafkaConsumer (/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/kafka-consumer.js:117:10)\n    at Producer.&lt;anonymous> (/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/bin/cronicle-script-temp-jjh8w6okq07.sh:38:20)\n    at Producer.emit (events.js:185:15)\n    at /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/client.js:212:12\n    at /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/node_modules/node-rdkafka/lib/client.js:348:7</pre>"}}]
[1526462206.934][2018-05-16 05:16:46][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w6okq07.log: jobs/jjh8w6okq07/log.txt.gz][]
[1526462206.953][2018-05-16 05:16:46][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8w6okq07/log.txt.gz][]
[1526462206.953][2018-05-16 05:16:46][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w6okq07.log][]
[1526462206.954][2018-05-16 05:16:46][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w6okq07.log][]
[1526462220.594][2018-05-16 05:17:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:17:00][]
[1526462280.772][2018-05-16 05:18:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:18:00][]
[1526462340.915][2018-05-16 05:19:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:19:00][]
[1526462365.526][2018-05-16 05:19:25][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462365,"created":1526461614,"username":"admin"}]
[1526462365.534][2018-05-16 05:19:25][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526462368.291][2018-05-16 05:19:28][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462365,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462368,"source":"Manual (admin)"}]
[1526462368.292][2018-05-16 05:19:28][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526462368.292][2018-05-16 05:19:28][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526462368.293][2018-05-16 05:19:28][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462368,"source":"Manual (admin)","id":"jjh8wa5gk08","time_start":1526462368.292,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log"}]
[1526462368.293][2018-05-16 05:19:28][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8wa5gk08","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log","JOB_NOW":"1526462368","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526462368.292","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526462368.297][2018-05-16 05:19:28][osboxes][Cronicle][debug][3][Spawned child process: 8375 for job: jjh8wa5gk08][bin/shell-plugin.js]
[1526462375.276][2018-05-16 05:19:35][osboxes][Cronicle][debug][5][New socket.io client connected: R1XVb9UFXBmoQ25FAAAE (IP: ::ffff:10.0.2.15)][]
[1526462375.335][2018-05-16 05:19:35][osboxes][Cronicle][debug][5][Socket client R1XVb9UFXBmoQ25FAAAE (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log][]
[1526462378.929][2018-05-16 05:19:38][osboxes][Cronicle][debug][5][Socket.io client disconnected: R1XVb9UFXBmoQ25FAAAE (IP: ::ffff:10.0.2.15)][]
[1526462381.977][2018-05-16 05:19:41][osboxes][Cronicle][debug][5][New socket.io client connected: TclmZvmmwzcjJ_OpAAAF (IP: ::ffff:10.0.2.15)][]
[1526462382.058][2018-05-16 05:19:42][osboxes][Cronicle][debug][5][Socket client TclmZvmmwzcjJ_OpAAAF (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log][]
[1526462400.13][2018-05-16 05:20:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:20:00][]
[1526462418.726][2018-05-16 05:20:18][osboxes][Cronicle][debug][5][Socket.io client disconnected: TclmZvmmwzcjJ_OpAAAF (IP: ::ffff:10.0.2.15)][]
[1526462421.453][2018-05-16 05:20:21][osboxes][Cronicle][debug][4][Aborting local job: jjh8wa5gk08: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462368,"source":"Manual (admin)","id":"jjh8wa5gk08","time_start":1526462368.292,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log","pid":8375,"cpu":{"min":0.8999999999999999,"max":3.7,"total":9.200000000000001,"count":5,"current":0.8999999999999999},"mem":{"min":68816896,"max":68816896,"total":344084480,"count":5,"current":68816896}}]
[1526462430.429][2018-05-16 05:20:30][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin"}]
[1526462430.436][2018-05-16 05:20:30][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526462430.478][2018-05-16 05:20:30][osboxes][Cronicle][debug][3][Child 8375 exited with code: 0][]
[1526462430.478][2018-05-16 05:20:30][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462368,"source":"Manual (admin)","id":"jjh8wa5gk08","time_start":1526462368.292,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log","pid":8375,"cpu":{"min":0.8,"max":3.7,"total":10.000000000000002,"count":6,"current":0.8},"mem":{"min":68816896,"max":68964352,"total":413048832,"count":6,"current":68964352},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526462430.479][2018-05-16 05:20:30][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log: jobs/jjh8wa5gk08/log.txt.gz][]
[1526462430.537][2018-05-16 05:20:30][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8wa5gk08/log.txt.gz][]
[1526462430.537][2018-05-16 05:20:30][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log][]
[1526462430.538][2018-05-16 05:20:30][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log][]
[1526462434.073][2018-05-16 05:20:34][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462434,"source":"Manual (admin)"}]
[1526462434.074][2018-05-16 05:20:34][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526462434.074][2018-05-16 05:20:34][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526462434.075][2018-05-16 05:20:34][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462434,"source":"Manual (admin)","id":"jjh8wbk7u09","time_start":1526462434.075,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log"}]
[1526462434.077][2018-05-16 05:20:34][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8wbk7u09","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log","JOB_NOW":"1526462434","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526462434.075","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526462434.08][2018-05-16 05:20:34][osboxes][Cronicle][debug][3][Spawned child process: 8541 for job: jjh8wbk7u09][bin/shell-plugin.js]
[1526462437.939][2018-05-16 05:20:37][osboxes][Cronicle][debug][5][New socket.io client connected: hLi7xvezN_0c60O1AAAG (IP: ::ffff:10.0.2.15)][]
[1526462438.018][2018-05-16 05:20:38][osboxes][Cronicle][debug][5][Socket client hLi7xvezN_0c60O1AAAG (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log][]
[1526462460.329][2018-05-16 05:21:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:21:00][]
[1526462464.511][2018-05-16 05:21:04][osboxes][Cronicle][debug][5][Socket.io client disconnected: hLi7xvezN_0c60O1AAAG (IP: ::ffff:10.0.2.15)][]
[1526462467.47][2018-05-16 05:21:07][osboxes][Cronicle][debug][5][New socket.io client connected: W96ZxuLTjM6L-bf9AAAH (IP: ::ffff:10.0.2.15)][]
[1526462467.564][2018-05-16 05:21:07][osboxes][Cronicle][debug][5][Socket client W96ZxuLTjM6L-bf9AAAH (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log][]
[1526462514.306][2018-05-16 05:21:54][osboxes][Cronicle][debug][5][Socket.io client disconnected: W96ZxuLTjM6L-bf9AAAH (IP: ::ffff:10.0.2.15)][]
[1526462520.517][2018-05-16 05:22:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:22:00][]
[1526462531.253][2018-05-16 05:22:11][osboxes][Cronicle][debug][5][New socket.io client connected: QOs_g6p5AQ6kOjDIAAAI (IP: ::ffff:10.0.2.15)][]
[1526462531.344][2018-05-16 05:22:11][osboxes][Cronicle][debug][5][Socket client QOs_g6p5AQ6kOjDIAAAI (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log][]
[1526462580.83][2018-05-16 05:23:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:23:00][]
[1526462584.151][2018-05-16 05:23:04][osboxes][Cronicle][debug][5][Socket.io client disconnected: QOs_g6p5AQ6kOjDIAAAI (IP: ::ffff:10.0.2.15)][]
[1526462589.822][2018-05-16 05:23:09][osboxes][Cronicle][debug][4][Aborting local job: jjh8wbk7u09: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462434,"source":"Manual (admin)","id":"jjh8wbk7u09","time_start":1526462434.075,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log","pid":8541,"cpu":{"min":0.5,"max":6,"total":16.9,"count":15,"current":0.5},"mem":{"min":68698112,"max":68698112,"total":1030471680,"count":15,"current":68698112}}]
[1526462598.867][2018-05-16 05:23:18][osboxes][Cronicle][debug][3][Child 8541 exited with code: 0][]
[1526462598.868][2018-05-16 05:23:18][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462434,"source":"Manual (admin)","id":"jjh8wbk7u09","time_start":1526462434.075,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log","pid":8541,"cpu":{"min":0.5,"max":6,"total":17.4,"count":16,"current":0.5},"mem":{"min":68698112,"max":68739072,"total":1099210752,"count":16,"current":68739072},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526462598.868][2018-05-16 05:23:18][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log: jobs/jjh8wbk7u09/log.txt.gz][]
[1526462598.899][2018-05-16 05:23:18][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8wbk7u09/log.txt.gz][]
[1526462598.902][2018-05-16 05:23:18][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log][]
[1526462598.903][2018-05-16 05:23:18][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log][]
[1526462602.34][2018-05-16 05:23:22][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462602,"source":"Manual (admin)"}]
[1526462602.341][2018-05-16 05:23:22][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526462602.341][2018-05-16 05:23:22][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526462602.341][2018-05-16 05:23:22][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462602,"source":"Manual (admin)","id":"jjh8wf61x0a","time_start":1526462602.341,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wf61x0a.log"}]
[1526462602.341][2018-05-16 05:23:22][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8wf61x0a","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wf61x0a.log","JOB_NOW":"1526462602","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526462602.341","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wf61x0a.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526462602.346][2018-05-16 05:23:22][osboxes][Cronicle][debug][3][Spawned child process: 8784 for job: jjh8wf61x0a][bin/shell-plugin.js]
[1526462618.813][2018-05-16 05:23:38][osboxes][Cronicle][debug][4][Aborting local job: jjh8wf61x0a: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462602,"source":"Manual (admin)","id":"jjh8wf61x0a","time_start":1526462602.341,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wf61x0a.log","pid":8784,"cpu":{"min":3.7,"max":3.7,"total":3.7,"count":1,"current":3.7},"mem":{"min":68980736,"max":68980736,"total":68980736,"count":1,"current":68980736}}]
[1526462627.856][2018-05-16 05:23:47][osboxes][Cronicle][debug][3][Child 8784 exited with code: 0][]
[1526462627.856][2018-05-16 05:23:47][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462602,"source":"Manual (admin)","id":"jjh8wf61x0a","time_start":1526462602.341,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wf61x0a.log","pid":8784,"cpu":{"min":2,"max":3.7,"total":5.7,"count":2,"current":2},"mem":{"min":68980736,"max":69128192,"total":138108928,"count":2,"current":69128192},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526462627.858][2018-05-16 05:23:47][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wf61x0a.log: jobs/jjh8wf61x0a/log.txt.gz][]
[1526462627.922][2018-05-16 05:23:47][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8wf61x0a/log.txt.gz][]
[1526462627.922][2018-05-16 05:23:47][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wf61x0a.log][]
[1526462627.923][2018-05-16 05:23:47][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wf61x0a.log][]
[1526462640.198][2018-05-16 05:24:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:24:00][]
[1526462695.566][2018-05-16 05:24:55][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462695,"source":"Manual (admin)"}]
[1526462695.567][2018-05-16 05:24:55][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526462695.567][2018-05-16 05:24:55][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526462695.567][2018-05-16 05:24:55][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462695,"source":"Manual (admin)","id":"jjh8wh5zj0b","time_start":1526462695.567,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wh5zj0b.log"}]
[1526462695.567][2018-05-16 05:24:55][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8wh5zj0b","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wh5zj0b.log","JOB_NOW":"1526462695","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526462695.567","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wh5zj0b.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526462695.572][2018-05-16 05:24:55][osboxes][Cronicle][debug][3][Spawned child process: 8995 for job: jjh8wh5zj0b][bin/shell-plugin.js]
[1526462700.391][2018-05-16 05:25:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:25:00][]
[1526462702.406][2018-05-16 05:25:02][osboxes][Cronicle][debug][3][Child 8995 exited with code: 0][]
[1526462702.406][2018-05-16 05:25:02][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462695,"source":"Manual (admin)","id":"jjh8wh5zj0b","time_start":1526462695.567,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wh5zj0b.log","pid":8995,"jobid":"ForecastOptimizationjjh8wh5zj0b","result":"Success","percentage":100,"progress":1,"complete":1,"code":0,"description":""}]
[1526462702.406][2018-05-16 05:25:02][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wh5zj0b.log: jobs/jjh8wh5zj0b/log.txt.gz][]
[1526462702.433][2018-05-16 05:25:02][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8wh5zj0b/log.txt.gz][]
[1526462702.433][2018-05-16 05:25:02][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wh5zj0b.log][]
[1526462702.436][2018-05-16 05:25:02][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wh5zj0b.log][]
[1526462760.636][2018-05-16 05:26:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:26:00][]
[1526462820.789][2018-05-16 05:27:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:27:00][]
[1526462880.951][2018-05-16 05:28:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:28:00][]
[1526462900.716][2018-05-16 05:28:20][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462900,"source":"Manual (admin)"}]
[1526462900.717][2018-05-16 05:28:20][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526462900.717][2018-05-16 05:28:20][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526462900.717][2018-05-16 05:28:20][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462900,"source":"Manual (admin)","id":"jjh8wlka50c","time_start":1526462900.717,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlka50c.log"}]
[1526462900.717][2018-05-16 05:28:20][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8wlka50c","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlka50c.log","JOB_NOW":"1526462900","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526462900.717","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlka50c.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526462900.721][2018-05-16 05:28:20][osboxes][Cronicle][debug][3][Spawned child process: 9195 for job: jjh8wlka50c][bin/shell-plugin.js]
[1526462906.69][2018-05-16 05:28:26][osboxes][Cronicle][debug][3][Child 9195 exited with code: 0][]
[1526462906.69][2018-05-16 05:28:26][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462900,"source":"Manual (admin)","id":"jjh8wlka50c","time_start":1526462900.717,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlka50c.log","pid":9195,"jobid":"ForecastOptimizationjjh8wlka50c","result":"Success","percentage":100,"progress":1,"complete":1,"code":0,"description":""}]
[1526462906.69][2018-05-16 05:28:26][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlka50c.log: jobs/jjh8wlka50c/log.txt.gz][]
[1526462906.735][2018-05-16 05:28:26][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlka50c.log][]
[1526462906.735][2018-05-16 05:28:26][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8wlka50c/log.txt.gz][]
[1526462906.736][2018-05-16 05:28:26][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlka50c.log][]
[1526462920.041][2018-05-16 05:28:40][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462920,"source":"Manual (admin)"}]
[1526462920.042][2018-05-16 05:28:40][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526462920.042][2018-05-16 05:28:40][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526462920.043][2018-05-16 05:28:40][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462920,"source":"Manual (admin)","id":"jjh8wlz6z0d","time_start":1526462920.043,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlz6z0d.log"}]
[1526462920.043][2018-05-16 05:28:40][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8wlz6z0d","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlz6z0d.log","JOB_NOW":"1526462920","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526462920.043","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlz6z0d.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526462920.046][2018-05-16 05:28:40][osboxes][Cronicle][debug][3][Spawned child process: 9284 for job: jjh8wlz6z0d][bin/shell-plugin.js]
[1526462926.141][2018-05-16 05:28:46][osboxes][Cronicle][debug][5][New socket.io client connected: HdHYg6oy5kGylTPdAAAJ (IP: ::ffff:10.0.2.15)][]
[1526462926.232][2018-05-16 05:28:46][osboxes][Cronicle][debug][5][Socket client HdHYg6oy5kGylTPdAAAJ (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlz6z0d.log][]
[1526462937.131][2018-05-16 05:28:57][osboxes][Cronicle][debug][5][Socket.io client disconnected: HdHYg6oy5kGylTPdAAAJ (IP: ::ffff:10.0.2.15)][]
[1526462940.195][2018-05-16 05:29:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:29:00][]
[1526462940.359][2018-05-16 05:29:00][osboxes][Cronicle][debug][4][Aborting local job: jjh8wlz6z0d: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462920,"source":"Manual (admin)","id":"jjh8wlz6z0d","time_start":1526462920.043,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlz6z0d.log","pid":9284,"cpu":{"min":1.7,"max":3.3,"total":5,"count":2,"current":1.7},"mem":{"min":68939776,"max":68939776,"total":137879552,"count":2,"current":68939776}}]
[1526462949.391][2018-05-16 05:29:09][osboxes][Cronicle][debug][3][Child 9284 exited with code: 0][]
[1526462949.391][2018-05-16 05:29:09][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462920,"source":"Manual (admin)","id":"jjh8wlz6z0d","time_start":1526462920.043,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlz6z0d.log","pid":9284,"cpu":{"min":1.7,"max":3.3,"total":5,"count":2,"current":1.7},"mem":{"min":68939776,"max":68939776,"total":137879552,"count":2,"current":68939776},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526462949.392][2018-05-16 05:29:09][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlz6z0d.log: jobs/jjh8wlz6z0d/log.txt.gz][]
[1526462949.413][2018-05-16 05:29:09][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8wlz6z0d/log.txt.gz][]
[1526462949.413][2018-05-16 05:29:09][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlz6z0d.log][]
[1526462949.414][2018-05-16 05:29:09][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlz6z0d.log][]
[1526463000.424][2018-05-16 05:30:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:30:00][]
[1526463060.569][2018-05-16 05:31:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:31:00][]
[1526463120.695][2018-05-16 05:32:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:32:00][]
[1526463180.848][2018-05-16 05:33:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:33:00][]
[1526463213.425][2018-05-16 05:33:33][osboxes][Cronicle][debug][5][Socket.io client disconnected: 2OlavseF7YrCqhoRAAAB (IP: ::ffff:10.0.2.15)][]
[1526463240.979][2018-05-16 05:34:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:34:00][]
[1526463253.065][2018-05-16 05:34:13][osboxes][Cronicle][debug][5][New socket.io client connected: HXV8kxg0fTF0u4FrAAAK (IP: ::ffff:10.0.2.15)][]
[1526463253.158][2018-05-16 05:34:13][osboxes][Cronicle][debug][4][Socket client HXV8kxg0fTF0u4FrAAAK has authenticated via user session (IP: ::ffff:10.0.2.15)][]
[1526463253.237][2018-05-16 05:34:13][osboxes][Cronicle][debug][4][Socket client HXV8kxg0fTF0u4FrAAAK has authenticated via user session (IP: ::ffff:10.0.2.15)][]
[1526463257.535][2018-05-16 05:34:17][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463257,"source":"Manual (admin)"}]
[1526463257.541][2018-05-16 05:34:17][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526463257.541][2018-05-16 05:34:17][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526463257.541][2018-05-16 05:34:17][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463257,"source":"Manual (admin)","id":"jjh8wt7lx0e","time_start":1526463257.541,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wt7lx0e.log"}]
[1526463257.542][2018-05-16 05:34:17][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8wt7lx0e","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wt7lx0e.log","JOB_NOW":"1526463257","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526463257.541","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wt7lx0e.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526463257.546][2018-05-16 05:34:17][osboxes][Cronicle][debug][3][Spawned child process: 9637 for job: jjh8wt7lx0e][bin/shell-plugin.js]
[1526463300.136][2018-05-16 05:35:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:35:00][]
[1526463324.222][2018-05-16 05:35:24][osboxes][Cronicle][debug][5][New socket.io client connected: Un7WuL3aikQk96pAAAAL (IP: ::ffff:10.0.2.15)][]
[1526463324.368][2018-05-16 05:35:24][osboxes][Cronicle][debug][5][Socket client Un7WuL3aikQk96pAAAAL (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wt7lx0e.log][]
[1526463332.539][2018-05-16 05:35:32][osboxes][Cronicle][debug][5][Socket.io client disconnected: Un7WuL3aikQk96pAAAAL (IP: ::ffff:10.0.2.15)][]
[1526463337.248][2018-05-16 05:35:37][osboxes][Cronicle][debug][4][Aborting local job: jjh8wt7lx0e: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463257,"source":"Manual (admin)","id":"jjh8wt7lx0e","time_start":1526463257.541,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wt7lx0e.log","pid":9637,"cpu":{"min":0.5,"max":3.1,"total":8.600000000000001,"count":7,"current":0.5},"mem":{"min":68771840,"max":68771840,"total":481402880,"count":7,"current":68771840}}]
[1526463346.323][2018-05-16 05:35:46][osboxes][Cronicle][debug][3][Child 9637 exited with code: 0][]
[1526463346.323][2018-05-16 05:35:46][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463257,"source":"Manual (admin)","id":"jjh8wt7lx0e","time_start":1526463257.541,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wt7lx0e.log","pid":9637,"cpu":{"min":0.5,"max":3.1,"total":9.100000000000001,"count":8,"current":0.5},"mem":{"min":68771840,"max":68923392,"total":550326272,"count":8,"current":68923392},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526463346.323][2018-05-16 05:35:46][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wt7lx0e.log: jobs/jjh8wt7lx0e/log.txt.gz][]
[1526463346.371][2018-05-16 05:35:46][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8wt7lx0e/log.txt.gz][]
[1526463346.371][2018-05-16 05:35:46][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wt7lx0e.log][]
[1526463346.372][2018-05-16 05:35:46][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wt7lx0e.log][]
[1526463352.953][2018-05-16 05:35:52][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463352,"source":"Manual (admin)"}]
[1526463352.956][2018-05-16 05:35:52][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526463352.956][2018-05-16 05:35:52][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526463352.956][2018-05-16 05:35:52][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463352,"source":"Manual (admin)","id":"jjh8wv98c0f","time_start":1526463352.956,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wv98c0f.log"}]
[1526463352.957][2018-05-16 05:35:52][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8wv98c0f","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wv98c0f.log","JOB_NOW":"1526463352","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526463352.956","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wv98c0f.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526463352.961][2018-05-16 05:35:52][osboxes][Cronicle][debug][3][Spawned child process: 9796 for job: jjh8wv98c0f][bin/shell-plugin.js]
[1526463360.334][2018-05-16 05:36:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:36:00][]
[1526463375.446][2018-05-16 05:36:15][osboxes][Cronicle][debug][4][Aborting local job: jjh8wv98c0f: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463352,"source":"Manual (admin)","id":"jjh8wv98c0f","time_start":1526463352.956,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wv98c0f.log","pid":9796,"cpu":{"min":2,"max":3.6,"total":5.6,"count":2,"current":2},"mem":{"min":68857856,"max":68857856,"total":137715712,"count":2,"current":68857856}}]
[1526463384.471][2018-05-16 05:36:24][osboxes][Cronicle][debug][3][Child 9796 exited with code: 0][]
[1526463384.471][2018-05-16 05:36:24][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463352,"source":"Manual (admin)","id":"jjh8wv98c0f","time_start":1526463352.956,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wv98c0f.log","pid":9796,"cpu":{"min":1.4,"max":3.6,"total":7,"count":3,"current":1.4},"mem":{"min":68857856,"max":69005312,"total":206721024,"count":3,"current":69005312},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526463384.472][2018-05-16 05:36:24][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wv98c0f.log: jobs/jjh8wv98c0f/log.txt.gz][]
[1526463384.537][2018-05-16 05:36:24][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8wv98c0f/log.txt.gz][]
[1526463384.538][2018-05-16 05:36:24][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wv98c0f.log][]
[1526463384.539][2018-05-16 05:36:24][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wv98c0f.log][]
[1526463420.553][2018-05-16 05:37:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:37:00][]
[1526463460.859][2018-05-16 05:37:40][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463460,"source":"Manual (admin)"}]
[1526463460.86][2018-05-16 05:37:40][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526463460.86][2018-05-16 05:37:40][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526463460.861][2018-05-16 05:37:40][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463460,"source":"Manual (admin)","id":"jjh8wxkho0g","time_start":1526463460.86,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wxkho0g.log"}]
[1526463460.861][2018-05-16 05:37:40][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8wxkho0g","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wxkho0g.log","JOB_NOW":"1526463460","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526463460.86","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wxkho0g.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526463460.864][2018-05-16 05:37:40][osboxes][Cronicle][debug][3][Spawned child process: 10024 for job: jjh8wxkho0g][bin/shell-plugin.js]
[1526463469.934][2018-05-16 05:37:49][osboxes][Cronicle][debug][3][Child 10024 exited with code: 0][]
[1526463469.935][2018-05-16 05:37:49][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463460,"source":"Manual (admin)","id":"jjh8wxkho0g","time_start":1526463460.86,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wxkho0g.log","pid":10024,"jobid":"ForecastOptimizationjjh8wxkho0g","result":"Success","percentage":100,"progress":1,"complete":1,"code":0,"description":""}]
[1526463469.935][2018-05-16 05:37:49][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wxkho0g.log: jobs/jjh8wxkho0g/log.txt.gz][]
[1526463469.974][2018-05-16 05:37:49][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8wxkho0g/log.txt.gz][]
[1526463469.974][2018-05-16 05:37:49][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wxkho0g.log][]
[1526463469.988][2018-05-16 05:37:49][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wxkho0g.log][]
[1526463480.691][2018-05-16 05:38:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:38:00][]
[1526463540.836][2018-05-16 05:39:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:39:00][]
[1526463600.967][2018-05-16 05:40:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:40:00][]
[1526463612.481][2018-05-16 05:40:12][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463612,"source":"Manual (admin)"}]
[1526463612.486][2018-05-16 05:40:12][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526463612.486][2018-05-16 05:40:12][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526463612.486][2018-05-16 05:40:12][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463612,"source":"Manual (admin)","id":"jjh8x0thi0h","time_start":1526463612.486,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x0thi0h.log"}]
[1526463612.487][2018-05-16 05:40:12][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8x0thi0h","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x0thi0h.log","JOB_NOW":"1526463612","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526463612.486","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x0thi0h.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526463612.491][2018-05-16 05:40:12][osboxes][Cronicle][debug][3][Spawned child process: 10224 for job: jjh8x0thi0h][bin/shell-plugin.js]
[1526463646.763][2018-05-16 05:40:46][osboxes][Cronicle][debug][5][New socket.io client connected: 2EsF2RfZRWXwkbe8AAAM (IP: ::ffff:10.0.2.15)][]
[1526463646.832][2018-05-16 05:40:46][osboxes][Cronicle][debug][5][Socket client 2EsF2RfZRWXwkbe8AAAM (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x0thi0h.log][]
[1526463658.616][2018-05-16 05:40:58][osboxes][Cronicle][debug][5][Socket.io client disconnected: 2EsF2RfZRWXwkbe8AAAM (IP: ::ffff:10.0.2.15)][]
[1526463660.147][2018-05-16 05:41:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:41:00][]
[1526463685.113][2018-05-16 05:41:25][osboxes][Cronicle][debug][3][Child 10224 exited with code: 0][]
[1526463685.113][2018-05-16 05:41:25][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463612,"source":"Manual (admin)","id":"jjh8x0thi0h","time_start":1526463612.486,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x0thi0h.log","pid":10224,"cpu":{"min":0.7,"max":3.3,"total":9.899999999999999,"count":7,"current":0.7},"mem":{"min":67739648,"max":67739648,"total":474177536,"count":7,"current":67739648},"jobid":"ForecastOptimizationjjh8x0thi0h","result":"Success","percentage":100,"progress":1,"complete":1,"code":0,"description":""}]
[1526463685.113][2018-05-16 05:41:25][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x0thi0h.log: jobs/jjh8x0thi0h/log.txt.gz][]
[1526463685.13][2018-05-16 05:41:25][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8x0thi0h/log.txt.gz][]
[1526463685.13][2018-05-16 05:41:25][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x0thi0h.log][]
[1526463685.131][2018-05-16 05:41:25][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x0thi0h.log][]
[1526463720.32][2018-05-16 05:42:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:42:00][]
[1526463780.515][2018-05-16 05:43:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:43:00][]
[1526463840.674][2018-05-16 05:44:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:44:00][]
[1526463861.024][2018-05-16 05:44:21][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526463861,"created":1526461614,"username":"admin"}]
[1526463861.042][2018-05-16 05:44:21][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526463868.069][2018-05-16 05:44:28][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526463861,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463868,"source":"Manual (admin)"}]
[1526463868.07][2018-05-16 05:44:28][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526463868.071][2018-05-16 05:44:28][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526463868.071][2018-05-16 05:44:28][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463868,"source":"Manual (admin)","id":"jjh8x6ap30i","time_start":1526463868.071,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x6ap30i.log"}]
[1526463868.072][2018-05-16 05:44:28][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8x6ap30i","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x6ap30i.log","JOB_NOW":"1526463868","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526463868.071","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x6ap30i.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526463868.075][2018-05-16 05:44:28][osboxes][Cronicle][debug][3][Spawned child process: 10596 for job: jjh8x6ap30i][bin/shell-plugin.js]
[1526463893.881][2018-05-16 05:44:53][osboxes][Cronicle][debug][5][New socket.io client connected: 4qQkLyCnm1hKAkt8AAAN (IP: ::ffff:10.0.2.15)][]
[1526463893.957][2018-05-16 05:44:53][osboxes][Cronicle][debug][5][Socket client 4qQkLyCnm1hKAkt8AAAN (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x6ap30i.log][]
[1526463897.903][2018-05-16 05:44:57][osboxes][Cronicle][debug][5][Socket.io client disconnected: 4qQkLyCnm1hKAkt8AAAN (IP: ::ffff:10.0.2.15)][]
[1526463900.008][2018-05-16 05:45:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:45:00][]
[1526463903.97][2018-05-16 05:45:03][osboxes][Cronicle][debug][3][Child 10596 exited with code: 0][]
[1526463903.97][2018-05-16 05:45:03][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463868,"source":"Manual (admin)","id":"jjh8x6ap30i","time_start":1526463868.071,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x6ap30i.log","pid":10596,"cpu":{"min":1.2,"max":5.2,"total":10.299999999999999,"count":4,"current":1.2},"mem":{"min":68866048,"max":68866048,"total":275464192,"count":4,"current":68866048},"jobid":"ForecastOptimizationjjh8x6ap30i","result":"Success","percentage":100,"progress":1,"complete":1,"code":0,"description":""}]
[1526463903.971][2018-05-16 05:45:03][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x6ap30i.log: jobs/jjh8x6ap30i/log.txt.gz][]
[1526463904.002][2018-05-16 05:45:04][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8x6ap30i/log.txt.gz][]
[1526463904.002][2018-05-16 05:45:04][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x6ap30i.log][]
[1526463904.006][2018-05-16 05:45:04][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x6ap30i.log][]
[1526463932.974][2018-05-16 05:45:32][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526463861,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463932,"source":"Manual (admin)"}]
[1526463932.975][2018-05-16 05:45:32][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526463932.975][2018-05-16 05:45:32][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526463932.975][2018-05-16 05:45:32][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463932,"source":"Manual (admin)","id":"jjh8x7orz0j","time_start":1526463932.975,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x7orz0j.log"}]
[1526463932.98][2018-05-16 05:45:32][osboxes][Cronicle][debug][3][Spawned child process: 10745 for job: jjh8x7orz0j][bin/shell-plugin.js]
[1526463932.976][2018-05-16 05:45:32][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8x7orz0j","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x7orz0j.log","JOB_NOW":"1526463932","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526463932.975","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x7orz0j.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526463938.184][2018-05-16 05:45:38][osboxes][Cronicle][debug][3][Child 10745 exited with code: 0][]
[1526463938.184][2018-05-16 05:45:38][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463932,"source":"Manual (admin)","id":"jjh8x7orz0j","time_start":1526463932.975,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x7orz0j.log","pid":10745,"jobid":"ForecastOptimizationjjh8x7orz0j","result":"Success","percentage":100,"progress":1,"complete":1,"code":0,"description":""}]
[1526463938.185][2018-05-16 05:45:38][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x7orz0j.log: jobs/jjh8x7orz0j/log.txt.gz][]
[1526463938.218][2018-05-16 05:45:38][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8x7orz0j/log.txt.gz][]
[1526463938.218][2018-05-16 05:45:38][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x7orz0j.log][]
[1526463938.22][2018-05-16 05:45:38][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x7orz0j.log][]
[1526463953.372][2018-05-16 05:45:53][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526463861,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463953,"source":"Manual (admin)"}]
[1526463953.373][2018-05-16 05:45:53][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526463953.373][2018-05-16 05:45:53][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526463953.373][2018-05-16 05:45:53][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463953,"source":"Manual (admin)","id":"jjh8x84il0k","time_start":1526463953.373,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x84il0k.log"}]
[1526463953.373][2018-05-16 05:45:53][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8x84il0k","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x84il0k.log","JOB_NOW":"1526463953","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526463953.373","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x84il0k.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526463953.376][2018-05-16 05:45:53][osboxes][Cronicle][debug][3][Spawned child process: 10818 for job: jjh8x84il0k][bin/shell-plugin.js]
[1526463960.232][2018-05-16 05:46:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:46:00][]
[1526463964.078][2018-05-16 05:46:04][osboxes][Cronicle][debug][4][Aborting local job: jjh8x84il0k: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463953,"source":"Manual (admin)","id":"jjh8x84il0k","time_start":1526463953.373,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x84il0k.log","pid":10818}]
[1526463973.121][2018-05-16 05:46:13][osboxes][Cronicle][debug][3][Child 10818 exited with code: 0][]
[1526463973.122][2018-05-16 05:46:13][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463953,"source":"Manual (admin)","id":"jjh8x84il0k","time_start":1526463953.373,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x84il0k.log","pid":10818,"abort_reason":"Manually aborted by user: admin","cpu":{"min":3.2,"max":3.2,"total":3.2,"count":1,"current":3.2},"mem":{"min":67944448,"max":67944448,"total":67944448,"count":1,"current":67944448},"complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526463973.123][2018-05-16 05:46:13][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x84il0k.log: jobs/jjh8x84il0k/log.txt.gz][]
[1526463973.171][2018-05-16 05:46:13][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8x84il0k/log.txt.gz][]
[1526463973.173][2018-05-16 05:46:13][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x84il0k.log][]
[1526463973.179][2018-05-16 05:46:13][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x84il0k.log][]
[1526464020.631][2018-05-16 05:47:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:47:00][]
[1526464080.779][2018-05-16 05:48:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:48:00][]
[1526464130.442][2018-05-16 05:48:50][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'beginning'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526464130,"created":1526461614,"username":"admin"}]
[1526464130.448][2018-05-16 05:48:50][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526464133.67][2018-05-16 05:48:53][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'beginning'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526464130,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464133,"source":"Manual (admin)"}]
[1526464133.67][2018-05-16 05:48:53][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526464133.67][2018-05-16 05:48:53][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526464133.671][2018-05-16 05:48:53][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'beginning'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464133,"source":"Manual (admin)","id":"jjh8xbzmv0l","time_start":1526464133.671,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xbzmv0l.log"}]
[1526464133.671][2018-05-16 05:48:53][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8xbzmv0l","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xbzmv0l.log","JOB_NOW":"1526464133","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526464133.671","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xbzmv0l.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'beginning'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526464133.676][2018-05-16 05:48:53][osboxes][Cronicle][debug][3][Spawned child process: 11036 for job: jjh8xbzmv0l][bin/shell-plugin.js]
[1526464140.976][2018-05-16 05:49:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:49:00][]
[1526464143.923][2018-05-16 05:49:03][osboxes][Cronicle][debug][5][New socket.io client connected: 4zFJuT8wHsu5D9L8AAAO (IP: ::ffff:10.0.2.15)][]
[1526464144.039][2018-05-16 05:49:04][osboxes][Cronicle][debug][5][Socket client 4zFJuT8wHsu5D9L8AAAO (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xbzmv0l.log][]
[1526464147.03][2018-05-16 05:49:07][osboxes][Cronicle][debug][5][Socket.io client disconnected: 4zFJuT8wHsu5D9L8AAAO (IP: ::ffff:10.0.2.15)][]
[1526464149.356][2018-05-16 05:49:09][osboxes][Cronicle][debug][4][Aborting local job: jjh8xbzmv0l: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'beginning'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464133,"source":"Manual (admin)","id":"jjh8xbzmv0l","time_start":1526464133.671,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xbzmv0l.log","pid":11036,"cpu":{"min":3.8,"max":3.8,"total":3.8,"count":1,"current":3.8},"mem":{"min":68796416,"max":68796416,"total":68796416,"count":1,"current":68796416}}]
[1526464158.398][2018-05-16 05:49:18][osboxes][Cronicle][debug][3][Child 11036 exited with code: 0][]
[1526464158.398][2018-05-16 05:49:18][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'beginning'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464133,"source":"Manual (admin)","id":"jjh8xbzmv0l","time_start":1526464133.671,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xbzmv0l.log","pid":11036,"cpu":{"min":2.2,"max":3.8,"total":6,"count":2,"current":2.2},"mem":{"min":68796416,"max":68947968,"total":137744384,"count":2,"current":68947968},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526464158.4][2018-05-16 05:49:18][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xbzmv0l.log: jobs/jjh8xbzmv0l/log.txt.gz][]
[1526464158.478][2018-05-16 05:49:18][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8xbzmv0l/log.txt.gz][]
[1526464158.478][2018-05-16 05:49:18][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xbzmv0l.log][]
[1526464158.478][2018-05-16 05:49:18][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xbzmv0l.log][]
[1526464200.252][2018-05-16 05:50:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:50:00][]
[1526464260.519][2018-05-16 05:51:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:51:00][]
[1526464320.649][2018-05-16 05:52:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:52:00][]
[1526464380.777][2018-05-16 05:53:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:53:00][]
[1526464423.602][2018-05-16 05:53:43][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': false,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526464423,"created":1526461614,"username":"admin"}]
[1526464423.606][2018-05-16 05:53:43][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526464426.692][2018-05-16 05:53:46][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': false,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526464423,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464426,"source":"Manual (admin)"}]
[1526464426.693][2018-05-16 05:53:46][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526464426.693][2018-05-16 05:53:46][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526464426.693][2018-05-16 05:53:46][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': false,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464426,"source":"Manual (admin)","id":"jjh8xi9qd0m","time_start":1526464426.693,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xi9qd0m.log"}]
[1526464426.694][2018-05-16 05:53:46][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8xi9qd0m","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xi9qd0m.log","JOB_NOW":"1526464426","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526464426.693","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xi9qd0m.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': false,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526464426.698][2018-05-16 05:53:46][osboxes][Cronicle][debug][3][Spawned child process: 11407 for job: jjh8xi9qd0m][bin/shell-plugin.js]
[1526464429.147][2018-05-16 05:53:49][osboxes][Cronicle][debug][5][New socket.io client connected: cNuAU5OnXN6qOnv0AAAP (IP: ::ffff:10.0.2.15)][]
[1526464429.276][2018-05-16 05:53:49][osboxes][Cronicle][debug][5][Socket client cNuAU5OnXN6qOnv0AAAP (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xi9qd0m.log][]
[1526464440.986][2018-05-16 05:54:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:54:00][]
[1526464500.116][2018-05-16 05:55:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:55:00][]
[1526464560.311][2018-05-16 05:56:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:56:00][]
[1526464620.435][2018-05-16 05:57:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:57:00][]
[1526464680.645][2018-05-16 05:58:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:58:00][]
[1526464740.769][2018-05-16 05:59:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 05:59:00][]
[1526464800.897][2018-05-16 06:00:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:00:00][]
[1526464829.681][2018-05-16 06:00:29][osboxes][Cronicle][debug][5][Socket.io client disconnected: cNuAU5OnXN6qOnv0AAAP (IP: ::ffff:10.0.2.15)][]
[1526464832.993][2018-05-16 06:00:32][osboxes][Cronicle][debug][4][Aborting local job: jjh8xi9qd0m: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': false,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464426,"source":"Manual (admin)","id":"jjh8xi9qd0m","time_start":1526464426.693,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xi9qd0m.log","pid":11407,"cpu":{"min":0.3,"max":3.2,"total":20.700000000000017,"count":40,"current":0.3},"mem":{"min":67751936,"max":67751936,"total":2710077440,"count":40,"current":67751936}}]
[1526464842.031][2018-05-16 06:00:42][osboxes][Cronicle][debug][3][Child 11407 exited with code: 0][]
[1526464842.031][2018-05-16 06:00:42][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': false,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464426,"source":"Manual (admin)","id":"jjh8xi9qd0m","time_start":1526464426.693,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xi9qd0m.log","pid":11407,"cpu":{"min":0.3,"max":3.2,"total":21.000000000000018,"count":41,"current":0.3},"mem":{"min":67751936,"max":67903488,"total":2777980928,"count":41,"current":67903488},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526464842.032][2018-05-16 06:00:42][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xi9qd0m.log: jobs/jjh8xi9qd0m/log.txt.gz][]
[1526464842.057][2018-05-16 06:00:42][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8xi9qd0m/log.txt.gz][]
[1526464842.057][2018-05-16 06:00:42][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xi9qd0m.log][]
[1526464842.058][2018-05-16 06:00:42][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xi9qd0m.log][]
[1526464854.263][2018-05-16 06:00:54][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526464854,"created":1526461614,"username":"admin"}]
[1526464854.272][2018-05-16 06:00:54][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526464860.121][2018-05-16 06:01:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:01:00][]
[1526464864.404][2018-05-16 06:01:04][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526464854,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464864,"source":"Manual (admin)"}]
[1526464864.405][2018-05-16 06:01:04][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526464864.405][2018-05-16 06:01:04][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526464864.405][2018-05-16 06:01:04][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464864,"source":"Manual (admin)","id":"jjh8xrnh10n","time_start":1526464864.405,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xrnh10n.log"}]
[1526464864.406][2018-05-16 06:01:04][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8xrnh10n","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xrnh10n.log","JOB_NOW":"1526464864","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526464864.405","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xrnh10n.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526464864.409][2018-05-16 06:01:04][osboxes][Cronicle][debug][3][Spawned child process: 12353 for job: jjh8xrnh10n][bin/shell-plugin.js]
[1526464872.425][2018-05-16 06:01:12][osboxes][Cronicle][debug][5][New socket.io client connected: grffBa1biY-oiX8sAAAQ (IP: ::ffff:10.0.2.15)][]
[1526464872.484][2018-05-16 06:01:12][osboxes][Cronicle][debug][5][Socket client grffBa1biY-oiX8sAAAQ (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xrnh10n.log][]
[1526464898.903][2018-05-16 06:01:38][osboxes][Cronicle][debug][5][Socket.io client disconnected: grffBa1biY-oiX8sAAAQ (IP: ::ffff:10.0.2.15)][]
[1526464902.891][2018-05-16 06:01:42][osboxes][Cronicle][debug][4][Aborting local job: jjh8xrnh10n: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464864,"source":"Manual (admin)","id":"jjh8xrnh10n","time_start":1526464864.405,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xrnh10n.log","pid":12353,"cpu":{"min":3.9000000000000004,"max":6.5,"total":15,"count":3,"current":3.9000000000000004},"mem":{"min":72687616,"max":74321920,"total":219975680,"count":3,"current":72966144}}]
[1526464911.919][2018-05-16 06:01:51][osboxes][Cronicle][debug][3][Child 12353 exited with code: 0][]
[1526464911.919][2018-05-16 06:01:51][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464864,"source":"Manual (admin)","id":"jjh8xrnh10n","time_start":1526464864.405,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xrnh10n.log","pid":12353,"cpu":{"min":3.5999999999999996,"max":6.5,"total":18.6,"count":4,"current":3.5999999999999996},"mem":{"min":72687616,"max":74321920,"total":293122048,"count":4,"current":73146368},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526464911.92][2018-05-16 06:01:51][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xrnh10n.log: jobs/jjh8xrnh10n/log.txt.gz][]
[1526464911.981][2018-05-16 06:01:51][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8xrnh10n/log.txt.gz][]
[1526464911.981][2018-05-16 06:01:51][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xrnh10n.log][]
[1526464911.983][2018-05-16 06:01:51][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xrnh10n.log][]
[1526464920.279][2018-05-16 06:02:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:02:00][]
[1526464980.497][2018-05-16 06:03:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:03:00][]
[1526465040.674][2018-05-16 06:04:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:04:00][]
[1526465100.83][2018-05-16 06:05:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:05:00][]
[1526465161.001][2018-05-16 06:06:01][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:06:00][]
[1526465220.133][2018-05-16 06:07:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:07:00][]
[1526465262.465][2018-05-16 06:07:42][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465262,"created":1526461614,"username":"admin"}]
[1526465262.468][2018-05-16 06:07:42][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526465265.966][2018-05-16 06:07:45][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465262,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465265,"source":"Manual (admin)"}]
[1526465265.969][2018-05-16 06:07:45][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526465265.969][2018-05-16 06:07:45][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526465265.969][2018-05-16 06:07:45][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465265,"source":"Manual (admin)","id":"jjh8y09bl0o","time_start":1526465265.969,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log"}]
[1526465265.97][2018-05-16 06:07:45][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8y09bl0o","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log","JOB_NOW":"1526465265","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526465265.969","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526465265.972][2018-05-16 06:07:45][osboxes][Cronicle][debug][3][Spawned child process: 13066 for job: jjh8y09bl0o][bin/shell-plugin.js]
[1526465280.395][2018-05-16 06:08:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:08:00][]
[1526465283.002][2018-05-16 06:08:03][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465262,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465282,"source":"Manual (admin)"}]
[1526465283.002][2018-05-16 06:08:03][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526465283.002][2018-05-16 06:08:03][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526465283.003][2018-05-16 06:08:03][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465282,"source":"Manual (admin)","id":"jjh8y0mgr0p","time_start":1526465283.003,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y0mgr0p.log"}]
[1526465283.004][2018-05-16 06:08:03][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8y0mgr0p","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y0mgr0p.log","JOB_NOW":"1526465282","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526465283.003","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y0mgr0p.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526465283.007][2018-05-16 06:08:03][osboxes][Cronicle][debug][3][Spawned child process: 13115 for job: jjh8y0mgr0p][bin/shell-plugin.js]
[1526465286.419][2018-05-16 06:08:06][osboxes][Cronicle][debug][5][New socket.io client connected: MO_6nxvL-zjM2WVfAAAR (IP: ::ffff:10.0.2.15)][]
[1526465286.538][2018-05-16 06:08:06][osboxes][Cronicle][debug][5][Socket client MO_6nxvL-zjM2WVfAAAR (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log][]
[1526465295.274][2018-05-16 06:08:15][osboxes][Cronicle][debug][5][Socket.io client disconnected: MO_6nxvL-zjM2WVfAAAR (IP: ::ffff:10.0.2.15)][]
[1526465307.551][2018-05-16 06:08:27][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465262,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465307,"source":"Manual (admin)"}]
[1526465307.552][2018-05-16 06:08:27][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526465307.552][2018-05-16 06:08:27][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526465307.552][2018-05-16 06:08:27][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465307,"source":"Manual (admin)","id":"jjh8y15eo0q","time_start":1526465307.552,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y15eo0q.log"}]
[1526465307.553][2018-05-16 06:08:27][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8y15eo0q","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y15eo0q.log","JOB_NOW":"1526465307","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526465307.552","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y15eo0q.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526465307.569][2018-05-16 06:08:27][osboxes][Cronicle][debug][3][Spawned child process: 13176 for job: jjh8y15eo0q][bin/shell-plugin.js]
[1526465314.263][2018-05-16 06:08:34][osboxes][Cronicle][debug][5][New socket.io client connected: 1SIG92T7wd_uaMSUAAAS (IP: ::ffff:10.0.2.15)][]
[1526465314.358][2018-05-16 06:08:34][osboxes][Cronicle][debug][5][Socket client 1SIG92T7wd_uaMSUAAAS (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log][]
[1526465340.568][2018-05-16 06:09:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:09:00][]
[1526465367.358][2018-05-16 06:09:27][osboxes][Cronicle][debug][5][Socket.io client disconnected: 1SIG92T7wd_uaMSUAAAS (IP: ::ffff:10.0.2.15)][]
[1526465369.765][2018-05-16 06:09:29][osboxes][Cronicle][debug][5][New socket.io client connected: 7Yqi7nK5P0ilu0TMAAAT (IP: ::ffff:10.0.2.15)][]
[1526465369.841][2018-05-16 06:09:29][osboxes][Cronicle][debug][5][Socket client 7Yqi7nK5P0ilu0TMAAAT (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y15eo0q.log][]
[1526465371.523][2018-05-16 06:09:31][osboxes][Cronicle][debug][5][Socket.io client disconnected: 7Yqi7nK5P0ilu0TMAAAT (IP: ::ffff:10.0.2.15)][]
[1526465373.944][2018-05-16 06:09:33][osboxes][Cronicle][debug][5][New socket.io client connected: VmKRA4OMw6ToE4P4AAAU (IP: ::ffff:10.0.2.15)][]
[1526465373.987][2018-05-16 06:09:33][osboxes][Cronicle][debug][5][Socket client VmKRA4OMw6ToE4P4AAAU (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log][]
[1526465400.757][2018-05-16 06:10:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:10:00][]
[1526465412.777][2018-05-16 06:10:12][osboxes][Cronicle][debug][5][Socket.io client disconnected: VmKRA4OMw6ToE4P4AAAU (IP: ::ffff:10.0.2.15)][]
[1526465414.629][2018-05-16 06:10:14][osboxes][Cronicle][debug][5][New socket.io client connected: UEaSuU-D5lQRMq1DAAAV (IP: ::ffff:10.0.2.15)][]
[1526465414.7][2018-05-16 06:10:14][osboxes][Cronicle][debug][5][Socket client UEaSuU-D5lQRMq1DAAAV (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log][]
[1526465434.842][2018-05-16 06:10:34][osboxes][Cronicle][debug][5][Socket.io client disconnected: UEaSuU-D5lQRMq1DAAAV (IP: ::ffff:10.0.2.15)][]
[1526465460.058][2018-05-16 06:11:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:11:00][]
[1526465489.889][2018-05-16 06:11:29][osboxes][Cronicle][debug][4][Aborting local job: jjh8y15eo0q: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465307,"source":"Manual (admin)","id":"jjh8y15eo0q","time_start":1526465307.552,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y15eo0q.log","pid":13176,"cpu":{"min":0.5,"max":4.4,"total":17.1,"count":18,"current":0.5},"mem":{"min":68866048,"max":68866048,"total":1239588864,"count":18,"current":68866048}}]
[1526465491.923][2018-05-16 06:11:31][osboxes][Cronicle][debug][4][Aborting local job: jjh8y0mgr0p: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465282,"source":"Manual (admin)","id":"jjh8y0mgr0p","time_start":1526465283.003,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y0mgr0p.log","pid":13115,"cpu":{"min":0.4,"max":3.2,"total":17.099999999999998,"count":20,"current":0.4},"mem":{"min":68743168,"max":68743168,"total":1374863360,"count":20,"current":68743168},"jobid":"ForecastOptimizationjjh8y15eo0q","result":"Success","percentage":100}]
[1526465493.881][2018-05-16 06:11:33][osboxes][Cronicle][debug][4][Aborting local job: jjh8y09bl0o: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465265,"source":"Manual (admin)","id":"jjh8y09bl0o","time_start":1526465265.969,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log","pid":13066,"cpu":{"min":0.4,"max":4.2,"total":20.099999999999994,"count":22,"current":0.4},"mem":{"min":68542464,"max":68542464,"total":1507934208,"count":22,"current":68542464},"jobid":"ForecastOptimizationjjh8y15eo0q","result":"Success","percentage":100}]
[1526465498.919][2018-05-16 06:11:38][osboxes][Cronicle][debug][3][Child 13176 exited with code: 0][]
[1526465498.919][2018-05-16 06:11:38][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465307,"source":"Manual (admin)","id":"jjh8y15eo0q","time_start":1526465307.552,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y15eo0q.log","pid":13176,"cpu":{"min":0.5,"max":4.4,"total":17.6,"count":19,"current":0.5},"mem":{"min":68866048,"max":69013504,"total":1308602368,"count":19,"current":69013504},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526465498.92][2018-05-16 06:11:38][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y15eo0q.log: jobs/jjh8y15eo0q/log.txt.gz][]
[1526465498.968][2018-05-16 06:11:38][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8y15eo0q/log.txt.gz][]
[1526465498.968][2018-05-16 06:11:38][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y15eo0q.log][]
[1526465498.979][2018-05-16 06:11:38][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y15eo0q.log][]
[1526465500.962][2018-05-16 06:11:40][osboxes][Cronicle][debug][3][Child 13115 exited with code: 0][]
[1526465500.962][2018-05-16 06:11:40][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465282,"source":"Manual (admin)","id":"jjh8y0mgr0p","time_start":1526465283.003,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y0mgr0p.log","pid":13115,"cpu":{"min":0.4,"max":3.2,"total":17.499999999999996,"count":21,"current":0.4},"mem":{"min":68743168,"max":69001216,"total":1443864576,"count":21,"current":69001216},"jobid":"ForecastOptimizationjjh8y15eo0q","result":"Success","percentage":100,"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526465500.963][2018-05-16 06:11:40][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y0mgr0p.log: jobs/jjh8y0mgr0p/log.txt.gz][]
[1526465501.006][2018-05-16 06:11:41][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8y0mgr0p/log.txt.gz][]
[1526465501.006][2018-05-16 06:11:41][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y0mgr0p.log][]
[1526465501.007][2018-05-16 06:11:41][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y0mgr0p.log][]
[1526465502.935][2018-05-16 06:11:42][osboxes][Cronicle][debug][3][Child 13066 exited with code: 0][]
[1526465502.935][2018-05-16 06:11:42][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465265,"source":"Manual (admin)","id":"jjh8y09bl0o","time_start":1526465265.969,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log","pid":13066,"cpu":{"min":0.4,"max":4.2,"total":20.499999999999993,"count":23,"current":0.4},"mem":{"min":68542464,"max":68747264,"total":1576681472,"count":23,"current":68747264},"jobid":"ForecastOptimizationjjh8y15eo0q","result":"Success","percentage":100,"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526465502.936][2018-05-16 06:11:42][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log: jobs/jjh8y09bl0o/log.txt.gz][]
[1526465503][2018-05-16 06:11:43][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8y09bl0o/log.txt.gz][]
[1526465503][2018-05-16 06:11:43][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log][]
[1526465503.002][2018-05-16 06:11:43][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log][]
[1526465510.569][2018-05-16 06:11:50][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465510,"created":1526461614,"username":"admin"}]
[1526465510.573][2018-05-16 06:11:50][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526465513.97][2018-05-16 06:11:53][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465510,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465513,"source":"Manual (admin)"}]
[1526465513.972][2018-05-16 06:11:53][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526465513.972][2018-05-16 06:11:53][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526465513.973][2018-05-16 06:11:53][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465513,"source":"Manual (admin)","id":"jjh8y5kok0r","time_start":1526465513.972,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y5kok0r.log"}]
[1526465513.973][2018-05-16 06:11:53][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8y5kok0r","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y5kok0r.log","JOB_NOW":"1526465513","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526465513.972","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y5kok0r.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526465513.976][2018-05-16 06:11:53][osboxes][Cronicle][debug][3][Spawned child process: 13521 for job: jjh8y5kok0r][bin/shell-plugin.js]
[1526465520.354][2018-05-16 06:12:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:12:00][]
[1526465523.376][2018-05-16 06:12:03][osboxes][Cronicle][debug][5][New socket.io client connected: xptskW7TEUvDGBlcAAAW (IP: ::ffff:10.0.2.15)][]
[1526465523.54][2018-05-16 06:12:03][osboxes][Cronicle][debug][5][Socket client xptskW7TEUvDGBlcAAAW (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y5kok0r.log][]
[1526465529.842][2018-05-16 06:12:09][osboxes][Cronicle][debug][5][Socket.io client disconnected: xptskW7TEUvDGBlcAAAW (IP: ::ffff:10.0.2.15)][]
[1526465533.394][2018-05-16 06:12:13][osboxes][Cronicle][debug][4][Aborting local job: jjh8y5kok0r: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465513,"source":"Manual (admin)","id":"jjh8y5kok0r","time_start":1526465513.972,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y5kok0r.log","pid":13521,"cpu":{"min":6.7,"max":6.7,"total":6.7,"count":1,"current":6.7},"mem":{"min":73498624,"max":73498624,"total":73498624,"count":1,"current":73498624}}]
[1526465542.413][2018-05-16 06:12:22][osboxes][Cronicle][debug][3][Child 13521 exited with code: 0][]
[1526465542.413][2018-05-16 06:12:22][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465513,"source":"Manual (admin)","id":"jjh8y5kok0r","time_start":1526465513.972,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y5kok0r.log","pid":13521,"cpu":{"min":4.7,"max":6.7,"total":11.4,"count":2,"current":4.7},"mem":{"min":71909376,"max":73498624,"total":145408000,"count":2,"current":71909376},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526465542.414][2018-05-16 06:12:22][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y5kok0r.log: jobs/jjh8y5kok0r/log.txt.gz][]
[1526465542.436][2018-05-16 06:12:22][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8y5kok0r/log.txt.gz][]
[1526465542.436][2018-05-16 06:12:22][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y5kok0r.log][]
[1526465542.437][2018-05-16 06:12:22][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y5kok0r.log][]
[1526465563.89][2018-05-16 06:12:43][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465563,"created":1526461614,"username":"admin"}]
[1526465563.903][2018-05-16 06:12:43][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526465567.859][2018-05-16 06:12:47][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465563,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465567,"source":"Manual (admin)"}]
[1526465567.86][2018-05-16 06:12:47][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526465567.86][2018-05-16 06:12:47][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526465567.861][2018-05-16 06:12:47][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465567,"source":"Manual (admin)","id":"jjh8y6q9h0s","time_start":1526465567.861,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log"}]
[1526465567.862][2018-05-16 06:12:47][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8y6q9h0s","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log","JOB_NOW":"1526465567","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526465567.861","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526465567.865][2018-05-16 06:12:47][osboxes][Cronicle][debug][3][Spawned child process: 13806 for job: jjh8y6q9h0s][bin/shell-plugin.js]
[1526465573.491][2018-05-16 06:12:53][osboxes][Cronicle][debug][5][New socket.io client connected: yrzyw5KGnepaidHiAAAX (IP: ::ffff:10.0.2.15)][]
[1526465573.571][2018-05-16 06:12:53][osboxes][Cronicle][debug][5][Socket client yrzyw5KGnepaidHiAAAX (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log][]
[1526465576.217][2018-05-16 06:12:56][osboxes][Cronicle][debug][5][Socket.io client disconnected: yrzyw5KGnepaidHiAAAX (IP: ::ffff:10.0.2.15)][]
[1526465580.669][2018-05-16 06:13:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:13:00][]
[1526465582.564][2018-05-16 06:13:02][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465563,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465582,"source":"Manual (admin)"}]
[1526465582.565][2018-05-16 06:13:02][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526465582.565][2018-05-16 06:13:02][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526465582.565][2018-05-16 06:13:02][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465582,"source":"Manual (admin)","id":"jjh8y71lx0t","time_start":1526465582.565,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y71lx0t.log"}]
[1526465582.566][2018-05-16 06:13:02][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8y71lx0t","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y71lx0t.log","JOB_NOW":"1526465582","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526465582.565","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y71lx0t.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526465582.569][2018-05-16 06:13:02][osboxes][Cronicle][debug][3][Spawned child process: 13865 for job: jjh8y71lx0t][bin/shell-plugin.js]
[1526465586.014][2018-05-16 06:13:06][osboxes][Cronicle][debug][5][New socket.io client connected: mf404g7ZkAbzT6sQAAAY (IP: ::ffff:10.0.2.15)][]
[1526465586.057][2018-05-16 06:13:06][osboxes][Cronicle][debug][5][Socket client mf404g7ZkAbzT6sQAAAY (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log][]
[1526465626.37][2018-05-16 06:13:46][osboxes][Cronicle][debug][5][Socket.io client disconnected: mf404g7ZkAbzT6sQAAAY (IP: ::ffff:10.0.2.15)][]
[1526465627.432][2018-05-16 06:13:47][osboxes][Cronicle][debug][5][New socket.io client connected: pIVOl9Lg8Lg5lCK9AAAZ (IP: ::ffff:10.0.2.15)][]
[1526465627.516][2018-05-16 06:13:47][osboxes][Cronicle][debug][5][Socket client pIVOl9Lg8Lg5lCK9AAAZ (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y71lx0t.log][]
[1526465629.742][2018-05-16 06:13:49][osboxes][Cronicle][debug][5][Socket.io client disconnected: pIVOl9Lg8Lg5lCK9AAAZ (IP: ::ffff:10.0.2.15)][]
[1526465631.302][2018-05-16 06:13:51][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465563,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465631,"source":"Manual (admin)"}]
[1526465631.305][2018-05-16 06:13:51][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526465631.305][2018-05-16 06:13:51][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526465631.306][2018-05-16 06:13:51][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465631,"source":"Manual (admin)","id":"jjh8y837u0u","time_start":1526465631.306,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y837u0u.log"}]
[1526465631.306][2018-05-16 06:13:51][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8y837u0u","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y837u0u.log","JOB_NOW":"1526465631","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526465631.306","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y837u0u.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526465631.309][2018-05-16 06:13:51][osboxes][Cronicle][debug][3][Spawned child process: 13953 for job: jjh8y837u0u][bin/shell-plugin.js]
[1526465634.397][2018-05-16 06:13:54][osboxes][Cronicle][debug][5][New socket.io client connected: McZseArjqSDTRyBKAAAa (IP: ::ffff:10.0.2.15)][]
[1526465634.482][2018-05-16 06:13:54][osboxes][Cronicle][debug][5][Socket client McZseArjqSDTRyBKAAAa (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log][]
[1526465640.884][2018-05-16 06:14:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:14:00][]
[1526465700.098][2018-05-16 06:15:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:15:00][]
[1526465760.35][2018-05-16 06:16:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:16:00][]
[1526465820.496][2018-05-16 06:17:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:17:00][]
[1526465880.623][2018-05-16 06:18:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:18:00][]
[1526465934.407][2018-05-16 06:18:54][osboxes][Cronicle][debug][5][Socket.io client disconnected: McZseArjqSDTRyBKAAAa (IP: ::ffff:10.0.2.15)][]
[1526465940.826][2018-05-16 06:19:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:19:00][]
[1526465955.691][2018-05-16 06:19:15][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465955,"created":1526461614,"username":"admin"}]
[1526465955.697][2018-05-16 06:19:15][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526465960.341][2018-05-16 06:19:20][osboxes][Cronicle][debug][4][Aborting local job: jjh8y837u0u: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465631,"source":"Manual (admin)","id":"jjh8y837u0u","time_start":1526465631.306,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y837u0u.log","pid":13953,"cpu":{"min":0.3,"max":4.3,"total":20.00000000000001,"count":33,"current":0.3},"mem":{"min":68927488,"max":68927488,"total":2274607104,"count":33,"current":68927488}}]
[1526465962.144][2018-05-16 06:19:22][osboxes][Cronicle][debug][4][Aborting local job: jjh8y71lx0t: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465582,"source":"Manual (admin)","id":"jjh8y71lx0t","time_start":1526465582.565,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y71lx0t.log","pid":13865,"cpu":{"min":0.3,"max":5.4,"total":24.000000000000014,"count":38,"current":0.3},"mem":{"min":67837952,"max":67837952,"total":2577842176,"count":38,"current":67837952},"jobid":"ForecastOptimizationjjh8y837u0u","result":"Success","percentage":100}]
[1526465963.992][2018-05-16 06:19:23][osboxes][Cronicle][debug][4][Aborting local job: jjh8y6q9h0s: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465567,"source":"Manual (admin)","id":"jjh8y6q9h0s","time_start":1526465567.861,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log","pid":13806,"cpu":{"min":0.3,"max":3,"total":20.100000000000016,"count":39,"current":0.3},"mem":{"min":68911104,"max":68911104,"total":2687533056,"count":39,"current":68911104},"jobid":"ForecastOptimizationjjh8y837u0u","result":"Success","percentage":100}]
[1526465969.377][2018-05-16 06:19:29][osboxes][Cronicle][debug][3][Child 13953 exited with code: 0][]
[1526465969.378][2018-05-16 06:19:29][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465631,"source":"Manual (admin)","id":"jjh8y837u0u","time_start":1526465631.306,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y837u0u.log","pid":13953,"cpu":{"min":0.3,"max":4.3,"total":20.30000000000001,"count":34,"current":0.3},"mem":{"min":68927488,"max":69144576,"total":2343751680,"count":34,"current":69144576},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526465969.378][2018-05-16 06:19:29][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y837u0u.log: jobs/jjh8y837u0u/log.txt.gz][]
[1526465969.442][2018-05-16 06:19:29][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8y837u0u/log.txt.gz][]
[1526465969.442][2018-05-16 06:19:29][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y837u0u.log][]
[1526465969.443][2018-05-16 06:19:29][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y837u0u.log][]
[1526465971.192][2018-05-16 06:19:31][osboxes][Cronicle][debug][3][Child 13865 exited with code: 0][]
[1526465971.192][2018-05-16 06:19:31][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465582,"source":"Manual (admin)","id":"jjh8y71lx0t","time_start":1526465582.565,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y71lx0t.log","pid":13865,"cpu":{"min":0.3,"max":5.4,"total":24.300000000000015,"count":39,"current":0.3},"mem":{"min":67837952,"max":68034560,"total":2645876736,"count":39,"current":68034560},"jobid":"ForecastOptimizationjjh8y837u0u","result":"Success","percentage":100,"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526465971.192][2018-05-16 06:19:31][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y71lx0t.log: jobs/jjh8y71lx0t/log.txt.gz][]
[1526465971.234][2018-05-16 06:19:31][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8y71lx0t/log.txt.gz][]
[1526465971.238][2018-05-16 06:19:31][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y71lx0t.log][]
[1526465971.24][2018-05-16 06:19:31][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y71lx0t.log][]
[1526465973.028][2018-05-16 06:19:33][osboxes][Cronicle][debug][3][Child 13806 exited with code: 0][]
[1526465973.028][2018-05-16 06:19:33][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465567,"source":"Manual (admin)","id":"jjh8y6q9h0s","time_start":1526465567.861,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log","pid":13806,"cpu":{"min":0.3,"max":3,"total":20.400000000000016,"count":40,"current":0.3},"mem":{"min":68911104,"max":69115904,"total":2756648960,"count":40,"current":69115904},"jobid":"ForecastOptimizationjjh8y837u0u","result":"Success","percentage":100,"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526465973.029][2018-05-16 06:19:33][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log: jobs/jjh8y6q9h0s/log.txt.gz][]
[1526465973.092][2018-05-16 06:19:33][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8y6q9h0s/log.txt.gz][]
[1526465973.092][2018-05-16 06:19:33][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log][]
[1526465973.103][2018-05-16 06:19:33][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log][]
[1526465979.169][2018-05-16 06:19:39][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465955,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465979,"source":"Manual (admin)"}]
[1526465979.17][2018-05-16 06:19:39][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526465979.17][2018-05-16 06:19:39][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526465979.17][2018-05-16 06:19:39][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465979,"source":"Manual (admin)","id":"jjh8yfjmq0v","time_start":1526465979.17,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yfjmq0v.log"}]
[1526465979.171][2018-05-16 06:19:39][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8yfjmq0v","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yfjmq0v.log","JOB_NOW":"1526465979","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526465979.17","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yfjmq0v.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526465979.174][2018-05-16 06:19:39][osboxes][Cronicle][debug][3][Spawned child process: 14366 for job: jjh8yfjmq0v][bin/shell-plugin.js]
[1526466000.054][2018-05-16 06:20:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:20:00][]
[1526466004.2][2018-05-16 06:20:04][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465955,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466004,"source":"Manual (admin)"}]
[1526466004.201][2018-05-16 06:20:04][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526466004.201][2018-05-16 06:20:04][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526466004.201][2018-05-16 06:20:04][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466004,"source":"Manual (admin)","id":"jjh8yg2y10w","time_start":1526466004.201,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yg2y10w.log"}]
[1526466004.201][2018-05-16 06:20:04][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8yg2y10w","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yg2y10w.log","JOB_NOW":"1526466004","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526466004.201","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yg2y10w.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526466004.205][2018-05-16 06:20:04][osboxes][Cronicle][debug][3][Spawned child process: 14425 for job: jjh8yg2y10w][bin/shell-plugin.js]
[1526466006.43][2018-05-16 06:20:06][osboxes][Cronicle][debug][5][New socket.io client connected: FPgAQBdQfpJ-6Qv_AAAb (IP: ::ffff:10.0.2.15)][]
[1526466006.512][2018-05-16 06:20:06][osboxes][Cronicle][debug][5][Socket client FPgAQBdQfpJ-6Qv_AAAb (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yfjmq0v.log][]
[1526466011.445][2018-05-16 06:20:11][osboxes][Cronicle][debug][5][Socket.io client disconnected: FPgAQBdQfpJ-6Qv_AAAb (IP: ::ffff:10.0.2.15)][]
[1526466060.373][2018-05-16 06:21:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:21:00][]
[1526466120.543][2018-05-16 06:22:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:22:00][]
[1526466180.795][2018-05-16 06:23:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:23:00][]
[1526466240.976][2018-05-16 06:24:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:24:00][]
[1526466250.361][2018-05-16 06:24:10][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526466250,"created":1526461614,"username":"admin"}]
[1526466250.366][2018-05-16 06:24:10][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526466256.232][2018-05-16 06:24:16][osboxes][Cronicle][debug][4][Aborting local job: jjh8yg2y10w: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466004,"source":"Manual (admin)","id":"jjh8yg2y10w","time_start":1526466004.201,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yg2y10w.log","pid":14425,"cpu":{"min":0.3,"max":3,"total":17.400000000000002,"count":24,"current":0.3},"mem":{"min":68870144,"max":68870144,"total":1652883456,"count":24,"current":68870144}}]
[1526466258.291][2018-05-16 06:24:18][osboxes][Cronicle][debug][4][Aborting local job: jjh8yfjmq0v: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465979,"source":"Manual (admin)","id":"jjh8yfjmq0v","time_start":1526465979.17,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yfjmq0v.log","pid":14366,"cpu":{"min":0.3,"max":3.5,"total":17.900000000000006,"count":27,"current":0.3},"mem":{"min":67837952,"max":67837952,"total":1831624704,"count":27,"current":67837952},"value":{"type":"Buffer","data":[123,34,116,105,109,101,111,117,116,34,58,32,51,54,48,48,44,32,34,106,111,98,105,100,34,58,32,34,70,111,114,101,99,97,115,116,79,112,116,105,109,105,122,97,116,105,111,110,106,106,104,56,121,103,50,121,49,48,119,34,44,32,34,114,101,115,117,108,116,34,58,32,34,83,117,99,99,101,115,115,34,44,32,34,112,101,114,99,101,110,116,97,103,101,34,58,32,49,48,48,125]},"size":101,"key":null,"topic":"TriggerForecastOptimizationResponse","offset":86,"partition":0,"timestamp":1526466005905,"jobid":"ForecastOptimizationjjh8yg2y10w","result":"Success","percentage":100}]
[1526466265.265][2018-05-16 06:24:25][osboxes][Cronicle][debug][3][Child 14425 exited with code: 0][]
[1526466265.265][2018-05-16 06:24:25][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466004,"source":"Manual (admin)","id":"jjh8yg2y10w","time_start":1526466004.201,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yg2y10w.log","pid":14425,"cpu":{"min":0.3,"max":3,"total":17.700000000000003,"count":25,"current":0.3},"mem":{"min":68870144,"max":69033984,"total":1721917440,"count":25,"current":69033984},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526466265.275][2018-05-16 06:24:25][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yg2y10w.log: jobs/jjh8yg2y10w/log.txt.gz][]
[1526466265.378][2018-05-16 06:24:25][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8yg2y10w/log.txt.gz][]
[1526466265.379][2018-05-16 06:24:25][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yg2y10w.log][]
[1526466265.383][2018-05-16 06:24:25][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yg2y10w.log][]
[1526466267.317][2018-05-16 06:24:27][osboxes][Cronicle][debug][3][Child 14366 exited with code: 0][]
[1526466267.317][2018-05-16 06:24:27][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465979,"source":"Manual (admin)","id":"jjh8yfjmq0v","time_start":1526465979.17,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yfjmq0v.log","pid":14366,"cpu":{"min":0.3,"max":3.5,"total":18.200000000000006,"count":28,"current":0.3},"mem":{"min":67837952,"max":67891200,"total":1899515904,"count":28,"current":67891200},"value":{"type":"Buffer","data":[123,34,116,105,109,101,111,117,116,34,58,32,51,54,48,48,44,32,34,106,111,98,105,100,34,58,32,34,70,111,114,101,99,97,115,116,79,112,116,105,109,105,122,97,116,105,111,110,106,106,104,56,121,103,50,121,49,48,119,34,44,32,34,114,101,115,117,108,116,34,58,32,34,83,117,99,99,101,115,115,34,44,32,34,112,101,114,99,101,110,116,97,103,101,34,58,32,49,48,48,125]},"size":101,"key":null,"topic":"TriggerForecastOptimizationResponse","offset":86,"partition":0,"timestamp":1526466005905,"jobid":"ForecastOptimizationjjh8yg2y10w","result":"Success","percentage":100,"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526466267.318][2018-05-16 06:24:27][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yfjmq0v.log: jobs/jjh8yfjmq0v/log.txt.gz][]
[1526466267.366][2018-05-16 06:24:27][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8yfjmq0v/log.txt.gz][]
[1526466267.366][2018-05-16 06:24:27][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yfjmq0v.log][]
[1526466267.368][2018-05-16 06:24:27][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yfjmq0v.log][]
[1526466293.579][2018-05-16 06:24:53][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526466293,"created":1526461614,"username":"admin"}]
[1526466293.584][2018-05-16 06:24:53][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526466297.235][2018-05-16 06:24:57][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526466293,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466297,"source":"Manual (admin)"}]
[1526466297.236][2018-05-16 06:24:57][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526466297.236][2018-05-16 06:24:57][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526466297.236][2018-05-16 06:24:57][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466297,"source":"Manual (admin)","id":"jjh8ymd1w0x","time_start":1526466297.236,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log"}]
[1526466297.237][2018-05-16 06:24:57][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8ymd1w0x","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log","JOB_NOW":"1526466297","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526466297.236","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526466297.249][2018-05-16 06:24:57][osboxes][Cronicle][debug][3][Spawned child process: 14945 for job: jjh8ymd1w0x][bin/shell-plugin.js]
[1526466299.358][2018-05-16 06:24:59][osboxes][Cronicle][debug][5][New socket.io client connected: u_ZIsSrsxF5iv-c4AAAc (IP: ::ffff:10.0.2.15)][]
[1526466299.41][2018-05-16 06:24:59][osboxes][Cronicle][debug][5][Socket client u_ZIsSrsxF5iv-c4AAAc (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log][]
[1526466300.195][2018-05-16 06:25:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:25:00][]
[1526466317.142][2018-05-16 06:25:17][osboxes][Cronicle][debug][5][Socket.io client disconnected: u_ZIsSrsxF5iv-c4AAAc (IP: ::ffff:10.0.2.15)][]
[1526466335.83][2018-05-16 06:25:35][osboxes][Cronicle][debug][5][New socket.io client connected: dG1weK1dG4mB2YeRAAAd (IP: ::ffff:10.0.2.15)][]
[1526466335.892][2018-05-16 06:25:35][osboxes][Cronicle][debug][5][Socket client dG1weK1dG4mB2YeRAAAd (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log][]
[1526466360.483][2018-05-16 06:26:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:26:00][]
[1526466420.606][2018-05-16 06:27:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:27:00][]
[1526466480.781][2018-05-16 06:28:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:28:00][]
[1526466503.715][2018-05-16 06:28:23][osboxes][Cronicle][debug][5][Socket.io client disconnected: dG1weK1dG4mB2YeRAAAd (IP: ::ffff:10.0.2.15)][]
[1526466506.378][2018-05-16 06:28:26][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526466293,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466506,"source":"Manual (admin)"}]
[1526466506.38][2018-05-16 06:28:26][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526466506.38][2018-05-16 06:28:26][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526466506.381][2018-05-16 06:28:26][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466506,"source":"Manual (admin)","id":"jjh8yqufh0y","time_start":1526466506.381,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yqufh0y.log"}]
[1526466506.381][2018-05-16 06:28:26][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8yqufh0y","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yqufh0y.log","JOB_NOW":"1526466506","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526466506.381","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yqufh0y.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526466506.384][2018-05-16 06:28:26][osboxes][Cronicle][debug][3][Spawned child process: 15344 for job: jjh8yqufh0y][bin/shell-plugin.js]
[1526466510.915][2018-05-16 06:28:30][osboxes][Cronicle][debug][3][Child 15344 exited with code: 0][]
[1526466510.915][2018-05-16 06:28:30][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466506,"source":"Manual (admin)","id":"jjh8yqufh0y","time_start":1526466506.381,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yqufh0y.log","pid":15344,"jobid":"asdfasdf345","result":"Success","percentage":100,"progress":1,"complete":1,"code":0,"description":""}]
[1526466510.915][2018-05-16 06:28:30][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yqufh0y.log: jobs/jjh8yqufh0y/log.txt.gz][]
[1526466510.956][2018-05-16 06:28:30][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8yqufh0y/log.txt.gz][]
[1526466510.956][2018-05-16 06:28:30][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yqufh0y.log][]
[1526466510.956][2018-05-16 06:28:30][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yqufh0y.log][]
[1526466540.967][2018-05-16 06:29:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:29:00][]
[1526466600.165][2018-05-16 06:30:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:30:00][]
[1526466660.356][2018-05-16 06:31:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:31:00][]
[1526466669.863][2018-05-16 06:31:09][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526466669,"created":1526461614,"username":"admin"}]
[1526466669.877][2018-05-16 06:31:09][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526466672.365][2018-05-16 06:31:12][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","now":1526466672,"source":"Manual (admin)"}]
[1526466672.367][2018-05-16 06:31:12][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526466672.368][2018-05-16 06:31:12][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526466672.37][2018-05-16 06:31:12][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","now":1526466672,"source":"Manual (admin)","id":"jjh8yuei90z","time_start":1526466672.369,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","plugin_title":"Shell Script","category_title":"General","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yuei90z.log"}]
[1526466672.371][2018-05-16 06:31:12][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8yuei90z","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yuei90z.log","JOB_NOW":"1526466672","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526466672.369","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_PLUGIN_TITLE":"Shell Script","JOB_CATEGORY_TITLE":"General","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yuei90z.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526466672.383][2018-05-16 06:31:12][osboxes][Cronicle][debug][3][Spawned child process: 15663 for job: jjh8yuei90z][bin/shell-plugin.js]
[1526466679.545][2018-05-16 06:31:19][osboxes][Cronicle][debug][5][New socket.io client connected: PHcwYgtUgQUGthXQAAAe (IP: ::ffff:10.0.2.15)][]
[1526466679.577][2018-05-16 06:31:19][osboxes][Cronicle][debug][5][Socket client PHcwYgtUgQUGthXQAAAe (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yuei90z.log][]
[1526466687.164][2018-05-16 06:31:27][osboxes][Cronicle][debug][3][Child 15663 exited with code: 0][]
[1526466687.164][2018-05-16 06:31:27][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","now":1526466672,"source":"Manual (admin)","id":"jjh8yuei90z","time_start":1526466672.369,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","plugin_title":"Shell Script","category_title":"General","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yuei90z.log","pid":15663,"cpu":{"min":6.3,"max":6.3,"total":6.3,"count":1,"current":6.3},"mem":{"min":69206016,"max":69206016,"total":69206016,"count":1,"current":69206016},"jobid":"asdfasdf345","result":"Success","percentage":100,"progress":1,"complete":1,"code":0,"description":""}]
[1526466687.165][2018-05-16 06:31:27][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yuei90z.log: jobs/jjh8yuei90z/log.txt.gz][]
[1526466687.194][2018-05-16 06:31:27][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8yuei90z/log.txt.gz][]
[1526466687.194][2018-05-16 06:31:27][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yuei90z.log][]
[1526466687.209][2018-05-16 06:31:27][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yuei90z.log][]
[1526466687.542][2018-05-16 06:31:27][osboxes][Cronicle][debug][5][Socket.io client disconnected: PHcwYgtUgQUGthXQAAAe (IP: ::ffff:10.0.2.15)][]
[1526466695.326][2018-05-16 06:31:35][osboxes][Cronicle][debug][4][Aborting local job: jjh8ymd1w0x: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466297,"source":"Manual (admin)","id":"jjh8ymd1w0x","time_start":1526466297.236,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log","pid":14945,"cpu":{"min":0.3,"max":3.0999999999999996,"total":20.600000000000016,"count":39,"current":0.3},"mem":{"min":68796416,"max":68796416,"total":2683060224,"count":39,"current":68796416}}]
[1526466704.343][2018-05-16 06:31:44][osboxes][Cronicle][debug][3][Child 14945 exited with code: 0][]
[1526466704.343][2018-05-16 06:31:44][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466297,"source":"Manual (admin)","id":"jjh8ymd1w0x","time_start":1526466297.236,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log","pid":14945,"cpu":{"min":0.3,"max":3.0999999999999996,"total":20.900000000000016,"count":40,"current":0.3},"mem":{"min":68796416,"max":69066752,"total":2752126976,"count":40,"current":69066752},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526466704.344][2018-05-16 06:31:44][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log: jobs/jjh8ymd1w0x/log.txt.gz][]
[1526466704.378][2018-05-16 06:31:44][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8ymd1w0x/log.txt.gz][]
[1526466704.378][2018-05-16 06:31:44][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log][]
[1526466704.379][2018-05-16 06:31:44][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log][]
[1526466715.817][2018-05-16 06:31:55][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526466669,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466715,"source":"Manual (admin)"}]
[1526466715.819][2018-05-16 06:31:55][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526466715.819][2018-05-16 06:31:55][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526466715.82][2018-05-16 06:31:55][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466715,"source":"Manual (admin)","id":"jjh8yvc1810","time_start":1526466715.82,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yvc1810.log"}]
[1526466715.821][2018-05-16 06:31:55][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8yvc1810","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yvc1810.log","JOB_NOW":"1526466715","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526466715.82","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yvc1810.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526466715.824][2018-05-16 06:31:55][osboxes][Cronicle][debug][3][Spawned child process: 15778 for job: jjh8yvc1810][bin/shell-plugin.js]
[1526466720.598][2018-05-16 06:32:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:32:00][]
[1526466724.898][2018-05-16 06:32:04][osboxes][Cronicle][debug][4][Aborting local job: jjh8yvc1810: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466715,"source":"Manual (admin)","id":"jjh8yvc1810","time_start":1526466715.82,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yvc1810.log","pid":15778}]
[1526466733.922][2018-05-16 06:32:13][osboxes][Cronicle][debug][3][Child 15778 exited with code: 0][]
[1526466733.922][2018-05-16 06:32:13][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466715,"source":"Manual (admin)","id":"jjh8yvc1810","time_start":1526466715.82,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yvc1810.log","pid":15778,"abort_reason":"Manually aborted by user: admin","cpu":{"min":3.3,"max":3.3,"total":3.3,"count":1,"current":3.3},"mem":{"min":69038080,"max":69038080,"total":69038080,"count":1,"current":69038080},"complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526466733.922][2018-05-16 06:32:13][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yvc1810.log: jobs/jjh8yvc1810/log.txt.gz][]
[1526466733.976][2018-05-16 06:32:13][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8yvc1810/log.txt.gz][]
[1526466733.977][2018-05-16 06:32:13][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yvc1810.log][]
[1526466733.978][2018-05-16 06:32:13][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yvc1810.log][]
[1526466780.878][2018-05-16 06:33:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:33:00][]
[1526466840.05][2018-05-16 06:34:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:34:00][]
[1526466900.194][2018-05-16 06:35:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:35:00][]
[1526466954.579][2018-05-16 06:35:54][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526466669,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466954,"source":"Manual (admin)"}]
[1526466954.581][2018-05-16 06:35:54][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526466954.581][2018-05-16 06:35:54][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526466954.581][2018-05-16 06:35:54][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466954,"source":"Manual (admin)","id":"jjh8z0g9h11","time_start":1526466954.581,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8z0g9h11.log"}]
[1526466954.582][2018-05-16 06:35:54][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8z0g9h11","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8z0g9h11.log","JOB_NOW":"1526466954","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526466954.581","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8z0g9h11.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526466954.589][2018-05-16 06:35:54][osboxes][Cronicle][debug][3][Spawned child process: 16137 for job: jjh8z0g9h11][bin/shell-plugin.js]
[1526466960.363][2018-05-16 06:36:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:36:00][]
[1526466962.399][2018-05-16 06:36:02][osboxes][Cronicle][debug][5][New socket.io client connected: 2CgIUgvZFSP3geUkAAAf (IP: ::ffff:10.0.2.15)][]
[1526466962.453][2018-05-16 06:36:02][osboxes][Cronicle][debug][5][Socket client 2CgIUgvZFSP3geUkAAAf (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8z0g9h11.log][]
[1526467020.495][2018-05-16 06:37:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:37:00][]
[1526467080.617][2018-05-16 06:38:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:38:00][]
[1526467140.783][2018-05-16 06:39:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:39:00][]
[1526467200.907][2018-05-16 06:40:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:40:00][]
[1526467260.02][2018-05-16 06:41:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:41:00][]
[1526467320.156][2018-05-16 06:42:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:42:00][]
[1526467380.309][2018-05-16 06:43:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:43:00][]
[1526467440.559][2018-05-16 06:44:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:44:00][]
[1526467500.709][2018-05-16 06:45:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:45:00][]
[1526467560.83][2018-05-16 06:46:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:46:00][]
[1526467620.969][2018-05-16 06:47:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:47:00][]
[1526467637.873][2018-05-16 06:47:17][osboxes][Cronicle][debug][5][Socket.io client disconnected: 2CgIUgvZFSP3geUkAAAf (IP: ::ffff:10.0.2.15)][]
[1526467640.341][2018-05-16 06:47:20][osboxes][Cronicle][debug][4][Aborting local job: jjh8z0g9h11: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466954,"source":"Manual (admin)","id":"jjh8z0g9h11","time_start":1526466954.581,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8z0g9h11.log","pid":16137,"cpu":{"min":0.2,"max":3.2,"total":26.200000000000003,"count":68,"current":0.2},"mem":{"min":69140480,"max":69140480,"total":4701552640,"count":68,"current":69140480}}]
[1526467648.495][2018-05-16 06:47:28][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin"}]
[1526467648.505][2018-05-16 06:47:28][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526467649.359][2018-05-16 06:47:29][osboxes][Cronicle][debug][3][Child 16137 exited with code: 0][]
[1526467649.359][2018-05-16 06:47:29][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466954,"source":"Manual (admin)","id":"jjh8z0g9h11","time_start":1526466954.581,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8z0g9h11.log","pid":16137,"cpu":{"min":0.2,"max":3.2,"total":26.400000000000002,"count":69,"current":0.2},"mem":{"min":69140480,"max":69464064,"total":4771016704,"count":69,"current":69464064},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526467649.359][2018-05-16 06:47:29][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8z0g9h11.log: jobs/jjh8z0g9h11/log.txt.gz][]
[1526467649.379][2018-05-16 06:47:29][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8z0g9h11.log][]
[1526467649.379][2018-05-16 06:47:29][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8z0g9h11/log.txt.gz][]
[1526467649.398][2018-05-16 06:47:29][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8z0g9h11.log][]
[1526467652.586][2018-05-16 06:47:32][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467652,"source":"Manual (admin)"}]
[1526467652.586][2018-05-16 06:47:32][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526467652.587][2018-05-16 06:47:32][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526467652.587][2018-05-16 06:47:32][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467652,"source":"Manual (admin)","id":"jjh8zfeuj12","time_start":1526467652.587,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log"}]
[1526467652.587][2018-05-16 06:47:32][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8zfeuj12","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log","JOB_NOW":"1526467652","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526467652.587","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526467652.59][2018-05-16 06:47:32][osboxes][Cronicle][debug][3][Spawned child process: 16860 for job: jjh8zfeuj12][bin/shell-plugin.js]
[1526467657.072][2018-05-16 06:47:37][osboxes][Cronicle][debug][5][New socket.io client connected: nwJeizs_9kE6AFQHAAAg (IP: ::ffff:10.0.2.15)][]
[1526467657.163][2018-05-16 06:47:37][osboxes][Cronicle][debug][5][Socket client nwJeizs_9kE6AFQHAAAg (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log][]
[1526467675.175][2018-05-16 06:47:55][osboxes][Cronicle][debug][5][Socket.io client disconnected: nwJeizs_9kE6AFQHAAAg (IP: ::ffff:10.0.2.15)][]
[1526467680.25][2018-05-16 06:48:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:48:00][]
[1526467683.695][2018-05-16 06:48:03][osboxes][Cronicle][debug][5][New socket.io client connected: acGhUxYrcZNbOawLAAAh (IP: ::ffff:10.0.2.15)][]
[1526467683.744][2018-05-16 06:48:03][osboxes][Cronicle][debug][5][Socket client acGhUxYrcZNbOawLAAAh (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log][]
[1526467703.579][2018-05-16 06:48:23][osboxes][Cronicle][debug][5][Socket.io client disconnected: acGhUxYrcZNbOawLAAAh (IP: ::ffff:10.0.2.15)][]
[1526467704.715][2018-05-16 06:48:24][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467704,"source":"Manual (admin)"}]
[1526467704.718][2018-05-16 06:48:24][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526467704.718][2018-05-16 06:48:24][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526467704.719][2018-05-16 06:48:24][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467704,"source":"Manual (admin)","id":"jjh8zgj2m13","time_start":1526467704.719,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zgj2m13.log"}]
[1526467704.719][2018-05-16 06:48:24][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8zgj2m13","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zgj2m13.log","JOB_NOW":"1526467704","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526467704.719","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zgj2m13.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526467704.722][2018-05-16 06:48:24][osboxes][Cronicle][debug][3][Spawned child process: 17049 for job: jjh8zgj2m13][bin/shell-plugin.js]
[1526467740.466][2018-05-16 06:49:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:49:00][]
[1526467779.062][2018-05-16 06:49:39][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467779,"source":"Manual (admin)"}]
[1526467779.063][2018-05-16 06:49:39][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526467779.063][2018-05-16 06:49:39][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526467779.063][2018-05-16 06:49:39][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467779,"source":"Manual (admin)","id":"jjh8zi4fr14","time_start":1526467779.063,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zi4fr14.log"}]
[1526467779.064][2018-05-16 06:49:39][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8zi4fr14","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zi4fr14.log","JOB_NOW":"1526467779","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526467779.063","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zi4fr14.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526467779.067][2018-05-16 06:49:39][osboxes][Cronicle][debug][3][Spawned child process: 17219 for job: jjh8zi4fr14][bin/shell-plugin.js]
[1526467800.731][2018-05-16 06:50:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:50:00][]
[1526467860.992][2018-05-16 06:51:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:51:00][]
[1526467900.316][2018-05-16 06:51:40][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467900,"source":"Manual (admin)"}]
[1526467900.318][2018-05-16 06:51:40][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526467900.318][2018-05-16 06:51:40][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526467900.318][2018-05-16 06:51:40][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467900,"source":"Manual (admin)","id":"jjh8zkpzy15","time_start":1526467900.318,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zkpzy15.log"}]
[1526467900.319][2018-05-16 06:51:40][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8zkpzy15","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zkpzy15.log","JOB_NOW":"1526467900","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526467900.318","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zkpzy15.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526467900.325][2018-05-16 06:51:40][osboxes][Cronicle][debug][3][Spawned child process: 17779 for job: jjh8zkpzy15][bin/shell-plugin.js]
[1526467920.252][2018-05-16 06:52:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:52:00][]
[1526467930.66][2018-05-16 06:52:10][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467930,"source":"Manual (admin)"}]
[1526467930.66][2018-05-16 06:52:10][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526467930.661][2018-05-16 06:52:10][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526467930.661][2018-05-16 06:52:10][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467930,"source":"Manual (admin)","id":"jjh8zldet16","time_start":1526467930.661,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zldet16.log"}]
[1526467930.661][2018-05-16 06:52:10][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8zldet16","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zldet16.log","JOB_NOW":"1526467930","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526467930.661","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zldet16.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526467930.665][2018-05-16 06:52:10][osboxes][Cronicle][debug][3][Spawned child process: 18056 for job: jjh8zldet16][bin/shell-plugin.js]
[1526467980.469][2018-05-16 06:53:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:53:00][]
[1526468031.51][2018-05-16 06:53:51][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468031,"source":"Manual (admin)"}]
[1526468031.511][2018-05-16 06:53:51][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526468031.511][2018-05-16 06:53:51][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526468031.511][2018-05-16 06:53:51][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468031,"source":"Manual (admin)","id":"jjh8znj8717","time_start":1526468031.511,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8znj8717.log"}]
[1526468031.512][2018-05-16 06:53:51][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8znj8717","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8znj8717.log","JOB_NOW":"1526468031","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526468031.511","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8znj8717.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526468031.517][2018-05-16 06:53:51][osboxes][Cronicle][debug][3][Spawned child process: 18330 for job: jjh8znj8717][bin/shell-plugin.js]
[1526468040.727][2018-05-16 06:54:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:54:00][]
[1526468100.955][2018-05-16 06:55:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:55:00][]
[1526468121.781][2018-05-16 06:55:21][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526468121,"created":1526461614,"username":"admin"}]
[1526468121.792][2018-05-16 06:55:21][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526468125.757][2018-05-16 06:55:25][osboxes][Cronicle][debug][4][Aborting local job: jjh8znj8717: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468031,"source":"Manual (admin)","id":"jjh8znj8717","time_start":1526468031.511,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8znj8717.log","pid":18330,"cpu":{"min":0.4,"max":2.2,"total":7.500000000000002,"count":9,"current":0.4},"mem":{"min":69091328,"max":69091328,"total":621821952,"count":9,"current":69091328}}]
[1526468127.966][2018-05-16 06:55:27][osboxes][Cronicle][debug][4][Aborting local job: jjh8zldet16: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467930,"source":"Manual (admin)","id":"jjh8zldet16","time_start":1526467930.661,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zldet16.log","pid":18056,"cpu":{"min":0.1,"max":2.2,"total":9.900000000000002,"count":19,"current":0.1},"mem":{"min":69058560,"max":69058560,"total":1312112640,"count":19,"current":69058560}}]
[1526468130.315][2018-05-16 06:55:30][osboxes][Cronicle][debug][4][Aborting local job: jjh8zi4fr14: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467779,"source":"Manual (admin)","id":"jjh8zi4fr14","time_start":1526467779.063,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zi4fr14.log","pid":17219,"cpu":{"min":0.1,"max":4.4,"total":14.499999999999998,"count":35,"current":0.1},"mem":{"min":68800512,"max":68800512,"total":2408017920,"count":35,"current":68800512}}]
[1526468132.418][2018-05-16 06:55:32][osboxes][Cronicle][debug][4][Aborting local job: jjh8zfeuj12: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467652,"source":"Manual (admin)","id":"jjh8zfeuj12","time_start":1526467652.587,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log","pid":16860,"cpu":{"min":0.1,"max":2.7,"total":16.7,"count":47,"current":0.1},"mem":{"min":69038080,"max":69038080,"total":3244789760,"count":47,"current":69038080}}]
[1526468134.825][2018-05-16 06:55:34][osboxes][Cronicle][debug][3][Child 18330 exited with code: 0][]
[1526468134.825][2018-05-16 06:55:34][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468031,"source":"Manual (admin)","id":"jjh8znj8717","time_start":1526468031.511,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8znj8717.log","pid":18330,"cpu":{"min":0.4,"max":2.2,"total":7.500000000000002,"count":9,"current":0.4},"mem":{"min":69091328,"max":69091328,"total":621821952,"count":9,"current":69091328},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526468134.826][2018-05-16 06:55:34][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8znj8717.log: jobs/jjh8znj8717/log.txt.gz][]
[1526468134.873][2018-05-16 06:55:34][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8znj8717/log.txt.gz][]
[1526468134.873][2018-05-16 06:55:34][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8znj8717.log][]
[1526468134.876][2018-05-16 06:55:34][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8znj8717.log][]
[1526468137.031][2018-05-16 06:55:37][osboxes][Cronicle][debug][3][Child 18056 exited with code: 0][]
[1526468137.031][2018-05-16 06:55:37][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467930,"source":"Manual (admin)","id":"jjh8zldet16","time_start":1526467930.661,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zldet16.log","pid":18056,"cpu":{"min":0.1,"max":2.2,"total":10.000000000000002,"count":20,"current":0.1},"mem":{"min":69058560,"max":69332992,"total":1381445632,"count":20,"current":69332992},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526468137.032][2018-05-16 06:55:37][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zldet16.log: jobs/jjh8zldet16/log.txt.gz][]
[1526468137.118][2018-05-16 06:55:37][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8zldet16/log.txt.gz][]
[1526468137.118][2018-05-16 06:55:37][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zldet16.log][]
[1526468137.119][2018-05-16 06:55:37][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zldet16.log][]
[1526468139.34][2018-05-16 06:55:39][osboxes][Cronicle][debug][3][Child 17219 exited with code: 0][]
[1526468139.34][2018-05-16 06:55:39][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467779,"source":"Manual (admin)","id":"jjh8zi4fr14","time_start":1526467779.063,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zi4fr14.log","pid":17219,"cpu":{"min":0.1,"max":4.4,"total":14.599999999999998,"count":36,"current":0.1},"mem":{"min":68800512,"max":69050368,"total":2477068288,"count":36,"current":69050368},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526468139.34][2018-05-16 06:55:39][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zi4fr14.log: jobs/jjh8zi4fr14/log.txt.gz][]
[1526468139.375][2018-05-16 06:55:39][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8zi4fr14/log.txt.gz][]
[1526468139.375][2018-05-16 06:55:39][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zi4fr14.log][]
[1526468139.398][2018-05-16 06:55:39][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zi4fr14.log][]
[1526468141.465][2018-05-16 06:55:41][osboxes][Cronicle][debug][3][Child 16860 exited with code: 0][]
[1526468141.465][2018-05-16 06:55:41][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467652,"source":"Manual (admin)","id":"jjh8zfeuj12","time_start":1526467652.587,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log","pid":16860,"cpu":{"min":0.1,"max":2.7,"total":16.8,"count":48,"current":0.1},"mem":{"min":69038080,"max":69308416,"total":3314098176,"count":48,"current":69308416},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526468141.468][2018-05-16 06:55:41][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log: jobs/jjh8zfeuj12/log.txt.gz][]
[1526468141.514][2018-05-16 06:55:41][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8zfeuj12/log.txt.gz][]
[1526468141.514][2018-05-16 06:55:41][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log][]
[1526468141.515][2018-05-16 06:55:41][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log][]
[1526468149.003][2018-05-16 06:55:49][osboxes][Cronicle][debug][4][Aborting local job: jjh8zgj2m13: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467704,"source":"Manual (admin)","id":"jjh8zgj2m13","time_start":1526467704.719,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zgj2m13.log","pid":17049,"cpu":{"min":0.3,"max":3.5,"total":22.700000000000017,"count":44,"current":0.3},"mem":{"min":68980736,"max":68980736,"total":3035152384,"count":44,"current":68980736}}]
[1526468155.322][2018-05-16 06:55:55][osboxes][Cronicle][debug][4][Aborting local job: jjh8zkpzy15: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467900,"source":"Manual (admin)","id":"jjh8zkpzy15","time_start":1526467900.318,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zkpzy15.log","pid":17779,"cpu":{"min":0.1,"max":2.3,"total":11.3,"count":24,"current":0.2},"mem":{"min":69074944,"max":69074944,"total":1657798656,"count":24,"current":69074944}}]
[1526468158.045][2018-05-16 06:55:58][osboxes][Cronicle][debug][3][Child 17049 exited with code: 0][]
[1526468158.049][2018-05-16 06:55:58][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467704,"source":"Manual (admin)","id":"jjh8zgj2m13","time_start":1526467704.719,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zgj2m13.log","pid":17049,"cpu":{"min":0.3,"max":3.5,"total":23.000000000000018,"count":45,"current":0.3},"mem":{"min":68980736,"max":69238784,"total":3104391168,"count":45,"current":69238784},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526468158.049][2018-05-16 06:55:58][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zgj2m13.log: jobs/jjh8zgj2m13/log.txt.gz][]
[1526468158.137][2018-05-16 06:55:58][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8zgj2m13/log.txt.gz][]
[1526468158.137][2018-05-16 06:55:58][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zgj2m13.log][]
[1526468158.139][2018-05-16 06:55:58][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zgj2m13.log][]
[1526468160.299][2018-05-16 06:56:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:56:00][]
[1526468164.353][2018-05-16 06:56:04][osboxes][Cronicle][debug][3][Child 17779 exited with code: 0][]
[1526468164.353][2018-05-16 06:56:04][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467900,"source":"Manual (admin)","id":"jjh8zkpzy15","time_start":1526467900.318,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zkpzy15.log","pid":17779,"cpu":{"min":0.1,"max":2.3,"total":11.5,"count":25,"current":0.2},"mem":{"min":69074944,"max":69074944,"total":1726873600,"count":25,"current":69074944},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin"}]
[1526468164.354][2018-05-16 06:56:04][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zkpzy15.log: jobs/jjh8zkpzy15/log.txt.gz][]
[1526468164.403][2018-05-16 06:56:04][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8zkpzy15/log.txt.gz][]
[1526468164.403][2018-05-16 06:56:04][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zkpzy15.log][]
[1526468164.404][2018-05-16 06:56:04][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zkpzy15.log][]
[1526468220.442][2018-05-16 06:57:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:57:00][]
[1526468280.588][2018-05-16 06:58:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:58:00][]
[1526468340.718][2018-05-16 06:59:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 06:59:00][]
[1526468400.846][2018-05-16 07:00:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:00:00][]
[1526468458.212][2018-05-16 07:00:58][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526468121,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468458,"source":"Manual (admin)"}]
[1526468458.218][2018-05-16 07:00:58][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526468458.218][2018-05-16 07:00:58][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526468458.218][2018-05-16 07:00:58][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468458,"source":"Manual (admin)","id":"jjh8zwoh618","time_start":1526468458.218,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zwoh618.log"}]
[1526468458.219][2018-05-16 07:00:58][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh8zwoh618","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zwoh618.log","JOB_NOW":"1526468458","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526468458.218","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zwoh618.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526468458.248][2018-05-16 07:00:58][osboxes][Cronicle][debug][3][Spawned child process: 18815 for job: jjh8zwoh618][bin/shell-plugin.js]
[1526468460.003][2018-05-16 07:01:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:01:00][]
[1526468476.581][2018-05-16 07:01:16][osboxes][Cronicle][debug][5][New socket.io client connected: YKCqgxST8xF4-LLnAAAi (IP: ::ffff:10.0.2.15)][]
[1526468476.651][2018-05-16 07:01:16][osboxes][Cronicle][debug][5][Socket client YKCqgxST8xF4-LLnAAAi (IP: ::ffff:10.0.2.15) now watching job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zwoh618.log][]
[1526468520.228][2018-05-16 07:02:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:02:00][]
[1526468580.364][2018-05-16 07:03:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:03:00][]
[1526468640.499][2018-05-16 07:04:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:04:00][]
[1526468700.638][2018-05-16 07:05:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:05:00][]
[1526468760.778][2018-05-16 07:06:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:06:00][]
[1526468820.924][2018-05-16 07:07:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:07:00][]
[1526468880.04][2018-05-16 07:08:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:08:00][]
[1526468940.177][2018-05-16 07:09:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:09:00][]
[1526469000.296][2018-05-16 07:10:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:10:00][]
[1526469060.436][2018-05-16 07:11:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:11:00][]
[1526469120.569][2018-05-16 07:12:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:12:00][]
[1526469180.697][2018-05-16 07:13:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:13:00][]
[1526469240.641][2018-05-16 07:14:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:14:00][]
[1526469300.488][2018-05-16 07:15:00][osboxes][Cronicle][debug][5][Socket.io client disconnected: YKCqgxST8xF4-LLnAAAi (IP: ::ffff:10.0.2.15)][]
[1526469300.84][2018-05-16 07:15:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:15:00][]
[1526469305.758][2018-05-16 07:15:05][osboxes][Cronicle][debug][4][Aborting local job: jjh8zwoh618: Manually aborted by user: admin][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468458,"source":"Manual (admin)","id":"jjh8zwoh618","time_start":1526468458.218,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zwoh618.log","pid":18815,"cpu":{"min":0.2,"max":5,"total":33.700000000000024,"count":84,"current":0.2},"mem":{"min":69029888,"max":69029888,"total":5798510592,"count":84,"current":69029888}}]
[1526469314.779][2018-05-16 07:15:14][osboxes][Cronicle][debug][3][Child 18815 exited with code: 0][]
[1526469314.78][2018-05-16 07:15:14][osboxes][Cronicle][debug][5][Job completed with error][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468458,"source":"Manual (admin)","id":"jjh8zwoh618","time_start":1526468458.218,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zwoh618.log","pid":18815,"cpu":{"min":0.2,"max":5,"total":33.90000000000003,"count":85,"current":0.2},"mem":{"min":69029888,"max":69103616,"total":5867614208,"count":85,"current":69103616},"abort_reason":"Manually aborted by user: admin","complete":1,"code":1,"description":"Job Aborted: Manually aborted by user: admin","html":{"title":"Error Output","content":"<pre>undefined:1\n\n\n\nSyntaxError: Unexpected token</pre>"}}]
[1526469314.78][2018-05-16 07:15:14][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zwoh618.log: jobs/jjh8zwoh618/log.txt.gz][]
[1526469314.796][2018-05-16 07:15:14][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh8zwoh618/log.txt.gz][]
[1526469314.796][2018-05-16 07:15:14][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zwoh618.log][]
[1526469314.797][2018-05-16 07:15:14][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zwoh618.log][]
[1526469360.076][2018-05-16 07:16:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:16:00][]
[1526469420.248][2018-05-16 07:17:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:17:00][]
[1526469480.38][2018-05-16 07:18:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:18:00][]
[1526469540.576][2018-05-16 07:19:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:19:00][]
[1526469600.787][2018-05-16 07:20:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:20:00][]
[1526469660.917][2018-05-16 07:21:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 07:21:00][]
[1526473094.368][2018-05-16 08:18:14][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:18:00][]
[1526473140.511][2018-05-16 08:19:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:19:00][]
[1526473200.632][2018-05-16 08:20:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:20:00][]
[1526473260.782][2018-05-16 08:21:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:21:00][]
[1526473320.92][2018-05-16 08:22:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:22:00][]
[1526473380.046][2018-05-16 08:23:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:23:00][]
[1526473440.193][2018-05-16 08:24:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:24:00][]
[1526473500.343][2018-05-16 08:25:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:25:00][]
[1526473560.475][2018-05-16 08:26:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:26:00][]
[1526473620.603][2018-05-16 08:27:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:27:00][]
[1526473680.756][2018-05-16 08:28:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:28:00][]
[1526473740.92][2018-05-16 08:29:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:29:00][]
[1526473767.999][2018-05-16 08:29:27][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526473767,"created":1526461614,"username":"admin"}]
[1526473768.026][2018-05-16 08:29:28][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526473771.167][2018-05-16 08:29:31][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526473767,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526473771,"source":"Manual (admin)"}]
[1526473771.169][2018-05-16 08:29:31][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526473771.169][2018-05-16 08:29:31][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526473771.17][2018-05-16 08:29:31][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526473771,"source":"Manual (admin)","id":"jjh932jz619","time_start":1526473771.17,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh932jz619.log"}]
[1526473771.171][2018-05-16 08:29:31][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh932jz619","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh932jz619.log","JOB_NOW":"1526473771","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526473771.17","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh932jz619.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526473771.177][2018-05-16 08:29:31][osboxes][Cronicle][debug][3][Spawned child process: 24096 for job: jjh932jz619][bin/shell-plugin.js]
[1526473774.919][2018-05-16 08:29:34][osboxes][Cronicle][debug][3][Child 24096 exited with code: 0][]
[1526473774.919][2018-05-16 08:29:34][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526473771,"source":"Manual (admin)","id":"jjh932jz619","time_start":1526473771.17,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh932jz619.log","pid":24096,"jobid":"ForecastOptimizationjjh932jz619","result":"Success","percentage":100,"progress":1,"complete":1,"code":0,"description":""}]
[1526473774.92][2018-05-16 08:29:34][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh932jz619.log: jobs/jjh932jz619/log.txt.gz][]
[1526473774.956][2018-05-16 08:29:34][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh932jz619/log.txt.gz][]
[1526473774.956][2018-05-16 08:29:34][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh932jz619.log][]
[1526473774.958][2018-05-16 08:29:34][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh932jz619.log][]
[1526473799.272][2018-05-16 08:29:59][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526473767,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526473799,"source":"Manual (admin)"}]
[1526473799.274][2018-05-16 08:29:59][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526473799.274][2018-05-16 08:29:59][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526473799.274][2018-05-16 08:29:59][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526473799,"source":"Manual (admin)","id":"jjh9335nu1a","time_start":1526473799.274,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh9335nu1a.log"}]
[1526473799.275][2018-05-16 08:29:59][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh9335nu1a","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh9335nu1a.log","JOB_NOW":"1526473799","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526473799.274","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh9335nu1a.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526473799.279][2018-05-16 08:29:59][osboxes][Cronicle][debug][3][Spawned child process: 24166 for job: jjh9335nu1a][bin/shell-plugin.js]
[1526473800.13][2018-05-16 08:30:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:30:00][]
[1526473803.079][2018-05-16 08:30:03][osboxes][Cronicle][debug][3][Child 24166 exited with code: 0][]
[1526473803.079][2018-05-16 08:30:03][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526473799,"source":"Manual (admin)","id":"jjh9335nu1a","time_start":1526473799.274,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh9335nu1a.log","pid":24166,"jobid":"ForecastOptimizationjjh9335nu1a","result":"Success","percentage":100,"progress":1,"complete":1,"code":0,"description":""}]
[1526473803.079][2018-05-16 08:30:03][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh9335nu1a.log: jobs/jjh9335nu1a/log.txt.gz][]
[1526473803.105][2018-05-16 08:30:03][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh9335nu1a/log.txt.gz][]
[1526473803.105][2018-05-16 08:30:03][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh9335nu1a.log][]
[1526473803.106][2018-05-16 08:30:03][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh9335nu1a.log][]
[1526473860.269][2018-05-16 08:31:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:31:00][]
[1526473920.45][2018-05-16 08:32:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:32:00][]
[1526473980.606][2018-05-16 08:33:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:33:00][]
[1526473993.596][2018-05-16 08:33:13][osboxes][Cronicle][debug][6][Updating event: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log('A message received');\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526473993,"created":1526461614,"username":"admin"}]
[1526473993.6][2018-05-16 08:33:13][osboxes][Cronicle][debug][6][Successfully updated event: ejh8vu06v01 (ForecastOptimization)][]
[1526474000.131][2018-05-16 08:33:20][osboxes][Cronicle][debug][6][Running event manually: ForecastOptimization][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log('A message received');\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526473993,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526474000,"source":"Manual (admin)"}]
[1526474000.133][2018-05-16 08:33:20][osboxes][Cronicle][debug][9][Choosing server for event using algo: random][["osboxes"]]
[1526474000.133][2018-05-16 08:33:20][osboxes][Cronicle][debug][9][Chose server: osboxes via algo: random][]
[1526474000.134][2018-05-16 08:33:20][osboxes][Cronicle][debug][6][Launching local job][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log('A message received');\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526474000,"source":"Manual (admin)","id":"jjh937gna1b","time_start":1526474000.134,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh937gna1b.log"}]
[1526474000.134][2018-05-16 08:33:20][osboxes][Cronicle][debug][9][Child spawn options:][{"cwd":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","uid":0,"gid":0,"env":{"SUDO_GID":"0","LESSOPEN":"| /usr/bin/lesspipe %s","MAIL":"/var/mail/root","USER":"root","LANGUAGE":"it:en","LC_TIME":"it_IT.UTF-8","SHLVL":"1","HOME":"/root","OLDPWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_MONETARY":"it_IT.UTF-8","COLORTERM":"truecolor","SUDO_UID":"0","LOGNAME":"root","_":"./bin/control.sh","USERNAME":"root","TERM":"xterm-256color","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games","LC_ADDRESS":"it_IT.UTF-8","DISPLAY":":0","LANG":"en_US.UTF-8","LC_TELEPHONE":"it_IT.UTF-8","LS_COLORS":"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:","SUDO_COMMAND":"/bin/su","LC_NAME":"it_IT.UTF-8","SHELL":"/bin/bash","LESSCLOSE":"/usr/bin/lesspipe %s %s","SUDO_USER":"root","LC_MEASUREMENT":"it_IT.UTF-8","LC_IDENTIFICATION":"it_IT.UTF-8","PWD":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle","LC_NUMERIC":"it_IT.UTF-8","LC_PAPER":"it_IT.UTF-8","__daemon":"true","CRONICLE":"0.8.2","JOB_ID":"jjh937gna1b","JOB_LOG":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh937gna1b.log","JOB_NOW":"1526474000","JOB_TIMEOUT":"3600","JOB_CATCH_UP":"0","JOB_QUEUE_MAX":"1000","JOB_TIMEZONE":"America/New_York","JOB_PLUGIN":"shellplug","JOB_CATEGORY":"general","JOB_ALGO":"random","JOB_MULTIPLEX":"0","JOB_STAGGER":"0","JOB_RETRIES":"0","JOB_RETRY_DELAY":"0","JOB_DETACHED":"0","JOB_QUEUE":"0","JOB_CHAIN":"","JOB_CHAIN_ERROR":"","JOB_NOTIFY_SUCCESS":"","JOB_NOTIFY_FAIL":"","JOB_WEB_HOOK":"","JOB_CPU_LIMIT":"0","JOB_CPU_SUSTAIN":"0","JOB_MEMORY_LIMIT":"0","JOB_MEMORY_SUSTAIN":"0","JOB_NOTES":"","JOB_CATEGORY_TITLE":"General","JOB_GROUP_TITLE":"All Servers","JOB_PLUGIN_TITLE":"Shell Script","JOB_SOURCE":"Manual (admin)","JOB_TIME_START":"1526474000.134","JOB_HOSTNAME":"osboxes","JOB_EVENT":"ejh8vu06v01","JOB_EVENT_TITLE":"ForecastOptimization","JOB_NICE_TARGET":"All Servers","JOB_COMMAND":"bin/shell-plugin.js","JOB_LOG_FILE":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh937gna1b.log","SCRIPT":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log('A message received');\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","ANNOTATE":"1","JSON":"1","TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"}}]
[1526474000.137][2018-05-16 08:33:20][osboxes][Cronicle][debug][3][Spawned child process: 24347 for job: jjh937gna1b][bin/shell-plugin.js]
[1526474003.89][2018-05-16 08:33:23][osboxes][Cronicle][debug][3][Child 24347 exited with code: 0][]
[1526474003.89][2018-05-16 08:33:23][osboxes][Cronicle][debug][5][Job completed successfully][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log('A message received');\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526474000,"source":"Manual (admin)","id":"jjh937gna1b","time_start":1526474000.134,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh937gna1b.log","pid":24347,"value":{"type":"Buffer","data":[123,34,116,105,109,101,111,117,116,34,58,32,51,54,48,48,44,32,34,106,111,98,105,100,34,58,32,34,70,111,114,101,99,97,115,116,79,112,116,105,109,105,122,97,116,105,111,110,106,106,104,57,51,55,103,110,97,49,98,34,44,32,34,114,101,115,117,108,116,34,58,32,34,83,117,99,99,101,115,115,34,44,32,34,112,101,114,99,101,110,116,97,103,101,34,58,32,49,48,48,125]},"size":101,"key":null,"topic":"TriggerForecastOptimizationResponse","offset":5,"partition":0,"timestamp":1526474001719,"progress":1,"complete":1,"code":0,"description":""}]
[1526474003.89][2018-05-16 08:33:23][osboxes][Cronicle][debug][6][Storing job log: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh937gna1b.log: jobs/jjh937gna1b/log.txt.gz][]
[1526474003.932][2018-05-16 08:33:23][osboxes][Cronicle][debug][9][Job log stored successfully: jobs/jjh937gna1b/log.txt.gz][]
[1526474003.932][2018-05-16 08:33:23][osboxes][Cronicle][debug][9][Deleting local file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh937gna1b.log][]
[1526474003.949][2018-05-16 08:33:23][osboxes][Cronicle][debug][9][Successfully deleted local job log file: /home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh937gna1b.log][]
[1526474040.8][2018-05-16 08:34:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:34:00][]
[1526474100.951][2018-05-16 08:35:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:35:00][]
[1526474160.106][2018-05-16 08:36:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:36:00][]
[1526474220.245][2018-05-16 08:37:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:37:00][]
[1526474280.451][2018-05-16 08:38:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:38:00][]
[1526474340.695][2018-05-16 08:39:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:39:00][]
[1526474400.857][2018-05-16 08:40:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:40:00][]
[1526474460.002][2018-05-16 08:41:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:41:00][]
[1526474520.18][2018-05-16 08:42:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:42:00][]
[1526474533.878][2018-05-16 08:42:13][osboxes][Cronicle][debug][5][Socket.io client disconnected: HXV8kxg0fTF0u4FrAAAK (IP: ::ffff:10.0.2.15)][]
[1526474580.392][2018-05-16 08:43:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:43:00][]
[1526474640.524][2018-05-16 08:44:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:44:00][]
[1526474700.651][2018-05-16 08:45:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:45:00][]
[1526474760.819][2018-05-16 08:46:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:46:00][]
[1526474820.971][2018-05-16 08:47:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:47:00][]
[1526474880.184][2018-05-16 08:48:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:48:00][]
[1526474940.303][2018-05-16 08:49:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:49:00][]
[1526475000.478][2018-05-16 08:50:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:50:00][]
[1526475060.604][2018-05-16 08:51:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:51:00][]
[1526475120.712][2018-05-16 08:52:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:52:00][]
[1526475180.833][2018-05-16 08:53:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:53:00][]
[1526475240.951][2018-05-16 08:54:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:54:00][]
[1526475300.07][2018-05-16 08:55:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:55:00][]
[1526475360.239][2018-05-16 08:56:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:56:00][]
[1526475420.349][2018-05-16 08:57:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:57:00][]
[1526475480.503][2018-05-16 08:58:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:58:00][]
[1526475540.64][2018-05-16 08:59:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 08:59:00][]
[1526475600.786][2018-05-16 09:00:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:00:00][]
[1526475660.897][2018-05-16 09:01:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:01:00][]
[1526475720.025][2018-05-16 09:02:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:02:00][]
[1526475780.165][2018-05-16 09:03:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:03:00][]
[1526475840.295][2018-05-16 09:04:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:04:00][]
[1526475900.419][2018-05-16 09:05:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:05:00][]
[1526475960.536][2018-05-16 09:06:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:06:00][]
[1526476020.669][2018-05-16 09:07:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:07:00][]
[1526476080.805][2018-05-16 09:08:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:08:00][]
[1526476140.916][2018-05-16 09:09:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:09:00][]
[1526476200.021][2018-05-16 09:10:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:10:00][]
[1526476260.137][2018-05-16 09:11:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:11:00][]
[1526476320.26][2018-05-16 09:12:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:12:00][]
[1526476380.367][2018-05-16 09:13:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:13:00][]
[1526476440.479][2018-05-16 09:14:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:14:00][]
[1526476500.6][2018-05-16 09:15:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:15:00][]
[1526476560.74][2018-05-16 09:16:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:16:00][]
[1526476620.872][2018-05-16 09:17:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:17:00][]
[1526476680.991][2018-05-16 09:18:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:18:00][]
[1526476740.124][2018-05-16 09:19:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:19:00][]
[1526476800.25][2018-05-16 09:20:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:20:00][]
[1526476860.365][2018-05-16 09:21:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:21:00][]
[1526476920.474][2018-05-16 09:22:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:22:00][]
[1526476980.584][2018-05-16 09:23:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:23:00][]
[1526477040.714][2018-05-16 09:24:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:24:00][]
[1526477100.849][2018-05-16 09:25:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:25:00][]
[1526477160.958][2018-05-16 09:26:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:26:00][]
[1526477220.077][2018-05-16 09:27:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:27:00][]
[1526477280.224][2018-05-16 09:28:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:28:00][]
[1526477340.36][2018-05-16 09:29:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:29:00][]
[1526477400.49][2018-05-16 09:30:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:30:00][]
[1526477460.598][2018-05-16 09:31:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:31:00][]
[1526477520.715][2018-05-16 09:32:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:32:00][]
[1526477580.827][2018-05-16 09:33:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:33:00][]
[1526477640.936][2018-05-16 09:34:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:34:00][]
[1526477700.055][2018-05-16 09:35:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:35:00][]
[1526477760.175][2018-05-16 09:36:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:36:00][]
[1526477820.297][2018-05-16 09:37:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:37:00][]
[1526477880.415][2018-05-16 09:38:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:38:00][]
[1526477940.531][2018-05-16 09:39:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:39:00][]
[1526478000.656][2018-05-16 09:40:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:40:00][]
[1526478060.796][2018-05-16 09:41:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:41:00][]
[1526478120.936][2018-05-16 09:42:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:42:00][]
[1526478180.034][2018-05-16 09:43:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:43:00][]
[1526478240.162][2018-05-16 09:44:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:44:00][]
[1526478300.283][2018-05-16 09:45:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:45:00][]
[1526478360.398][2018-05-16 09:46:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:46:00][]
[1526478420.511][2018-05-16 09:47:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:47:00][]
[1526478480.653][2018-05-16 09:48:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:48:00][]
[1526478540.76][2018-05-16 09:49:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:49:00][]
[1526478600.895][2018-05-16 09:50:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:50:00][]
[1526478660.076][2018-05-16 09:51:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:51:00][]
[1526478720.458][2018-05-16 09:52:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:52:00][]
[1526478780.566][2018-05-16 09:53:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:53:00][]
[1526478840.705][2018-05-16 09:54:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:54:00][]
[1526478900.833][2018-05-16 09:55:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:55:00][]
[1526478960.955][2018-05-16 09:56:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:56:00][]
[1526479020.084][2018-05-16 09:57:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:57:00][]
[1526479080.286][2018-05-16 09:58:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:58:00][]
[1526479140.48][2018-05-16 09:59:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 09:59:00][]
[1526479200.608][2018-05-16 10:00:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 10:00:00][]
[1526479260.721][2018-05-16 10:01:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 10:01:00][]
[1526479320.822][2018-05-16 10:02:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 10:02:00][]
[1526479380.955][2018-05-16 10:03:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 10:03:00][]
[1526479440.105][2018-05-16 10:04:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 10:04:00][]
[1526479500.229][2018-05-16 10:05:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 10:05:00][]
[1526479560.414][2018-05-16 10:06:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 10:06:00][]
[1526479620.526][2018-05-16 10:07:00][osboxes][Cronicle][debug][4][Scheduler Minute Tick: Advancing time up to: 2018/05/16 10:07:00][]
[1526479674.247][2018-05-16 10:07:54][osboxes][Cronicle][debug][1][Shutting down][]
[1526479674.251][2018-05-16 10:07:54][osboxes][Cronicle][debug][3][Stopping component: Cronicle][]
[1526479674.252][2018-05-16 10:07:54][osboxes][Cronicle][debug][2][Shutting down Cronicle][]
[1526479674.243][2018-05-16 10:07:54][osboxes][Cronicle][debug][1][Caught SIGTERM][]
[1526479674.254][2018-05-16 10:07:54][osboxes][Cronicle][debug][3][Shutting down UDP server][]
[1526479674.254][2018-05-16 10:07:54][osboxes][Cronicle][debug][3][Stopping component: User][]
[1526479674.255][2018-05-16 10:07:54][osboxes][Cronicle][debug][3][Stopping component: API][]
[1526479674.255][2018-05-16 10:07:54][osboxes][Cronicle][debug][3][Stopping component: WebServer][]
[1526479674.259][2018-05-16 10:07:54][osboxes][Cronicle][debug][3][Stopping component: Storage][]
[1526479674.26][2018-05-16 10:07:54][osboxes][Cronicle][debug][2][Shutdown complete, exiting][]
