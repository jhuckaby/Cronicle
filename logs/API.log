[1526461443.234][2018-05-16 05:04:03][osboxes][API][debug][3][API service listening for base URI: /api][]
[1526461443.235][2018-05-16 05:04:03][osboxes][API][debug][3][Adding API namespace: user][]
[1526461443.245][2018-05-16 05:04:03][osboxes][API][debug][3][Adding API namespace: app][]
[1526461486.649][2018-05-16 05:04:46][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461486.649][2018-05-16 05:04:46][osboxes][API][debug][9][API Params][{}]
[1526461486.649][2018-05-16 05:04:46][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461488.471][2018-05-16 05:04:48][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461488.471][2018-05-16 05:04:48][osboxes][API][debug][9][API Params][{}]
[1526461488.471][2018-05-16 05:04:48][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461489.505][2018-05-16 05:04:49][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461489.505][2018-05-16 05:04:49][osboxes][API][debug][9][API Params][{}]
[1526461489.506][2018-05-16 05:04:49][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461490.524][2018-05-16 05:04:50][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461490.524][2018-05-16 05:04:50][osboxes][API][debug][9][API Params][{}]
[1526461490.524][2018-05-16 05:04:50][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461491.575][2018-05-16 05:04:51][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461491.575][2018-05-16 05:04:51][osboxes][API][debug][9][API Params][{}]
[1526461491.575][2018-05-16 05:04:51][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461492.605][2018-05-16 05:04:52][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461492.605][2018-05-16 05:04:52][osboxes][API][debug][9][API Params][{}]
[1526461492.606][2018-05-16 05:04:52][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461493.626][2018-05-16 05:04:53][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461493.627][2018-05-16 05:04:53][osboxes][API][debug][9][API Params][{}]
[1526461493.627][2018-05-16 05:04:53][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461494.684][2018-05-16 05:04:54][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461494.684][2018-05-16 05:04:54][osboxes][API][debug][9][API Params][{}]
[1526461494.686][2018-05-16 05:04:54][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461495.736][2018-05-16 05:04:55][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461495.736][2018-05-16 05:04:55][osboxes][API][debug][9][API Params][{}]
[1526461495.736][2018-05-16 05:04:55][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461496.774][2018-05-16 05:04:56][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461496.774][2018-05-16 05:04:56][osboxes][API][debug][9][API Params][{}]
[1526461496.774][2018-05-16 05:04:56][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461497.803][2018-05-16 05:04:57][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461497.804][2018-05-16 05:04:57][osboxes][API][debug][9][API Params][{}]
[1526461497.804][2018-05-16 05:04:57][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461498.848][2018-05-16 05:04:58][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461498.848][2018-05-16 05:04:58][osboxes][API][debug][9][API Params][{}]
[1526461498.848][2018-05-16 05:04:58][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461499.881][2018-05-16 05:04:59][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461499.881][2018-05-16 05:04:59][osboxes][API][debug][9][API Params][{}]
[1526461499.881][2018-05-16 05:04:59][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461500.917][2018-05-16 05:05:00][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461500.917][2018-05-16 05:05:00][osboxes][API][debug][9][API Params][{}]
[1526461500.917][2018-05-16 05:05:00][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461501.95][2018-05-16 05:05:01][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461501.95][2018-05-16 05:05:01][osboxes][API][debug][9][API Params][{}]
[1526461501.95][2018-05-16 05:05:01][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461502.974][2018-05-16 05:05:02][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461502.974][2018-05-16 05:05:02][osboxes][API][debug][9][API Params][{}]
[1526461502.975][2018-05-16 05:05:02][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461503.996][2018-05-16 05:05:03][osboxes][API][debug][9][API Params][{}]
[1526461503.996][2018-05-16 05:05:03][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526461503.996][2018-05-16 05:05:03][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526461504.392][2018-05-16 05:05:04][osboxes][API][debug][6][Handling API request: POST /api/user/resume_session][{}]
[1526461504.393][2018-05-16 05:05:04][osboxes][API][debug][9][API Params][{"session_id":"1c910f272de5b53aacf8d6c0f06bb2654de160ff34d53b5e14de92955e0aa71e"}]
[1526461504.393][2018-05-16 05:05:04][osboxes][API][debug][9][Activating namespaced API handler: user/api_resume_session for URI: /api/user/resume_session][]
[1526461504.491][2018-05-16 05:05:04][osboxes][API][debug][6][Handling API request: POST /api/user/logout][{}]
[1526461504.491][2018-05-16 05:05:04][osboxes][API][debug][9][API Params][{"session_id":"1c910f272de5b53aacf8d6c0f06bb2654de160ff34d53b5e14de92955e0aa71e"}]
[1526461504.491][2018-05-16 05:05:04][osboxes][API][debug][9][Activating namespaced API handler: user/api_logout for URI: /api/user/logout][]
[1526461507.226][2018-05-16 05:05:07][osboxes][API][debug][6][Handling API request: POST /api/user/login][{}]
[1526461507.226][2018-05-16 05:05:07][osboxes][API][debug][9][API Params][{"username":"admin","password":"admin"}]
[1526461507.226][2018-05-16 05:05:07][osboxes][API][debug][9][Activating namespaced API handler: user/api_login for URI: /api/user/login][]
[1526461510.188][2018-05-16 05:05:10][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526461510.188][2018-05-16 05:05:10][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461510.189][2018-05-16 05:05:10][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526461614.959][2018-05-16 05:06:54][osboxes][API][debug][6][Handling API request: POST /api/app/create_event][{}]
[1526461614.959][2018-05-16 05:06:54][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461614.959][2018-05-16 05:06:54][osboxes][API][debug][9][Activating namespaced API handler: app/api_create_event for URI: /api/app/create_event][]
[1526461619.443][2018-05-16 05:06:59][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526461619.443][2018-05-16 05:06:59][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461619,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461619.443][2018-05-16 05:06:59][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526461624.587][2018-05-16 05:07:04][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526461624.588][2018-05-16 05:07:04][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461624.588][2018-05-16 05:07:04][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526461671.324][2018-05-16 05:07:51][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526461671.324][2018-05-16 05:07:51][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461619,"source":"Manual (admin)","id":"jjh8vu3nt02","time_start":1526461619.465,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vu3nt02.log","pid":6948,"cpu":{"min":1,"max":3.3,"total":7.5,"count":4,"current":1},"mem":{"min":68730880,"max":68730880,"total":274923520,"count":4,"current":68730880},"progress":0,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461671.324][2018-05-16 05:07:51][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526461674.721][2018-05-16 05:07:54][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526461674.721][2018-05-16 05:07:54][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461674.721][2018-05-16 05:07:54][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526461677.123][2018-05-16 05:07:57][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526461677.123][2018-05-16 05:07:57][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461677.123][2018-05-16 05:07:57][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526461682.088][2018-05-16 05:08:02][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526461682.088][2018-05-16 05:08:02][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461682.088][2018-05-16 05:08:02][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526461693.387][2018-05-16 05:08:13][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526461693.387][2018-05-16 05:08:13][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461693,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461693.387][2018-05-16 05:08:13][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526461777.231][2018-05-16 05:09:37][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526461777.231][2018-05-16 05:09:37][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461777,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461777.231][2018-05-16 05:09:37][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526461822.375][2018-05-16 05:10:22][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526461822.375][2018-05-16 05:10:22][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461693,"source":"Manual (admin)","id":"jjh8vvopc03","time_start":1526461693.392,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8vvopc03.log","pid":7074,"cpu":{"min":0.6,"max":3.4000000000000004,"total":13.699999999999998,"count":12,"current":0.6},"mem":{"min":67620864,"max":67620864,"total":811450368,"count":12,"current":67620864},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461822.375][2018-05-16 05:10:22][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526461851.327][2018-05-16 05:10:51][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526461851.327][2018-05-16 05:10:51][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461851,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461851.328][2018-05-16 05:10:51][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526461861.888][2018-05-16 05:11:01][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526461861.888][2018-05-16 05:11:01][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461861.889][2018-05-16 05:11:01][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526461864.226][2018-05-16 05:11:04][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526461864.226][2018-05-16 05:11:04][osboxes][API][debug][9][API Params][{"id":"jjh8vz2kn04","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461864.226][2018-05-16 05:11:04][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526461864.264][2018-05-16 05:11:04][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8vz2kn04][]
[1526461864.264][2018-05-16 05:11:04][osboxes][API][debug][9][API Params][{}]
[1526461864.264][2018-05-16 05:11:04][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526461977.553][2018-05-16 05:12:57][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526461977.553][2018-05-16 05:12:57][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461977.553][2018-05-16 05:12:57][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526461981.416][2018-05-16 05:13:01][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526461981.416][2018-05-16 05:13:01][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461977,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461981,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526461981.416][2018-05-16 05:13:01][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526462011.228][2018-05-16 05:13:31][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526462011.229][2018-05-16 05:13:31][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526461981,"source":"Manual (admin)","id":"jjh8w1uy405","time_start":1526461981.42,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8w1uy405.log","pid":7806,"cpu":{"min":1.8,"max":3.2,"total":5,"count":2,"current":1.8},"mem":{"min":68640768,"max":68640768,"total":137281536,"count":2,"current":68640768},"progress":0,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462011.229][2018-05-16 05:13:31][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526462128.407][2018-05-16 05:15:28][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526462128.407][2018-05-16 05:15:28][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : latest\t\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462128.408][2018-05-16 05:15:28][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526462131.654][2018-05-16 05:15:31][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526462131.654][2018-05-16 05:15:31][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : latest\t\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\t\n   })\t\t\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n\n\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462128,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462131,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462131.655][2018-05-16 05:15:31][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526462163.608][2018-05-16 05:16:03][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526462163.608][2018-05-16 05:16:03][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462163.608][2018-05-16 05:16:03][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526462165.84][2018-05-16 05:16:05][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526462165.84][2018-05-16 05:16:05][osboxes][API][debug][9][API Params][{"id":"jjh8w52vh06","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462165.84][2018-05-16 05:16:05][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526462165.902][2018-05-16 05:16:05][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8w52vh06][]
[1526462165.902][2018-05-16 05:16:05][osboxes][API][debug][9][API Params][{}]
[1526462165.902][2018-05-16 05:16:05][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526462203.587][2018-05-16 05:16:43][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526462203.587][2018-05-16 05:16:43][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462203.587][2018-05-16 05:16:43][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526462206.439][2018-05-16 05:16:46][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526462206.439][2018-05-16 05:16:46][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.commit.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462203,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462206,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462206.439][2018-05-16 05:16:46][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526462209.794][2018-05-16 05:16:49][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526462209.794][2018-05-16 05:16:49][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462209.794][2018-05-16 05:16:49][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526462211.849][2018-05-16 05:16:51][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526462211.849][2018-05-16 05:16:51][osboxes][API][debug][9][API Params][{"id":"jjh8w6okq07","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462211.85][2018-05-16 05:16:51][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526462211.894][2018-05-16 05:16:51][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8w6okq07][]
[1526462211.894][2018-05-16 05:16:51][osboxes][API][debug][9][API Params][{}]
[1526462211.894][2018-05-16 05:16:51][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526462365.523][2018-05-16 05:19:25][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526462365.523][2018-05-16 05:19:25][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462365.523][2018-05-16 05:19:25][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526462368.286][2018-05-16 05:19:28][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526462368.286][2018-05-16 05:19:28][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462365,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462368,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462368.286][2018-05-16 05:19:28][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526462421.451][2018-05-16 05:20:21][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526462421.451][2018-05-16 05:20:21][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'latest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462368,"source":"Manual (admin)","id":"jjh8wa5gk08","time_start":1526462368.292,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wa5gk08.log","pid":8375,"cpu":{"min":1.1,"max":3.7,"total":8.3,"count":4,"current":1.1},"mem":{"min":68816896,"max":68816896,"total":275267584,"count":4,"current":68816896},"progress":0,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462421.451][2018-05-16 05:20:21][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526462430.426][2018-05-16 05:20:30][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526462430.427][2018-05-16 05:20:30][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526461614,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462430.427][2018-05-16 05:20:30][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526462434.07][2018-05-16 05:20:34][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526462434.07][2018-05-16 05:20:34][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462434,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462434.07][2018-05-16 05:20:34][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526462589.808][2018-05-16 05:23:09][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526462589.808][2018-05-16 05:23:09][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462434,"source":"Manual (admin)","id":"jjh8wbk7u09","time_start":1526462434.075,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wbk7u09.log","pid":8541,"cpu":{"min":0.5,"max":6,"total":16.9,"count":15,"current":0.5},"mem":{"min":68698112,"max":68698112,"total":1030471680,"count":15,"current":68698112},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462589.809][2018-05-16 05:23:09][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526462602.337][2018-05-16 05:23:22][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526462602.337][2018-05-16 05:23:22][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462602,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462602.338][2018-05-16 05:23:22][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526462618.791][2018-05-16 05:23:38][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526462618.792][2018-05-16 05:23:38][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462602,"source":"Manual (admin)","id":"jjh8wf61x0a","time_start":1526462602.341,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wf61x0a.log","pid":8784,"cpu":{"min":3.7,"max":3.7,"total":3.7,"count":1,"current":3.7},"mem":{"min":68980736,"max":68980736,"total":68980736,"count":1,"current":68980736},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462618.792][2018-05-16 05:23:38][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526462695.561][2018-05-16 05:24:55][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526462695.561][2018-05-16 05:24:55][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462695,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462695.562][2018-05-16 05:24:55][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526462706.386][2018-05-16 05:25:06][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526462706.386][2018-05-16 05:25:06][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462706.386][2018-05-16 05:25:06][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526462708.51][2018-05-16 05:25:08][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526462708.51][2018-05-16 05:25:08][osboxes][API][debug][9][API Params][{"id":"jjh8wh5zj0b","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462708.51][2018-05-16 05:25:08][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526462708.55][2018-05-16 05:25:08][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8wh5zj0b][]
[1526462708.55][2018-05-16 05:25:08][osboxes][API][debug][9][API Params][{}]
[1526462708.55][2018-05-16 05:25:08][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526462900.714][2018-05-16 05:28:20][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526462900.714][2018-05-16 05:28:20][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462900,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462900.714][2018-05-16 05:28:20][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526462911.3][2018-05-16 05:28:31][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526462911.3][2018-05-16 05:28:31][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462911.3][2018-05-16 05:28:31][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526462920.037][2018-05-16 05:28:40][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526462920.038][2018-05-16 05:28:40][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462920,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462920.038][2018-05-16 05:28:40][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526462940.357][2018-05-16 05:29:00][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526462940.357][2018-05-16 05:29:00][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526462920,"source":"Manual (admin)","id":"jjh8wlz6z0d","time_start":1526462920.043,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wlz6z0d.log","pid":9284,"cpu":{"min":3.3,"max":3.3,"total":3.3,"count":1,"current":3.3},"mem":{"min":68939776,"max":68939776,"total":68939776,"count":1,"current":68939776},"progress":0,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462940.357][2018-05-16 05:29:00][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526462953.229][2018-05-16 05:29:13][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526462953.229][2018-05-16 05:29:13][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462953.23][2018-05-16 05:29:13][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526462955.242][2018-05-16 05:29:15][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526462955.242][2018-05-16 05:29:15][osboxes][API][debug][9][API Params][{"id":"jjh8wlz6z0d","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526462955.242][2018-05-16 05:29:15][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526462955.282][2018-05-16 05:29:15][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8wlz6z0d][]
[1526462955.282][2018-05-16 05:29:15][osboxes][API][debug][9][API Params][{}]
[1526462955.282][2018-05-16 05:29:15][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526463203.756][2018-05-16 05:33:23][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526463203.756][2018-05-16 05:33:23][osboxes][API][debug][9][API Params][{}]
[1526463203.756][2018-05-16 05:33:23][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526463252.519][2018-05-16 05:34:12][osboxes][API][debug][6][Handling API request: GET /api/app/config?callback=app.receiveConfig][]
[1526463252.519][2018-05-16 05:34:12][osboxes][API][debug][9][API Params][{}]
[1526463252.519][2018-05-16 05:34:12][osboxes][API][debug][9][Activating namespaced API handler: app/api_config for URI: /api/app/config][]
[1526463253.133][2018-05-16 05:34:13][osboxes][API][debug][6][Handling API request: POST /api/user/resume_session][{}]
[1526463253.134][2018-05-16 05:34:13][osboxes][API][debug][9][API Params][{"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463253.134][2018-05-16 05:34:13][osboxes][API][debug][9][Activating namespaced API handler: user/api_resume_session for URI: /api/user/resume_session][]
[1526463257.532][2018-05-16 05:34:17][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526463257.532][2018-05-16 05:34:17][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463257,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463257.532][2018-05-16 05:34:17][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526463337.243][2018-05-16 05:35:37][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526463337.243][2018-05-16 05:35:37][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463257,"source":"Manual (admin)","id":"jjh8wt7lx0e","time_start":1526463257.541,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wt7lx0e.log","pid":9637,"cpu":{"min":0.5,"max":3.1,"total":8.600000000000001,"count":7,"current":0.5},"mem":{"min":68771840,"max":68771840,"total":481402880,"count":7,"current":68771840},"progress":0,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463337.244][2018-05-16 05:35:37][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526463352.951][2018-05-16 05:35:52][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526463352.951][2018-05-16 05:35:52][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463352,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463352.951][2018-05-16 05:35:52][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526463375.431][2018-05-16 05:36:15][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526463375.432][2018-05-16 05:36:15][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463352,"source":"Manual (admin)","id":"jjh8wv98c0f","time_start":1526463352.956,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8wv98c0f.log","pid":9796,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463375.433][2018-05-16 05:36:15][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526463460.856][2018-05-16 05:37:40][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526463460.856][2018-05-16 05:37:40][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463460,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463460.856][2018-05-16 05:37:40][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526463472.412][2018-05-16 05:37:52][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526463472.412][2018-05-16 05:37:52][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463472.412][2018-05-16 05:37:52][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526463475.279][2018-05-16 05:37:55][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526463475.282][2018-05-16 05:37:55][osboxes][API][debug][9][API Params][{"id":"jjh8wxkho0g","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463475.283][2018-05-16 05:37:55][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526463475.345][2018-05-16 05:37:55][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8wxkho0g][]
[1526463475.346][2018-05-16 05:37:55][osboxes][API][debug][9][API Params][{}]
[1526463475.346][2018-05-16 05:37:55][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526463612.478][2018-05-16 05:40:12][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526463612.478][2018-05-16 05:40:12][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463612,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463612.478][2018-05-16 05:40:12][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526463688.805][2018-05-16 05:41:28][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526463688.806][2018-05-16 05:41:28][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463688.806][2018-05-16 05:41:28][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526463726.076][2018-05-16 05:42:06][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526463726.076][2018-05-16 05:42:06][osboxes][API][debug][9][API Params][{"id":"jjh8x0thi0h","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463726.076][2018-05-16 05:42:06][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526463726.116][2018-05-16 05:42:06][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8x0thi0h][]
[1526463726.116][2018-05-16 05:42:06][osboxes][API][debug][9][API Params][{}]
[1526463726.117][2018-05-16 05:42:06][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526463852.854][2018-05-16 05:44:12][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526463852.854][2018-05-16 05:44:12][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463852.854][2018-05-16 05:44:12][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526463861.018][2018-05-16 05:44:21][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526463861.018][2018-05-16 05:44:21][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463861.018][2018-05-16 05:44:21][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526463868.065][2018-05-16 05:44:28][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526463868.067][2018-05-16 05:44:28][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526463861,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463868,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463868.067][2018-05-16 05:44:28][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526463873.813][2018-05-16 05:44:33][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526463873.815][2018-05-16 05:44:33][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526463873.813][2018-05-16 05:44:33][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463897.913][2018-05-16 05:44:57][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526463897.913][2018-05-16 05:44:57][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463897.913][2018-05-16 05:44:57][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526463908.401][2018-05-16 05:45:08][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526463908.401][2018-05-16 05:45:08][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463908.401][2018-05-16 05:45:08][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526463932.971][2018-05-16 05:45:32][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526463932.971][2018-05-16 05:45:32][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526463861,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463932,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463932.971][2018-05-16 05:45:32][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526463942.914][2018-05-16 05:45:42][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526463942.914][2018-05-16 05:45:42][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463942.914][2018-05-16 05:45:42][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526463953.369][2018-05-16 05:45:53][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526463953.369][2018-05-16 05:45:53][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526463861,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463953,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463953.37][2018-05-16 05:45:53][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526463964.075][2018-05-16 05:46:04][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526463964.075][2018-05-16 05:46:04][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526463953,"source":"Manual (admin)","id":"jjh8x84il0k","time_start":1526463953.373,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8x84il0k.log","pid":10818,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526463964.075][2018-05-16 05:46:04][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526464130.437][2018-05-16 05:48:50][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526464130.437][2018-05-16 05:48:50][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'beginning'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464130.437][2018-05-16 05:48:50][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526464133.667][2018-05-16 05:48:53][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526464133.667][2018-05-16 05:48:53][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'beginning'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526464130,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464133,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464133.667][2018-05-16 05:48:53][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526464149.351][2018-05-16 05:49:09][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526464149.351][2018-05-16 05:49:09][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'beginning'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464133,"source":"Manual (admin)","id":"jjh8xbzmv0l","time_start":1526464133.671,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xbzmv0l.log","pid":11036,"cpu":{"min":3.8,"max":3.8,"total":3.8,"count":1,"current":3.8},"mem":{"min":68796416,"max":68796416,"total":68796416,"count":1,"current":68796416},"progress":0,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464149.353][2018-05-16 05:49:09][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526464423.59][2018-05-16 05:53:43][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526464423.59][2018-05-16 05:53:43][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': false,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464423.59][2018-05-16 05:53:43][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526464426.689][2018-05-16 05:53:46][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526464426.689][2018-05-16 05:53:46][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': false,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526464423,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464426,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464426.69][2018-05-16 05:53:46][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526464832.99][2018-05-16 06:00:32][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526464832.99][2018-05-16 06:00:32][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': false,\n      'auto.offset.reset' : 'earliest'\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464426,"source":"Manual (admin)","id":"jjh8xi9qd0m","time_start":1526464426.693,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xi9qd0m.log","pid":11407,"cpu":{"min":0.3,"max":3.2,"total":20.700000000000017,"count":40,"current":0.3},"mem":{"min":67751936,"max":67751936,"total":2710077440,"count":40,"current":67751936},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464832.991][2018-05-16 06:00:32][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526464854.254][2018-05-16 06:00:54][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526464854.254][2018-05-16 06:00:54][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464854.254][2018-05-16 06:00:54][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526464864.401][2018-05-16 06:01:04][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526464864.401][2018-05-16 06:01:04][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":1,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526464854,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464864,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464864.403][2018-05-16 06:01:04][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526464902.885][2018-05-16 06:01:42][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526464902.885][2018-05-16 06:01:42][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526464864,"source":"Manual (admin)","id":"jjh8xrnh10n","time_start":1526464864.405,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8xrnh10n.log","pid":12353,"cpu":{"min":3.9000000000000004,"max":6.5,"total":15,"count":3,"current":3.9000000000000004},"mem":{"min":72687616,"max":74321920,"total":219975680,"count":3,"current":72966144},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464902.886][2018-05-16 06:01:42][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526464912.146][2018-05-16 06:01:52][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526464912.146][2018-05-16 06:01:52][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464912.146][2018-05-16 06:01:52][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526464912.278][2018-05-16 06:01:52][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526464912.278][2018-05-16 06:01:52][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464912.278][2018-05-16 06:01:52][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526464913.833][2018-05-16 06:01:53][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526464913.833][2018-05-16 06:01:53][osboxes][API][debug][9][API Params][{"id":"jjh8xrnh10n","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526464913.833][2018-05-16 06:01:53][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526464913.869][2018-05-16 06:01:53][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8xrnh10n][]
[1526464913.869][2018-05-16 06:01:53][osboxes][API][debug][9][API Params][{}]
[1526464913.869][2018-05-16 06:01:53][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526465262.451][2018-05-16 06:07:42][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526465262.451][2018-05-16 06:07:42][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465262.451][2018-05-16 06:07:42][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526465265.963][2018-05-16 06:07:45][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526465265.963][2018-05-16 06:07:45][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465262,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465265,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465265.963][2018-05-16 06:07:45][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526465282.999][2018-05-16 06:08:02][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526465282.999][2018-05-16 06:08:02][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465262,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465282,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465282.999][2018-05-16 06:08:02][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526465307.545][2018-05-16 06:08:27][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526465307.546][2018-05-16 06:08:27][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465262,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465307,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465307.546][2018-05-16 06:08:27][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526465489.877][2018-05-16 06:11:29][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526465489.878][2018-05-16 06:11:29][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465307,"source":"Manual (admin)","id":"jjh8y15eo0q","time_start":1526465307.552,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y15eo0q.log","pid":13176,"cpu":{"min":0.5,"max":4.4,"total":14.100000000000001,"count":12,"current":0.5},"mem":{"min":68866048,"max":68866048,"total":826392576,"count":12,"current":68866048},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465489.878][2018-05-16 06:11:29][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526465491.914][2018-05-16 06:11:31][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526465491.914][2018-05-16 06:11:31][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526465491.914][2018-05-16 06:11:31][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465282,"source":"Manual (admin)","id":"jjh8y0mgr0p","time_start":1526465283.003,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y0mgr0p.log","pid":13115,"cpu":{"min":0.5,"max":3.2,"total":14.4,"count":14,"current":0.5},"mem":{"min":68743168,"max":68743168,"total":962404352,"count":14,"current":68743168},"jobid":"ForecastOptimizationjjh8y15eo0q","result":"Success","percentage":100,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465493.877][2018-05-16 06:11:33][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526465493.878][2018-05-16 06:11:33][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(msg.value.toString());\n      if(record.jobid === jobid){\n        consumer.disconnect();\n        onNextMsgReceived(record);\n      }\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465265,"source":"Manual (admin)","id":"jjh8y09bl0o","time_start":1526465265.969,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y09bl0o.log","pid":13066,"cpu":{"min":0.5,"max":4.2,"total":17.5,"count":16,"current":0.5},"mem":{"min":68542464,"max":68542464,"total":1096679424,"count":16,"current":68542464},"jobid":"ForecastOptimizationjjh8y15eo0q","result":"Success","percentage":100,"progress":0,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465493.878][2018-05-16 06:11:33][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526465510.564][2018-05-16 06:11:50][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526465510.564][2018-05-16 06:11:50][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465510.564][2018-05-16 06:11:50][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526465513.965][2018-05-16 06:11:53][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526465513.965][2018-05-16 06:11:53][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465510,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465513,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465513.966][2018-05-16 06:11:53][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526465533.385][2018-05-16 06:12:13][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526465533.386][2018-05-16 06:12:13][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465513,"source":"Manual (admin)","id":"jjh8y5kok0r","time_start":1526465513.972,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y5kok0r.log","pid":13521,"cpu":{"min":6.7,"max":6.7,"total":6.7,"count":1,"current":6.7},"mem":{"min":73498624,"max":73498624,"total":73498624,"count":1,"current":73498624},"progress":0,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465533.386][2018-05-16 06:12:13][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526465555.825][2018-05-16 06:12:35][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526465555.825][2018-05-16 06:12:35][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465555.825][2018-05-16 06:12:35][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526465563.887][2018-05-16 06:12:43][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526465563.888][2018-05-16 06:12:43][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465563.888][2018-05-16 06:12:43][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526465567.856][2018-05-16 06:12:47][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526465567.856][2018-05-16 06:12:47][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465563,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465567,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465567.856][2018-05-16 06:12:47][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526465582.561][2018-05-16 06:13:02][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465563,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465582,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465582.561][2018-05-16 06:13:02][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526465582.561][2018-05-16 06:13:02][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526465631.299][2018-05-16 06:13:51][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526465631.299][2018-05-16 06:13:51][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465563,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465631,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465631.299][2018-05-16 06:13:51][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526465955.688][2018-05-16 06:19:15][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526465955.688][2018-05-16 06:19:15][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465955.688][2018-05-16 06:19:15][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526465960.338][2018-05-16 06:19:20][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526465960.339][2018-05-16 06:19:20][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465631,"source":"Manual (admin)","id":"jjh8y837u0u","time_start":1526465631.306,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y837u0u.log","pid":13953,"cpu":{"min":0.3,"max":4.3,"total":19.70000000000001,"count":32,"current":0.3},"mem":{"min":68927488,"max":68927488,"total":2205679616,"count":32,"current":68927488},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465960.339][2018-05-16 06:19:20][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526465962.137][2018-05-16 06:19:22][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526465962.137][2018-05-16 06:19:22][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465582,"source":"Manual (admin)","id":"jjh8y71lx0t","time_start":1526465582.565,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y71lx0t.log","pid":13865,"cpu":{"min":0.3,"max":5.4,"total":23.700000000000014,"count":37,"current":0.3},"mem":{"min":67837952,"max":67837952,"total":2510004224,"count":37,"current":67837952},"jobid":"ForecastOptimizationjjh8y837u0u","result":"Success","percentage":100,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465962.137][2018-05-16 06:19:22][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526465963.991][2018-05-16 06:19:23][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526465963.991][2018-05-16 06:19:23][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      //console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465567,"source":"Manual (admin)","id":"jjh8y6q9h0s","time_start":1526465567.861,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8y6q9h0s.log","pid":13806,"cpu":{"min":0.3,"max":3,"total":19.800000000000015,"count":38,"current":0.3},"mem":{"min":68911104,"max":68911104,"total":2618621952,"count":38,"current":68911104},"jobid":"ForecastOptimizationjjh8y837u0u","result":"Success","percentage":100,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465963.991][2018-05-16 06:19:23][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526465979.166][2018-05-16 06:19:39][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526465979.166][2018-05-16 06:19:39][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465955,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465979,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465979.167][2018-05-16 06:19:39][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526465989.754][2018-05-16 06:19:49][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526465989.754][2018-05-16 06:19:49][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526465989.754][2018-05-16 06:19:49][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526466004.198][2018-05-16 06:20:04][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526466004.198][2018-05-16 06:20:04][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526465955,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466004,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466004.198][2018-05-16 06:20:04][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526466250.359][2018-05-16 06:24:10][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526466250.359][2018-05-16 06:24:10][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466250.359][2018-05-16 06:24:10][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526466256.23][2018-05-16 06:24:16][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526466256.23][2018-05-16 06:24:16][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466004,"source":"Manual (admin)","id":"jjh8yg2y10w","time_start":1526466004.201,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yg2y10w.log","pid":14425,"cpu":{"min":0.3,"max":3,"total":17.400000000000002,"count":24,"current":0.3},"mem":{"min":68870144,"max":68870144,"total":1652883456,"count":24,"current":68870144},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466256.23][2018-05-16 06:24:16][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526466258.289][2018-05-16 06:24:18][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526466258.289][2018-05-16 06:24:18][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n\n    // start consuming response\n    var consumer = new Kafka.KafkaConsumer({\n      //'debug': 'all',\n      'metadata.broker.list': process.env.BROKER_LIST,\n      'group.id': jobid,\n      'enable.auto.commit': true,\n      'auto.offset.reset' : 'smallest'\n    });\n    //logging debug messages, if debug is enabled\n    consumer.on('event.log', function(log) {\n      console.log(log);\n    });\n    //logging all errors\n    consumer.on('event.error',onConsumerError);\n    // when consumer is ready , subscribe to topics\n    consumer.on('ready', function(arg) {\n      console.log('consumer ready.' + JSON.stringify(arg));\n      consumer.subscribe([topicToConsume]);\n      //start consuming messages\n      consumer.consume();\n    });\n    consumer.on('data', function(msg) {\n      // Output the actual message contents\n      console.log(JSON.stringify(msg));\n      let record = JSON.parse(msg.value.toString());\n      console.log(\"Ciao arnaldo\")\n      console.log(msg.value.toString());\n    });\n    consumer.on('disconnected', function(arg) {\n      console.log('consumer disconnected. ' + JSON.stringify(arg));\n    });\n    //starting the consumer\n    consumer.connect();\n\n    process.on('SIGTERM',()=>{\n \t console.log('Sending a message to abort job');\n\t consumer.disconnect();\n \t trigger.operation = 'Abort';\n \t // send message to job trigger\n \t producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n   })\n\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526465979,"source":"Manual (admin)","id":"jjh8yfjmq0v","time_start":1526465979.17,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yfjmq0v.log","pid":14366,"cpu":{"min":0.3,"max":3.5,"total":17.900000000000006,"count":27,"current":0.3},"mem":{"min":67837952,"max":67837952,"total":1831624704,"count":27,"current":67837952},"value":{"type":"Buffer","data":[123,34,116,105,109,101,111,117,116,34,58,32,51,54,48,48,44,32,34,106,111,98,105,100,34,58,32,34,70,111,114,101,99,97,115,116,79,112,116,105,109,105,122,97,116,105,111,110,106,106,104,56,121,103,50,121,49,48,119,34,44,32,34,114,101,115,117,108,116,34,58,32,34,83,117,99,99,101,115,115,34,44,32,34,112,101,114,99,101,110,116,97,103,101,34,58,32,49,48,48,125]},"size":101,"key":null,"topic":"TriggerForecastOptimizationResponse","offset":86,"partition":0,"timestamp":1526466005905,"jobid":"ForecastOptimizationjjh8yg2y10w","result":"Success","percentage":100,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466258.289][2018-05-16 06:24:18][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526466293.576][2018-05-16 06:24:53][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526466293.576][2018-05-16 06:24:53][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466293.577][2018-05-16 06:24:53][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526466297.231][2018-05-16 06:24:57][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526466297.232][2018-05-16 06:24:57][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526466293,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466297,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466297.232][2018-05-16 06:24:57][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526466506.376][2018-05-16 06:28:26][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526466506.376][2018-05-16 06:28:26][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526466293,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466506,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466506.376][2018-05-16 06:28:26][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526466512.54][2018-05-16 06:28:32][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526466512.54][2018-05-16 06:28:32][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466512.54][2018-05-16 06:28:32][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526466514.648][2018-05-16 06:28:34][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526466514.648][2018-05-16 06:28:34][osboxes][API][debug][9][API Params][{"id":"jjh8yqufh0y","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466514.648][2018-05-16 06:28:34][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526466514.688][2018-05-16 06:28:34][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8yqufh0y][]
[1526466514.689][2018-05-16 06:28:34][osboxes][API][debug][9][API Params][{}]
[1526466514.689][2018-05-16 06:28:34][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526466662.908][2018-05-16 06:31:02][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526466662.908][2018-05-16 06:31:02][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466662.908][2018-05-16 06:31:02][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526466669.858][2018-05-16 06:31:09][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526466669.859][2018-05-16 06:31:09][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466669.859][2018-05-16 06:31:09][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526466672.362][2018-05-16 06:31:12][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526466672.362][2018-05-16 06:31:12][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523","now":1526466672}]
[1526466672.362][2018-05-16 06:31:12][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526466687.527][2018-05-16 06:31:27][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526466687.527][2018-05-16 06:31:27][osboxes][API][debug][9][API Params][{"id":"jjh8yuei90z","need_log":1,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466687.527][2018-05-16 06:31:27][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526466687.547][2018-05-16 06:31:27][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526466687.548][2018-05-16 06:31:27][osboxes][API][debug][9][API Params][{"id":"jjh8yuei90z","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466687.548][2018-05-16 06:31:27][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526466687.592][2018-05-16 06:31:27][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8yuei90z][]
[1526466687.592][2018-05-16 06:31:27][osboxes][API][debug][9][API Params][{}]
[1526466687.592][2018-05-16 06:31:27][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526466689.91][2018-05-16 06:31:29][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526466689.91][2018-05-16 06:31:29][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526466689.91][2018-05-16 06:31:29][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466695.317][2018-05-16 06:31:35][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526466695.317][2018-05-16 06:31:35][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n// Connect to the broker manually\nproducer.connect();\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    console.log('Producer is ready');\n    // send message to job trigger\n    producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n    process.exit(1)\n  }\n});\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466297,"source":"Manual (admin)","id":"jjh8ymd1w0x","time_start":1526466297.236,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8ymd1w0x.log","pid":14945,"cpu":{"min":0.3,"max":3.0999999999999996,"total":20.600000000000016,"count":39,"current":0.3},"mem":{"min":68796416,"max":68796416,"total":2683060224,"count":39,"current":68796416},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466695.317][2018-05-16 06:31:35][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526466715.815][2018-05-16 06:31:55][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526466715.815][2018-05-16 06:31:55][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526466669,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466715,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466715.815][2018-05-16 06:31:55][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526466724.895][2018-05-16 06:32:04][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526466724.895][2018-05-16 06:32:04][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466715,"source":"Manual (admin)","id":"jjh8yvc1810","time_start":1526466715.82,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8yvc1810.log","pid":15778,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466724.896][2018-05-16 06:32:04][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526466735.312][2018-05-16 06:32:15][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526466735.312][2018-05-16 06:32:15][osboxes][API][debug][9][API Params][{"id":"jjh8yuei90z","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466735.312][2018-05-16 06:32:15][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526466735.353][2018-05-16 06:32:15][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8yuei90z][]
[1526466735.353][2018-05-16 06:32:15][osboxes][API][debug][9][API Params][{}]
[1526466735.353][2018-05-16 06:32:15][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526466782.598][2018-05-16 06:33:02][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526466782.598][2018-05-16 06:33:02][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466782.601][2018-05-16 06:33:02][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526466943.201][2018-05-16 06:35:43][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526466943.201][2018-05-16 06:35:43][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466943.201][2018-05-16 06:35:43][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526466954.571][2018-05-16 06:35:54][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526466954.571][2018-05-16 06:35:54][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526466669,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466954,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526466954.574][2018-05-16 06:35:54][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526467640.338][2018-05-16 06:47:20][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526467640.338][2018-05-16 06:47:20][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true,\n  'auto.offset.reset' : 'smallest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526466954,"source":"Manual (admin)","id":"jjh8z0g9h11","time_start":1526466954.581,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8z0g9h11.log","pid":16137,"cpu":{"min":0.2,"max":3.2,"total":26.200000000000003,"count":68,"current":0.2},"mem":{"min":69140480,"max":69140480,"total":4701552640,"count":68,"current":69140480},"progress":0,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526467640.338][2018-05-16 06:47:20][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526467641.375][2018-05-16 06:47:21][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526467641.375][2018-05-16 06:47:21][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526467641.375][2018-05-16 06:47:21][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526467648.492][2018-05-16 06:47:28][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526467648.492][2018-05-16 06:47:28][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526467648.492][2018-05-16 06:47:28][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526467652.582][2018-05-16 06:47:32][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526467652.582][2018-05-16 06:47:32][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467652,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526467652.582][2018-05-16 06:47:32][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526467680.513][2018-05-16 06:48:00][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526467680.513][2018-05-16 06:48:00][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526467680.513][2018-05-16 06:48:00][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526467704.711][2018-05-16 06:48:24][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526467704.711][2018-05-16 06:48:24][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467704,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526467704.711][2018-05-16 06:48:24][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526467779.059][2018-05-16 06:49:39][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467779,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526467779.059][2018-05-16 06:49:39][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526467779.059][2018-05-16 06:49:39][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526467900.309][2018-05-16 06:51:40][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526467900.309][2018-05-16 06:51:40][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467900,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526467900.31][2018-05-16 06:51:40][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526467930.658][2018-05-16 06:52:10][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526467930.658][2018-05-16 06:52:10][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467930,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526467930.658][2018-05-16 06:52:10][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526468031.504][2018-05-16 06:53:51][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526468031.504][2018-05-16 06:53:51][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526467648,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468031,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468031.504][2018-05-16 06:53:51][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526468121.765][2018-05-16 06:55:21][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526468121.765][2018-05-16 06:55:21][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468121.766][2018-05-16 06:55:21][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526468125.755][2018-05-16 06:55:25][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526468125.755][2018-05-16 06:55:25][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468031,"source":"Manual (admin)","id":"jjh8znj8717","time_start":1526468031.511,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8znj8717.log","pid":18330,"cpu":{"min":0.4,"max":2.2,"total":7.100000000000001,"count":8,"current":0.4},"mem":{"min":69091328,"max":69091328,"total":552730624,"count":8,"current":69091328},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468125.755][2018-05-16 06:55:25][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526468127.964][2018-05-16 06:55:27][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526468127.964][2018-05-16 06:55:27][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467930,"source":"Manual (admin)","id":"jjh8zldet16","time_start":1526467930.661,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zldet16.log","pid":18056,"cpu":{"min":0.1,"max":2.2,"total":9.800000000000002,"count":18,"current":0.1},"mem":{"min":69058560,"max":69058560,"total":1243054080,"count":18,"current":69058560},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468127.964][2018-05-16 06:55:27][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526468130.312][2018-05-16 06:55:30][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526468130.312][2018-05-16 06:55:30][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467779,"source":"Manual (admin)","id":"jjh8zi4fr14","time_start":1526467779.063,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zi4fr14.log","pid":17219,"cpu":{"min":0.1,"max":4.4,"total":14.399999999999999,"count":34,"current":0.1},"mem":{"min":68800512,"max":68800512,"total":2339217408,"count":34,"current":68800512},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468130.312][2018-05-16 06:55:30][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526468132.414][2018-05-16 06:55:32][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526468132.415][2018-05-16 06:55:32][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467652,"source":"Manual (admin)","id":"jjh8zfeuj12","time_start":1526467652.587,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zfeuj12.log","pid":16860,"cpu":{"min":0.1,"max":2.7,"total":16.599999999999998,"count":46,"current":0.1},"mem":{"min":69038080,"max":69038080,"total":3175751680,"count":46,"current":69038080},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468132.415][2018-05-16 06:55:32][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526468149.001][2018-05-16 06:55:49][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526468149.001][2018-05-16 06:55:49][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467704,"source":"Manual (admin)","id":"jjh8zgj2m13","time_start":1526467704.719,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zgj2m13.log","pid":17049,"cpu":{"min":0.3,"max":3.5,"total":22.400000000000016,"count":43,"current":0.3},"mem":{"min":68980736,"max":68980736,"total":2966171648,"count":43,"current":68980736},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468149.001][2018-05-16 06:55:49][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526468155.318][2018-05-16 06:55:55][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526468155.319][2018-05-16 06:55:55][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = 'asdfasdf345' || `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526467900,"source":"Manual (admin)","id":"jjh8zkpzy15","time_start":1526467900.318,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zkpzy15.log","pid":17779,"cpu":{"min":0.1,"max":2.3,"total":11.100000000000001,"count":23,"current":0.1},"mem":{"min":69074944,"max":69074944,"total":1588723712,"count":23,"current":69074944},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468155.319][2018-05-16 06:55:55][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526468165.963][2018-05-16 06:56:05][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526468165.963][2018-05-16 06:56:05][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468165.963][2018-05-16 06:56:05][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526468175.203][2018-05-16 06:56:15][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526468175.203][2018-05-16 06:56:15][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468175.203][2018-05-16 06:56:15][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526468397.442][2018-05-16 06:59:57][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526468397.442][2018-05-16 06:59:57][osboxes][API][debug][9][API Params][{"id":"jjh8zkpzy15","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468397.442][2018-05-16 06:59:57][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526468398.224][2018-05-16 06:59:58][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh8zkpzy15][]
[1526468398.224][2018-05-16 06:59:58][osboxes][API][debug][9][API Params][{}]
[1526468398.224][2018-05-16 06:59:58][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526468458.205][2018-05-16 07:00:58][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526468458.205][2018-05-16 07:00:58][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526468121,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468458,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468458.206][2018-05-16 07:00:58][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526468470.254][2018-05-16 07:01:10][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526468470.254][2018-05-16 07:01:10][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468470.255][2018-05-16 07:01:10][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526468474.844][2018-05-16 07:01:14][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526468474.844][2018-05-16 07:01:14][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526468474.845][2018-05-16 07:01:14][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526469305.755][2018-05-16 07:15:05][osboxes][API][debug][6][Handling API request: POST /api/app/abort_job][{}]
[1526469305.755][2018-05-16 07:15:05][osboxes][API][debug][9][API Params][{"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid = `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE || 'TriggerForecastOptimization' ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME || 'TriggerForecastOptimizationResponse';\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\nvalidate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT) || 1000,\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME || 1000  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","category":"general","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526468458,"source":"Manual (admin)","id":"jjh8zwoh618","time_start":1526468458.218,"hostname":"osboxes","event":"ejh8vu06v01","event_title":"ForecastOptimization","nice_target":"All Servers","command":"bin/shell-plugin.js","log_file":"/home/osboxes/git/cronicle-poc/custom-cronicle/Cronicle/logs/jobs/jjh8zwoh618.log","pid":18815,"cpu":{"min":0.2,"max":5,"total":33.700000000000024,"count":84,"current":0.2},"mem":{"min":69029888,"max":69029888,"total":5798510592,"count":84,"current":69029888},"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526469305.755][2018-05-16 07:15:05][osboxes][API][debug][9][Activating namespaced API handler: app/api_abort_job for URI: /api/app/abort_job][]
[1526473767.986][2018-05-16 08:29:27][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526473767.986][2018-05-16 08:29:27][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526473767.987][2018-05-16 08:29:27][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526473771.162][2018-05-16 08:29:31][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526473771.162][2018-05-16 08:29:31][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526473767,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526473771,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526473771.164][2018-05-16 08:29:31][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526473777.323][2018-05-16 08:29:37][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526473777.323][2018-05-16 08:29:37][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526473777.323][2018-05-16 08:29:37][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526473780.506][2018-05-16 08:29:40][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526473780.506][2018-05-16 08:29:40][osboxes][API][debug][9][API Params][{"id":"jjh932jz619","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526473780.507][2018-05-16 08:29:40][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526473780.566][2018-05-16 08:29:40][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh932jz619][]
[1526473780.566][2018-05-16 08:29:40][osboxes][API][debug][9][API Params][{}]
[1526473780.566][2018-05-16 08:29:40][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526473799.268][2018-05-16 08:29:59][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526473799.268][2018-05-16 08:29:59][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  //console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log(msg.value.toString());\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526473767,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526473799,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526473799.268][2018-05-16 08:29:59][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526473804.26][2018-05-16 08:30:04][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526473804.26][2018-05-16 08:30:04][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526473804.261][2018-05-16 08:30:04][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526473805.792][2018-05-16 08:30:05][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526473805.792][2018-05-16 08:30:05][osboxes][API][debug][9][API Params][{"id":"jjh9335nu1a","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526473805.795][2018-05-16 08:30:05][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526473805.832][2018-05-16 08:30:05][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh9335nu1a][]
[1526473805.832][2018-05-16 08:30:05][osboxes][API][debug][9][API Params][{}]
[1526473805.832][2018-05-16 08:30:05][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526473865.912][2018-05-16 08:31:05][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526473865.912][2018-05-16 08:31:05][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526473865.912][2018-05-16 08:31:05][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526473867.228][2018-05-16 08:31:07][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526473867.228][2018-05-16 08:31:07][osboxes][API][debug][9][API Params][{"id":"jjh9335nu1a","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526473867.228][2018-05-16 08:31:07][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526473867.266][2018-05-16 08:31:07][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh9335nu1a][]
[1526473867.266][2018-05-16 08:31:07][osboxes][API][debug][9][API Params][{}]
[1526473867.266][2018-05-16 08:31:07][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526473993.59][2018-05-16 08:33:13][osboxes][API][debug][6][Handling API request: POST /api/app/update_event][{}]
[1526473993.59][2018-05-16 08:33:13][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log('A message received');\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526462430,"created":1526461614,"username":"admin","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526473993.59][2018-05-16 08:33:13][osboxes][API][debug][9][Activating namespaced API handler: app/api_update_event for URI: /api/app/update_event][]
[1526473997.822][2018-05-16 08:33:17][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526473997.822][2018-05-16 08:33:17][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526473997.822][2018-05-16 08:33:17][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526474000.128][2018-05-16 08:33:20][osboxes][API][debug][6][Handling API request: POST /api/app/run_event][{}]
[1526474000.128][2018-05-16 08:33:20][osboxes][API][debug][9][API Params][{"enabled":1,"params":{"script":"#!/usr/bin/env node\n\n'use strict';\nlet Kafka = require('node-rdkafka');\nlet jobid =  `${process.env.JOB_EVENT_TITLE}${process.env.JOB_ID}`.replace(' ','');\nconst topicToProduce = process.env.TOPIC_TO_PRODUCE ;\nconst topicToConsume = process.env.TOPIC_TO_CONSUME ;\nlet requestDelay = process.env.REQUEST_DELAY || 10000;\n\n//validate();\n\nconsole.log('Creating producer')\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'client.id': jobid ,\n  'dr_cb': true\n});\n//message body\nlet trigger = {\n  timeout : Number(process.env.JOB_TIMEOUT),\n  jobid :  jobid || 'jobid' ,\n  processingTime : process.env.PROCESSING_TIME  ,\n  error : process.env.IN_ERROR || false,\n  operation: 'Start'\n};\n// message key\nlet key = {jobid: jobid};\n\nconsole.log('Producer connection');\n\n\n// start consuming response\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': process.env.BROKER_LIST,\n  'group.id': jobid,\n  'enable.auto.commit': true\n} , {\n    'auto.offset.reset' : 'earliest'\n});\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n//logging all errors\nconsumer.on('event.error',onConsumerError);\n// when consumer is ready , subscribe to topics\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n  consumer.subscribe([topicToConsume]);\n  //start consuming messages\n  consumer.consume();\n  // Connect to the broker manually\n  producer.connect();\n  // Wait for the ready event before proceeding\n  producer.on('ready', function() {\n    try {\n      console.log('Producer is ready');\n      // send message to job trigger\n      producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n    } catch (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n      process.exit(1)\n    }\n  });\n\n\n});\nconsumer.on('data', function(msg) {\n  // Output the actual message contents\n  console.log(JSON.stringify(msg));\n  let record = JSON.parse(msg.value.toString());\n  console.log('A message received');\n  if(record.jobid === jobid){\n    consumer.disconnect();\n    onNextMsgReceived(record);\n  }\n});\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n//starting the consumer\nconsumer.connect();\n\nprocess.on('SIGTERM',()=>{\n  console.log('Sending a message to abort job');\n  consumer.disconnect();\n  trigger.operation = 'Abort';\n  // send message to job trigger\n  producer.produce(topicToProduce,null,new Buffer(JSON.stringify(trigger)),new Buffer(JSON.stringify(key)),Date.now(),);\n})\n\n\n/**\n* Function execute on next message from kafka for a consumer\n*/\nfunction onNextMsgReceived(value){\n  console.log(`Job response received ${value.jobid} , status : ${value.result} , percentage : ${value.percentage}`);\n  console.log(`${value.percentage}%`);\n  if(value.result === 'Success' && value.percentage === 100){\n    process.exit();\n  }else if (value.result === 'Failure'){\n    process.exit(1);\n  }\n}\n\n/**\n* Function to validate if environment variable are properly configured\n*\n*/\nfunction validate() {\n  if(!process.env.JOB_ID) {\n    console.error('JOBID not defined !');\n    process.exit(1);\n  }else if(!process.env.JOB_EVENT_TITLE) {\n    console.error('JOB_EVENT_TITLE not defined !');\n    process.exit(1);\n  } else if(!process.env.JOB_TIMEOUT){\n    console.error('JOB_TIMEOUT not defined !');\n    process.exit(1);\n  } else if(!process.env.TOPIC_TO_CONSUME) {\n    console.error('TOPIC_TO_CONSUME not defined !');\n    process.exit(1);\n  }else if(!process.env.TOPIC_TO_PRODUCE) {\n    console.error('TOPIC_TO_PRODUCE not defined !');\n    process.exit(1);\n  }else if(!process.env.BROKER_LIST) {\n    console.error('BROKER_LIST not defined !');\n    process.exit(1);\n  }\n}\n\n/**\n*   An error occurs during a topic creation , so you need to shutdown a consumer instance\n*/\nfunction onConsumerError(err){\n  console.log(`Something wrong consuming event ${topicToConsume} : ${err}`);\n  process.exit(1);\n}\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n  process.exit(1);\n})\n","annotate":1,"json":1,"TOPIC_TO_CONSUME":"TriggerForecastOptimizationResponse","TOPIC_TO_PRODUCE":"TriggerForecastOptimization","BROKER_LIST":"localhost:9092"},"timing":{"minutes":[0],"months":[5],"days":[16],"hours":[5]},"max_children":32,"timeout":3600,"catch_up":0,"queue_max":1000,"timezone":"America/New_York","plugin":"shellplug","title":"ForecastOptimization","category":"general","target":"allgrp","algo":"random","multiplex":0,"stagger":0,"retries":0,"retry_delay":0,"detached":0,"queue":0,"chain":"","chain_error":"","notify_success":"","notify_fail":"","web_hook":"","cpu_limit":0,"cpu_sustain":0,"memory_limit":0,"memory_sustain":0,"notes":"","id":"ejh8vu06v01","modified":1526473993,"created":1526461614,"username":"admin","category_title":"General","group_title":"All Servers","plugin_title":"Shell Script","now":1526474000,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526474000.128][2018-05-16 08:33:20][osboxes][API][debug][9][Activating namespaced API handler: app/api_run_event for URI: /api/app/run_event][]
[1526474006.259][2018-05-16 08:33:26][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526474006.259][2018-05-16 08:33:26][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526474006.26][2018-05-16 08:33:26][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
[1526474007.289][2018-05-16 08:33:27][osboxes][API][debug][6][Handling API request: POST /api/app/get_job_details][{}]
[1526474007.289][2018-05-16 08:33:27][osboxes][API][debug][9][API Params][{"id":"jjh937gna1b","session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526474007.289][2018-05-16 08:33:27][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_details for URI: /api/app/get_job_details][]
[1526474007.331][2018-05-16 08:33:27][osboxes][API][debug][6][Handling API request: GET /api/app/get_job_log?id=jjh937gna1b][]
[1526474007.331][2018-05-16 08:33:27][osboxes][API][debug][9][API Params][{}]
[1526474007.331][2018-05-16 08:33:27][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_job_log for URI: /api/app/get_job_log][]
[1526474036.982][2018-05-16 08:33:56][osboxes][API][debug][6][Handling API request: POST /api/app/get_history][{}]
[1526474036.982][2018-05-16 08:33:56][osboxes][API][debug][9][API Params][{"sub":"history","offset":0,"limit":25,"session_id":"07b6cda6296d12d22ab1e6eb1358c14dd956b39924dedc665e7bde8372a0d523"}]
[1526474036.982][2018-05-16 08:33:56][osboxes][API][debug][9][Activating namespaced API handler: app/api_get_history for URI: /api/app/get_history][]
